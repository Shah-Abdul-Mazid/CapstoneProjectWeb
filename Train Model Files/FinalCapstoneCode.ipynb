{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d912bb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "import json\n",
    "import csv\n",
    "import warnings\n",
    "from tabulate import tabulate\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics as skm\n",
    "from sklearn import preprocessing as skp\n",
    "from sklearn import utils as sku\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.applications as applications\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    GlobalAveragePooling2D,\n",
    "    Concatenate,\n",
    "    Dense,\n",
    "    Dropout,\n",
    ")\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "\n",
    "from tensorflow.keras.applications import MobileNet, DenseNet121\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input as mobilenet_preprocess\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_v2_preprocess\n",
    "from tensorflow.keras.applications.densenet import preprocess_input as dense_preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d48f156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "  GPU Device: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if physical_devices:\n",
    "    print(\"Num GPUs Available: \", len(physical_devices))\n",
    "    for gpu in physical_devices:\n",
    "        print(f\"  GPU Device: {gpu}\")\n",
    "else:\n",
    "    print(\"No GPU devices found. TensorFlow will run on CPU.\")\n",
    "    print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87caa1f",
   "metadata": {},
   "source": [
    "Convert Data Mat To JPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c18b4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing folder: E:\\dataset001\\data\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'E:\\\\dataset001\\\\data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 89\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProcessing folder: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     90\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     91\u001b[0m             file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_path, filename)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'E:\\\\dataset001\\\\data'"
     ]
    }
   ],
   "source": [
    "folder_pairs = [\n",
    "    {\n",
    "        'input': r'E:\\dataset001\\data',\n",
    "        'output': r'E:\\dataset001\\datajpg'\n",
    "    },\n",
    "    {\n",
    "        'input': r'E:\\dataset001\\cvind.mat',\n",
    "        'output': r'E:\\dataset001',\n",
    "        'is_file': True  \n",
    "    }\n",
    "]\n",
    "\n",
    "label_to_class = {\n",
    "    1: 'meningioma',\n",
    "    2: 'glioma',\n",
    "    3: 'pituitary tumor'\n",
    "}\n",
    "\n",
    "def process_mat_file(file_path, output_folder):\n",
    "    filename = os.path.basename(file_path)\n",
    "    print(f\"Processing {filename}...\")\n",
    "\n",
    "    try:\n",
    "        with h5py.File(file_path, 'r') as f:\n",
    "            if 'cjdata' not in f:\n",
    "                print(f\"Skipping {filename}: No 'cjdata' group found\")\n",
    "                return\n",
    "\n",
    "            cjdata = f['cjdata']\n",
    "\n",
    "            if 'image' not in cjdata or 'label' not in cjdata:\n",
    "                print(f\"Skipping {filename}: Missing 'image' or 'label' dataset\")\n",
    "                return\n",
    "\n",
    "            image_array = cjdata['image'][:]\n",
    "\n",
    "            label = cjdata['label'][()]  \n",
    "            if isinstance(label, np.ndarray):\n",
    "                label = label.item()  \n",
    "            label = int(label) \n",
    "        if label not in label_to_class:\n",
    "            print(f\"Skipping {filename}: Invalid label {label}\")\n",
    "            return\n",
    "\n",
    "        image_array = image_array.astype(np.float64) \n",
    "        min_val = np.min(image_array)\n",
    "        max_val = np.max(image_array)\n",
    "        if max_val != min_val:  \n",
    "            image_array = (255 / (max_val - min_val) * (image_array - min_val)).astype(np.uint8)\n",
    "        else:\n",
    "            print(f\"Skipping {filename}: Image has no contrast (max == min)\")\n",
    "            return\n",
    "\n",
    "        if len(image_array.shape) == 2:\n",
    "            image = Image.fromarray(image_array, mode='L') \n",
    "        else:\n",
    "            print(f\"Skipping {filename}: Unsupported array shape {image_array.shape}\")\n",
    "            return\n",
    "\n",
    "        class_name = label_to_class[label]\n",
    "        label_folder = os.path.join(output_folder, class_name)\n",
    "        if not os.path.exists(label_folder):\n",
    "            os.makedirs(label_folder)\n",
    "\n",
    "        output_filename = os.path.splitext(filename)[0] + '.jpg'\n",
    "        output_file_path = os.path.join(label_folder, output_filename)\n",
    "\n",
    "        image.save(output_file_path, 'JPEG')\n",
    "        print(f\"Saved image as {output_file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {str(e)}\")\n",
    "\n",
    "for pair in folder_pairs:\n",
    "    input_path = pair['input']\n",
    "    output_folder = pair['output']\n",
    "    is_file = pair.get('is_file', False)\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    if is_file:\n",
    "        if os.path.exists(input_path) and input_path.endswith('.mat'):\n",
    "            process_mat_file(input_path, output_folder)\n",
    "        else:\n",
    "            print(f\"Skipping {input_path}: File not found or not a .mat file\")\n",
    "    else:\n",
    "        print(f\"\\nProcessing folder: {input_path}\")\n",
    "        for filename in os.listdir(input_path):\n",
    "            if filename.endswith('.mat'):\n",
    "                file_path = os.path.join(input_path, filename)\n",
    "                process_mat_file(file_path, output_folder)\n",
    "\n",
    "print(\"Conversion complete for all folders and files!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0ce242",
   "metadata": {},
   "source": [
    "Spliting Data 90% & \n",
    "10%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379f1e81",
   "metadata": {},
   "source": [
    "ImBalanced Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8156d857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting Dataset 1 (E:\\Data001\\Data)...\n",
      "Class Glioma: 1283 train, 143 test\n",
      "Class Pituitary: 837 train, 93 test\n",
      "Class Meningioma: 637 train, 71 test\n",
      "Dataset 1 split completed successfully.\n",
      "\n",
      "Splitting Dataset 2 (E:\\Data002\\Data)...\n",
      "Class Glioma: 833 train, 93 test\n",
      "Class Pituitary: 810 train, 91 test\n",
      "Class Meningioma: 843 train, 94 test\n",
      "Class No-tumor: 450 train, 50 test\n",
      "Dataset 2 split completed successfully.\n",
      "\n",
      "Combining datasets into E:\\CombineData001_002\\Data...\n",
      "Copied 708 images from E:\\Data001\\Data\\Meningioma to E:\\CombineData001_002\\Data\\Meningioma\n",
      "Copied 1426 images from E:\\Data001\\Data\\Glioma to E:\\CombineData001_002\\Data\\Glioma\n",
      "Copied 930 images from E:\\Data001\\Data\\Pituitary to E:\\CombineData001_002\\Data\\Pituitary\n",
      "Copied 937 images from E:\\Data002\\Data\\Meningioma to E:\\CombineData001_002\\Data\\Meningioma\n",
      "Copied 926 images from E:\\Data002\\Data\\Glioma to E:\\CombineData001_002\\Data\\Glioma\n",
      "Copied 901 images from E:\\Data002\\Data\\Pituitary to E:\\CombineData001_002\\Data\\Pituitary\n",
      "Copied 500 images from E:\\Data002\\Data\\No-tumor to E:\\CombineData001_002\\Data\\No-tumor\n",
      "\n",
      "Splitting Combined Dataset (E:\\CombineData001_002\\Data)...\n",
      "Class Glioma: 4233 train, 471 test\n",
      "Class Pituitary: 3295 train, 367 test\n",
      "Class Meningioma: 2961 train, 329 test\n",
      "Class No-tumor: 900 train, 100 test\n",
      "Combined Dataset split completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "\n",
    "DATASETS = [\n",
    "    {\n",
    "        'name': 'Dataset 1',\n",
    "        'input_dir': r\"E:\\Data001\\Data\",\n",
    "        'output_dir': r\"E:\\Data001\\SplitData\",\n",
    "        'class_names': ['Meningioma', 'Glioma', 'Pituitary']\n",
    "    },\n",
    "    {\n",
    "        'name': 'Dataset 2',\n",
    "        'input_dir': r\"E:\\Data002\\Data\",\n",
    "        'output_dir': r\"E:\\Data002\\SplitData\",\n",
    "        'class_names': ['Meningioma', 'Glioma', 'Pituitary', 'No-tumor']\n",
    "    }\n",
    "]\n",
    "\n",
    "COMBINED_DATASET = {\n",
    "    'name': 'Combined Dataset',\n",
    "    'input_dir': r\"E:\\CombineData001_002\\Data\",\n",
    "    'output_dir': r\"E:\\CombineData001_002\\SplitData\",\n",
    "    'class_names': ['Meningioma', 'Glioma', 'Pituitary', 'No-tumor']  \n",
    "}\n",
    "\n",
    "def ensure_unique_filename(dest_path):\n",
    "    \"\"\"Append a suffix to the filename if it already exists.\"\"\"\n",
    "    dest_path = Path(dest_path)\n",
    "    base, ext = dest_path.stem, dest_path.suffix\n",
    "    counter = 1\n",
    "    new_path = dest_path\n",
    "    while new_path.exists():\n",
    "        new_path = dest_path.with_name(f\"{base}_{counter}{ext}\")\n",
    "        counter += 1\n",
    "    return str(new_path)\n",
    "\n",
    "def combine_datasets(datasets, combined_input_dir):\n",
    "    \"\"\"Combine multiple datasets into a single directory structure.\"\"\"\n",
    "    combined_input_dir = Path(combined_input_dir)\n",
    "    combined_input_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        dataset_dir = Path(dataset['input_dir'])\n",
    "        if not dataset_dir.exists():\n",
    "            print(f\"Error: Dataset directory {dataset_dir} does not exist.\")\n",
    "            continue\n",
    "        \n",
    "        for class_name in dataset['class_names']:\n",
    "            class_path = dataset_dir / class_name\n",
    "            if not class_path.is_dir():\n",
    "                print(f\"Warning: Class {class_name} not found in {dataset_dir}\")\n",
    "                continue\n",
    "            \n",
    "            combined_class_path = combined_input_dir / class_name\n",
    "            combined_class_path.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            images = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "            for img in images:\n",
    "                src = class_path / img\n",
    "                dest = combined_class_path / img\n",
    "                dest = ensure_unique_filename(dest)\n",
    "                try:\n",
    "                    shutil.copy(src, dest)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error copying {src} to {dest}: {e}\")\n",
    "            \n",
    "            print(f\"Copied {len(images)} images from {class_path} to {combined_class_path}\")\n",
    "\n",
    "def split_dataset(dataset_dir, output_dir, class_names, train_size=0.9, test_size=0.1):\n",
    "    dataset_dir = Path(dataset_dir)\n",
    "    output_dir = Path(output_dir)\n",
    "    \n",
    "    if not dataset_dir.exists():\n",
    "        print(f\"Error: Dataset directory {dataset_dir} does not exist.\")\n",
    "        return False\n",
    "    if abs(train_size + test_size - 1.0) > 1e-6:\n",
    "        print(f\"Error: Split sizes must sum to 1. Got train={train_size}, test={test_size}.\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        for split in ['train', 'test']:\n",
    "            for cls in class_names:\n",
    "                (output_dir / split / cls).mkdir(parents=True, exist_ok=True)\n",
    "    except PermissionError as e:\n",
    "        print(f\"Error: Cannot create directories in {output_dir}: {e}\")\n",
    "        return False\n",
    "    \n",
    "    available_classes = {d for d in os.listdir(dataset_dir) if dataset_dir.joinpath(d).is_dir()}\n",
    "    missing_classes = set(class_names) - available_classes\n",
    "    if missing_classes:\n",
    "        print(f\"Warning: Missing classes in {dataset_dir}: {missing_classes}\")\n",
    "    \n",
    "    for class_name in available_classes:\n",
    "        if class_name not in class_names:\n",
    "            print(f\"Skipping unknown class: {class_name}\")\n",
    "            continue\n",
    "        \n",
    "        class_path = dataset_dir / class_name\n",
    "        images = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        if not images:\n",
    "            print(f\"No images found in class: {class_name}\")\n",
    "            continue\n",
    "        \n",
    "        train_images, test_images = train_test_split(images, train_size=train_size, random_state=42)\n",
    "        \n",
    "        for img in train_images:\n",
    "            src = class_path / img\n",
    "            dest = output_dir / 'train' / class_name / img\n",
    "            dest = ensure_unique_filename(dest)\n",
    "            try:\n",
    "                shutil.copy(src, dest)\n",
    "            except Exception as e:\n",
    "                print(f\"Error copying {src} to {dest}: {e}\")\n",
    "        \n",
    "        for img in test_images:\n",
    "            src = class_path / img\n",
    "            dest = output_dir / 'test' / class_name / img\n",
    "            dest = ensure_unique_filename(dest)\n",
    "            try:\n",
    "                shutil.copy(src, dest)\n",
    "            except Exception as e:\n",
    "                print(f\"Error copying {src} to {dest}: {e}\")\n",
    "        \n",
    "        print(f\"Class {class_name}: {len(train_images)} train, {len(test_images)} test\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def main():\n",
    "    for dataset in DATASETS:\n",
    "        print(f\"\\nSplitting {dataset['name']} ({dataset['input_dir']})...\")\n",
    "        success = split_dataset(\n",
    "            dataset['input_dir'],\n",
    "            dataset['output_dir'],\n",
    "            dataset['class_names'],\n",
    "            train_size=0.9,\n",
    "            test_size=0.1\n",
    "        )\n",
    "        if success:\n",
    "            print(f\"{dataset['name']} split completed successfully.\")\n",
    "        else:\n",
    "            print(f\"{dataset['name']} split failed.\")\n",
    "    \n",
    "    print(f\"\\nCombining datasets into {COMBINED_DATASET['input_dir']}...\")\n",
    "    combine_datasets(DATASETS, COMBINED_DATASET['input_dir'])\n",
    "    \n",
    "    print(f\"\\nSplitting {COMBINED_DATASET['name']} ({COMBINED_DATASET['input_dir']})...\")\n",
    "    success = split_dataset(\n",
    "        COMBINED_DATASET['input_dir'],\n",
    "        COMBINED_DATASET['output_dir'],\n",
    "        COMBINED_DATASET['class_names'],\n",
    "        train_size=0.9,\n",
    "        test_size=0.1\n",
    "    )\n",
    "    if success:\n",
    "        print(f\"{COMBINED_DATASET['name']} split completed successfully.\")\n",
    "    else:\n",
    "        print(f\"{COMBINED_DATASET['name']} split failed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c2e75a",
   "metadata": {},
   "source": [
    "Balanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd341025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting Dataset 1 (E:\\Data001\\Data1)...\n",
      "Error: Dataset directory E:\\Data001\\Data1 does not exist.\n",
      "Dataset 1 split failed.\n",
      "\n",
      "Splitting Dataset 2 (E:\\Data002\\Data1)...\n",
      "Error: Dataset directory E:\\Data002\\Data1 does not exist.\n",
      "Dataset 2 split failed.\n",
      "\n",
      "Combining datasets into E:\\CombineData001_002\\Data1...\n",
      "Error: Dataset directory E:\\Data001\\Data1 does not exist.\n",
      "Error: Dataset directory E:\\Data002\\Data1 does not exist.\n",
      "\n",
      "Splitting Combined Dataset (E:\\CombineData001_002\\Data1)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 161\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCOMBINED_DATASET[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m split failed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 161\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 148\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    145\u001b[0m combine_datasets(DATASETS, COMBINED_DATASET[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_dir\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSplitting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCOMBINED_DATASET[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCOMBINED_DATASET[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_dir\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 148\u001b[0m success \u001b[38;5;241m=\u001b[39m \u001b[43msplit_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCOMBINED_DATASET\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_dir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCOMBINED_DATASET\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput_dir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCOMBINED_DATASET\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclass_names\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\n\u001b[0;32m    154\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCOMBINED_DATASET[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m split completed successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 112\u001b[0m, in \u001b[0;36msplit_dataset\u001b[1;34m(dataset_dir, output_dir, class_names, train_size, test_size)\u001b[0m\n\u001b[0;32m    110\u001b[0m dest \u001b[38;5;241m=\u001b[39m ensure_unique_filename(dest)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 112\u001b[0m     \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError copying \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdest\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\shutil.py:417\u001b[0m, in \u001b[0;36mcopy\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(dst):\n\u001b[0;32m    416\u001b[0m     dst \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dst, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(src))\n\u001b[1;32m--> 417\u001b[0m \u001b[43mcopyfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    418\u001b[0m copymode(src, dst, follow_symlinks\u001b[38;5;241m=\u001b[39mfollow_symlinks)\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dst\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\shutil.py:258\u001b[0m, in \u001b[0;36mcopyfile\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(dst, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fdst:\n\u001b[0;32m    257\u001b[0m         \u001b[38;5;66;03m# macOS\u001b[39;00m\n\u001b[1;32m--> 258\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _HAS_FCOPYFILE:\n\u001b[0;32m    259\u001b[0m             \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    260\u001b[0m                 _fastcopy_fcopyfile(fsrc, fdst, posix\u001b[38;5;241m.\u001b[39m_COPYFILE_DATA)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATASETS = [\n",
    "    {\n",
    "        'name': 'Dataset 1',\n",
    "        'input_dir': r\"E:\\Data001\\Data1\",\n",
    "        'output_dir': r\"E:\\Data001\\SplitData1\",\n",
    "        'class_names': ['Meningioma', 'Glioma', 'Pituitary']\n",
    "    },\n",
    "    {\n",
    "        'name': 'Dataset 2',\n",
    "        'input_dir': r\"E:\\Data002\\Data1\",\n",
    "        'output_dir': r\"E:\\Data002\\SplitData1\",\n",
    "        'class_names': ['Meningioma', 'Glioma', 'Pituitary', 'No-tumor']\n",
    "    }\n",
    "]\n",
    "\n",
    "COMBINED_DATASET = {\n",
    "    'name': 'Combined Dataset',\n",
    "    'input_dir': r\"E:\\CombineData001_002\\Data1\",\n",
    "    'output_dir': r\"E:\\CombineData001_002\\SplitData1\",\n",
    "    'class_names': ['Meningioma', 'Glioma', 'Pituitary', 'No-tumor']  \n",
    "}\n",
    "\n",
    "def ensure_unique_filename(dest_path):\n",
    "    \"\"\"Append a suffix to the filename if it already exists.\"\"\"\n",
    "    dest_path = Path(dest_path)\n",
    "    base, ext = dest_path.stem, dest_path.suffix\n",
    "    counter = 1\n",
    "    new_path = dest_path\n",
    "    while new_path.exists():\n",
    "        new_path = dest_path.with_name(f\"{base}_{counter}{ext}\")\n",
    "        counter += 1\n",
    "    return str(new_path)\n",
    "\n",
    "def combine_datasets(datasets, combined_input_dir):\n",
    "    \"\"\"Combine multiple datasets into a single directory structure.\"\"\"\n",
    "    combined_input_dir = Path(combined_input_dir)\n",
    "    combined_input_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        dataset_dir = Path(dataset['input_dir'])\n",
    "        if not dataset_dir.exists():\n",
    "            print(f\"Error: Dataset directory {dataset_dir} does not exist.\")\n",
    "            continue\n",
    "        \n",
    "        for class_name in dataset['class_names']:\n",
    "            class_path = dataset_dir / class_name\n",
    "            if not class_path.is_dir():\n",
    "                print(f\"Warning: Class {class_name} not found in {dataset_dir}\")\n",
    "                continue\n",
    "            \n",
    "            combined_class_path = combined_input_dir / class_name\n",
    "            combined_class_path.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            images = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "            for img in images:\n",
    "                src = class_path / img\n",
    "                dest = combined_class_path / img\n",
    "                dest = ensure_unique_filename(dest)\n",
    "                try:\n",
    "                    shutil.copy(src, dest)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error copying {src} to {dest}: {e}\")\n",
    "            \n",
    "            print(f\"Copied {len(images)} images from {class_path} to {combined_class_path}\")\n",
    "\n",
    "def split_dataset(dataset_dir, output_dir, class_names, train_size=0.9, test_size=0.1):\n",
    "    dataset_dir = Path(dataset_dir)\n",
    "    output_dir = Path(output_dir)\n",
    "    \n",
    "    if not dataset_dir.exists():\n",
    "        print(f\"Error: Dataset directory {dataset_dir} does not exist.\")\n",
    "        return False\n",
    "    if abs(train_size + test_size - 1.0) > 1e-6:\n",
    "        print(f\"Error: Split sizes must sum to 1. Got train={train_size}, test={test_size}.\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        for split in ['train', 'test']:\n",
    "            for cls in class_names:\n",
    "                (output_dir / split / cls).mkdir(parents=True, exist_ok=True)\n",
    "    except PermissionError as e:\n",
    "        print(f\"Error: Cannot create directories in {output_dir}: {e}\")\n",
    "        return False\n",
    "    \n",
    "    available_classes = {d for d in os.listdir(dataset_dir) if dataset_dir.joinpath(d).is_dir()}\n",
    "    missing_classes = set(class_names) - available_classes\n",
    "    if missing_classes:\n",
    "        print(f\"Warning: Missing classes in {dataset_dir}: {missing_classes}\")\n",
    "    \n",
    "    for class_name in available_classes:\n",
    "        if class_name not in class_names:\n",
    "            print(f\"Skipping unknown class: {class_name}\")\n",
    "            continue\n",
    "        \n",
    "        class_path = dataset_dir / class_name\n",
    "        images = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        if not images:\n",
    "            print(f\"No images found in class: {class_name}\")\n",
    "            continue\n",
    "        \n",
    "        train_images, test_images = train_test_split(images, train_size=train_size, random_state=42)\n",
    "        \n",
    "        for img in train_images:\n",
    "            src = class_path / img\n",
    "            dest = output_dir / 'train' / class_name / img\n",
    "            dest = ensure_unique_filename(dest)\n",
    "            try:\n",
    "                shutil.copy(src, dest)\n",
    "            except Exception as e:\n",
    "                print(f\"Error copying {src} to {dest}: {e}\")\n",
    "        \n",
    "        for img in test_images:\n",
    "            src = class_path / img\n",
    "            dest = output_dir / 'test' / class_name / img\n",
    "            dest = ensure_unique_filename(dest)\n",
    "            try:\n",
    "                shutil.copy(src, dest)\n",
    "            except Exception as e:\n",
    "                print(f\"Error copying {src} to {dest}: {e}\")\n",
    "        \n",
    "        print(f\"Class {class_name}: {len(train_images)} train, {len(test_images)} test\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def main():\n",
    "    for dataset in DATASETS:\n",
    "        print(f\"\\nSplitting {dataset['name']} ({dataset['input_dir']})...\")\n",
    "        success = split_dataset(\n",
    "            dataset['input_dir'],\n",
    "            dataset['output_dir'],\n",
    "            dataset['class_names'],\n",
    "            train_size=0.9,\n",
    "            test_size=0.1\n",
    "        )\n",
    "        if success:\n",
    "            print(f\"{dataset['name']} split completed successfully.\")\n",
    "        else:\n",
    "            print(f\"{dataset['name']} split failed.\")\n",
    "    \n",
    "    print(f\"\\nCombining datasets into {COMBINED_DATASET['input_dir']}...\")\n",
    "    combine_datasets(DATASETS, COMBINED_DATASET['input_dir'])\n",
    "    \n",
    "    print(f\"\\nSplitting {COMBINED_DATASET['name']} ({COMBINED_DATASET['input_dir']})...\")\n",
    "    success = split_dataset(\n",
    "        COMBINED_DATASET['input_dir'],\n",
    "        COMBINED_DATASET['output_dir'],\n",
    "        COMBINED_DATASET['class_names'],\n",
    "        train_size=0.9,\n",
    "        test_size=0.1\n",
    "    )\n",
    "    if success:\n",
    "        print(f\"{COMBINED_DATASET['name']} split completed successfully.\")\n",
    "    else:\n",
    "        print(f\"{COMBINED_DATASET['name']} split failed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3676867",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "def compute_metrics(y_true, y_pred, dataset_name):\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    return {\n",
    "        f\"{dataset_name}_precision\": precision,\n",
    "        f\"{dataset_name}_recall\": recall,\n",
    "        f\"{dataset_name}_f1\": f1\n",
    "    }\n",
    "def evaluate_model(hybrid_model, test_generator, class_names, counter, results_dir, model_name):\n",
    "    start_time = time.time()\n",
    "    test_loss, test_acc = hybrid_model.evaluate(test_generator, verbose=0)\n",
    "    y_true = test_generator.classes\n",
    "    y_pred = np.argmax(hybrid_model.predict(test_generator, verbose=0), axis=1)\n",
    "    y_probs = hybrid_model.predict(test_generator, verbose=0)\n",
    "    inference_time = (time.time() - start_time) * 1000\n",
    "\n",
    "    y_probs = np.nan_to_num(y_probs / np.sum(y_probs, axis=1, keepdims=True), nan=0.0, posinf=1.0, neginf=0.0)\n",
    "    test_precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    test_recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    test_f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_probs, multi_class='ovr') if len(np.unique(y_true)) > 1 else None\n",
    "    auc_display = f\"{auc:.4f}\" if auc is not None else 'N/A'\n",
    "\n",
    "    print(f\"Test Results for {model_name}: Loss={test_loss:.4f}, Acc={test_acc:.4f}, \"\n",
    "          f\"Precision={test_precision:.4f}, Recall={test_recall:.4f}, F1={test_f1:.4f}, \"\n",
    "          f\"AUC={auc_display}, Inference Time={inference_time:.2f}ms\")\n",
    "\n",
    "    return {\n",
    "        'Model with input image size': model_name,\n",
    "        'Accuracy': test_acc,\n",
    "        'AUC': auc,\n",
    "        'Loss': test_loss,\n",
    "        'Precision': test_precision,\n",
    "        'Recall': test_recall,\n",
    "        'F1 Score': test_f1,\n",
    "        'Inference time (in miliseconds)': inference_time\n",
    "    }, y_true, y_pred\n",
    "\n",
    "def plot_training_history(history, results_dir, model_name, counter, class_names, best_y_true, best_y_pred):\n",
    "    plt.figure(figsize=(18, 6), dpi=300)\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history['accuracy'], label='Train Accuracy', color='#1f77b4', linewidth=2)\n",
    "    plt.plot(history['val_accuracy'], label='Test Accuracy', color='#ff7f0e', linestyle='--', linewidth=2)\n",
    "    plt.title('Train vs Test Accuracy', fontsize=14)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Accuracy', fontsize=12)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.ylim(0, 1.05)\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history['loss'], label='Train Loss', color='#1f77b4', linewidth=2)\n",
    "    plt.plot(history['val_loss'], label='Test Loss', color='#ff7f0e', linestyle='--', linewidth=2)\n",
    "    plt.title('Train vs Test Loss', fontsize=14)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    cm = confusion_matrix(best_y_true, best_y_pred, labels=range(len(class_names)))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, cbar=False, square=True)\n",
    "    plt.title('Test Confusion Matrix', fontsize=14)\n",
    "    plt.xlabel('Predicted', fontsize=12)\n",
    "    plt.ylabel('True', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    output_path = os.path.join(results_dir, f'{model_name}_training_metrics_{counter}.png')\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    loss_chart = {\n",
    "        \"type\": \"line\",\n",
    "        \"data\": {\n",
    "            \"labels\": [str(i+1) for i in range(len(history['loss']))],\n",
    "            \"datasets\": [\n",
    "                {\"label\": \"Train Loss\", \"data\": history['loss'], \"borderColor\": \"rgba(31, 119, 180, 1)\", \"fill\": False},\n",
    "                {\"label\": \"Test Loss\", \"data\": history['val_loss'], \"borderColor\": \"rgba(255, 127, 14, 1)\", \"fill\": False, \"borderDash\": [5, 5]}\n",
    "            ]\n",
    "        },\n",
    "        \"options\": {\n",
    "            \"responsive\": True,\n",
    "            \"plugins\": {\"title\": {\"display\": True, \"text\": \"Train vs Test Loss\"}},\n",
    "            \"scales\": {\"x\": {\"title\": {\"display\": True, \"text\": \"Epoch\"}}, \"y\": {\"title\": {\"display\": True, \"text\": \"Loss\"}}}\n",
    "        }\n",
    "    }\n",
    "    accuracy_chart = {\n",
    "        \"type\": \"line\",\n",
    "        \"data\": {\n",
    "            \"labels\": [str(i+1) for i in range(len(history['accuracy']))],\n",
    "            \"datasets\": [\n",
    "                {\"label\": \"Train Accuracy\", \"data\": history['accuracy'], \"borderColor\": \"rgba(31, 119, 180, 1)\", \"fill\": False},\n",
    "                {\"label\": \"Test Accuracy\", \"data\": history['val_accuracy'], \"borderColor\": \"rgba(255, 127, 14, 1)\", \"fill\": False, \"borderDash\": [5, 5]}\n",
    "            ]\n",
    "        },\n",
    "        \"options\": {\n",
    "            \"responsive\": True,\n",
    "            \"plugins\": {\"title\": {\"display\": True, \"text\": \"Train vs Test Accuracy\"}},\n",
    "            \"scales\": {\"x\": {\"title\": {\"display\": True, \"text\": \"Epoch\"}}, \"y\": {\"title\": {\"display\": True, \"text\": \"Accuracy\"}, \"max\": 1}}\n",
    "        }\n",
    "    }\n",
    "    with open(os.path.join(results_dir, f'{model_name}_loss_chart_{counter}.json'), 'w') as f:\n",
    "        json.dump(loss_chart, f, cls=NumpyEncoder, indent=2)\n",
    "    with open(os.path.join(results_dir, f'{model_name}_accuracy_chart_{counter}.json'), 'w') as f:\n",
    "        json.dump(accuracy_chart, f, cls=NumpyEncoder, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341c2b8e",
   "metadata": {},
   "source": [
    "Data Generator for Imbalance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a979f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5514 images belonging to 3 classes.\n",
      "Found 614 images belonging to 3 classes.\n",
      "Found 5872 images belonging to 4 classes.\n",
      "Found 656 images belonging to 4 classes.\n",
      "Found 17082 images belonging to 4 classes.\n",
      "Found 1902 images belonging to 4 classes.\n",
      "Total dataset1 images: 6128\n",
      "Total dataset2 images: 6528\n",
      "Total dataset Combined images: 18984\n"
     ]
    }
   ],
   "source": [
    "# Define parameters\n",
    "target_size = (224, 224)\n",
    "batch_size = 16\n",
    "EPOCHS_INITIAL = 5\n",
    "\n",
    "# Define dataset directories\n",
    "imbalance_dataset1_train_dir = r'E:\\Data001\\SplitData\\train'\n",
    "imbalance_dataset1_test_dir = r'E:\\Data001\\SplitData\\test'\n",
    "imbalance_dataset2_train_dir = r'E:\\Data002\\SplitData\\train'\n",
    "imbalance_dataset2_test_dir = r'E:\\Data002\\SplitData\\test'\n",
    "imbalance_combined_train_dir = r'E:\\CombineData001_002\\SplitData\\train'\n",
    "imbalance_combined_test_dir = r'E:\\CombineData001_002\\SplitData\\test'\n",
    "\n",
    "# Data augmentation for training\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1/255,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Dataset001 Generators\n",
    "imbalance_train_generator1 = datagen.flow_from_directory(\n",
    "    directory=imbalance_dataset1_train_dir,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "imbalance_test_generator1 = datagen.flow_from_directory(\n",
    "    directory=imbalance_dataset1_test_dir,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Dataset002 Generators\n",
    "imbalance_train_generator2 = datagen.flow_from_directory(\n",
    "    directory=imbalance_dataset2_train_dir,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "imbalance_test_generator2 = datagen.flow_from_directory(\n",
    "    directory=imbalance_dataset2_test_dir,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# CombinedDataset Generators\n",
    "imbalance_train_generator_combined = datagen.flow_from_directory(\n",
    "    directory=imbalance_combined_train_dir,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "imbalance_test_generator_combined = datagen.flow_from_directory(\n",
    "    directory=imbalance_combined_test_dir,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Calculate total images using generator .n attribute\n",
    "dataset1 = imbalance_train_generator1.n + imbalance_test_generator1.n\n",
    "dataset2 = imbalance_train_generator2.n + imbalance_test_generator2.n\n",
    "combineddataset = imbalance_train_generator_combined.n + imbalance_test_generator_combined.n\n",
    "\n",
    "print(f\"Total dataset1 images: {dataset1}\")\n",
    "print(f\"Total dataset2 images: {dataset2}\")\n",
    "print(f\"Total dataset Combined images: {combineddataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8573f9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1911 images belonging to 3 classes.\n",
      "Found 213 images belonging to 3 classes.\n",
      "Found 1800 images belonging to 4 classes.\n",
      "Found 200 images belonging to 4 classes.\n",
      "Found 4446 images belonging to 4 classes.\n",
      "Found 413 images belonging to 4 classes.\n",
      "Total dataset1 images: 2124\n",
      "Total dataset2 images: 2000\n",
      "Total dataset Combined images: 4859\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "target_size = (224, 224)\n",
    "batch_size = 16\n",
    "EPOCHS_INITIAL = 5\n",
    "\n",
    "balance_dataset1_train_dir = r'E:\\Data001\\SplitData1\\train'\n",
    "balance_dataset1_test_dir = r'E:\\Data001\\SplitData1\\test'\n",
    "balance_dataset2_train_dir = r'E:\\Data002\\SplitData1\\train'\n",
    "balance_dataset2_test_dir = r'E:\\Data002\\SplitData1\\test'\n",
    "balance_combined_train_dir = r'E:\\CombineData001_002\\SplitData1\\train'\n",
    "balance_combined_test_dir = r'E:\\CombineData001_002\\SplitData1\\test'\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1/255,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Dataset001 Generators\n",
    "balance_train_generator1 = datagen.flow_from_directory(\n",
    "    directory=balance_dataset1_train_dir,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "balance_test_generator1 = datagen.flow_from_directory(\n",
    "    directory=balance_dataset1_test_dir,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Dataset002 Generators\n",
    "balance_train_generator2 = datagen.flow_from_directory(\n",
    "    directory=balance_dataset2_train_dir,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "balance_test_generator2 = datagen.flow_from_directory(\n",
    "    directory=balance_dataset2_test_dir,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# CombinedDataset Generators\n",
    "balance_train_generator_combined = datagen.flow_from_directory(\n",
    "    directory=balance_combined_train_dir,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "balance_test_generator_combined = datagen.flow_from_directory(\n",
    "    directory=balance_combined_test_dir,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Calculate total images using generator .n attribute\n",
    "dataset1 = balance_train_generator1.n + balance_test_generator1.n\n",
    "dataset2 = balance_train_generator2.n + balance_test_generator2.n\n",
    "combineddataset = balance_train_generator_combined.n + balance_test_generator_combined.n\n",
    "\n",
    "print(f\"Total dataset1 images: {dataset1}\")\n",
    "print(f\"Total dataset2 images: {dataset2}\")\n",
    "print(f\"Total dataset Combined images: {combineddataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359344f7",
   "metadata": {},
   "source": [
    "Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fab8c96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mobdensenet(\n",
    "    num_classes,\n",
    "    input_shape=(224, 224, 3),\n",
    "    model_name=\"MobDenseNet\",\n",
    "    activation_function=\"relu\",\n",
    "    kernel_initializer=\"glorot_uniform\",\n",
    "    output_activation=\"softmax\",\n",
    "    output_kernel_initializer=\"random_uniform\",\n",
    "    dropout_rate=0.2,\n",
    "    learning_rate=0.002,\n",
    "    optimizer=\"nadam\",\n",
    "):\n",
    "\n",
    "    raw_input = Input(shape=input_shape)\n",
    "\n",
    "    mob_model = applications.MobileNet(\n",
    "        input_shape=input_shape,\n",
    "        alpha=1.0,\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        pooling=None\n",
    "    )\n",
    "    mob_out = mob_model(raw_input)  \n",
    "    mob_gap = GlobalAveragePooling2D()(mob_out)\n",
    "\n",
    "    dense_model = applications.DenseNet121(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        input_shape=input_shape,\n",
    "        pooling=None\n",
    "    )\n",
    "    dense_out = dense_model(raw_input)\n",
    "    dense_gap = GlobalAveragePooling2D()(dense_out)\n",
    "\n",
    "\n",
    "    combined = Concatenate()([mob_gap, dense_gap])\n",
    "    dropout = Dropout(dropout_rate)(combined)\n",
    "    output = Dense(\n",
    "        num_classes,\n",
    "        activation=output_activation,\n",
    "        kernel_initializer=output_kernel_initializer\n",
    "    )(dropout)\n",
    "\n",
    "    model = Model(inputs=raw_input, outputs=output, name=f\"{model_name}_{num_classes}class\")\n",
    "\n",
    "    print(f\"\\nModel Summary for {num_classes}-Class Dataset:\")\n",
    "    model.summary()\n",
    "\n",
    "    print(f\"\\nHyper-Parameters ({num_classes}-Class Model):\")\n",
    "    print(\"| Hyper-Parameter Name        | Value            |\")\n",
    "    print(\"|----------------------------|------------------|\")\n",
    "    print(f\"| Activation Function         | {activation_function}              |\")\n",
    "    print(f\"| Kernel Initializer          | {kernel_initializer}   |\")\n",
    "    print(f\"| Initial Learning Rate       | {learning_rate}            |\")\n",
    "    print(f\"| Optimizer                   | {optimizer}            |\")\n",
    "    print(f\"| Batch Size                  | {batch_size}               |\")\n",
    "    print(f\"| Epochs                      | {EPOCHS_INITIAL}               |\")\n",
    "    print(f\"| Train-test Split            | {train_test_split}           |\")\n",
    "    print(f\"| Output Activation Function  | {output_activation}            |\")\n",
    "    print(f\"| Output Kernel Initializer   | {output_kernel_initializer} |\")\n",
    "    print(f\"| Number of Output Classes    | {num_classes}                |\")\n",
    "    print(f\"| Dropout Rate                | {dropout_rate}               |\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b53d9d",
   "metadata": {},
   "source": [
    "Verify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "523598db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+--------------------+----------------------+-------------------+--------------------+\n",
      "| Dataset                    |   Training Samples |   Validation Samples |   Steps per Epoch |   Validation Steps |\n",
      "+============================+====================+======================+===================+====================+\n",
      "| Imbalance Dataset 1        |               5514 |                  614 |               344 |                 38 |\n",
      "+----------------------------+--------------------+----------------------+-------------------+--------------------+\n",
      "| Imbalance Dataset 2        |               5872 |                  656 |               367 |                 41 |\n",
      "+----------------------------+--------------------+----------------------+-------------------+--------------------+\n",
      "| Imbalance Combined Dataset |              17082 |                 1902 |              1067 |                118 |\n",
      "+----------------------------+--------------------+----------------------+-------------------+--------------------+\n",
      "| Balanced Dataset 1         |               1911 |                  213 |               119 |                 13 |\n",
      "+----------------------------+--------------------+----------------------+-------------------+--------------------+\n",
      "| Balanced Dataset 2         |               1800 |                  200 |               112 |                 12 |\n",
      "+----------------------------+--------------------+----------------------+-------------------+--------------------+\n",
      "| Balanced Combined Dataset  |               4446 |                  413 |               277 |                 25 |\n",
      "+----------------------------+--------------------+----------------------+-------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "# Calculate steps per epoch and validation steps for each dataset\n",
    "steps_per_epoch1 = max(1, imbalance_train_generator1.samples // batch_size)\n",
    "validation_steps1 = max(1, imbalance_test_generator1.samples // batch_size)\n",
    "\n",
    "steps_per_epoch2 = max(1, imbalance_train_generator2.samples // batch_size)\n",
    "validation_steps2 = max(1, imbalance_test_generator2.samples // batch_size)\n",
    "\n",
    "steps_per_epoch3 = max(1, imbalance_train_generator_combined.samples // batch_size)\n",
    "validation_steps3 = max(1, imbalance_test_generator_combined.samples // batch_size)\n",
    "\n",
    "steps_per_epoch4 = max(1, balance_train_generator1.samples // batch_size)\n",
    "validation_steps4 = max(1, balance_test_generator1.samples // batch_size)\n",
    "\n",
    "steps_per_epoch5 = max(1, balance_train_generator2.samples // batch_size)\n",
    "validation_steps5 = max(1, balance_test_generator2.samples // batch_size)\n",
    "\n",
    "steps_per_epoch6 = max(1, balance_train_generator_combined.samples // batch_size)\n",
    "validation_steps6 = max(1, balance_test_generator_combined.samples // batch_size)\n",
    "\n",
    "table_data = [\n",
    "    [\"Imbalance Dataset 1\", imbalance_train_generator1.samples, imbalance_test_generator1.samples,\n",
    "     steps_per_epoch1, validation_steps1],\n",
    "    [\"Imbalance Dataset 2\", imbalance_train_generator2.samples, imbalance_test_generator2.samples,\n",
    "     steps_per_epoch2, validation_steps2],\n",
    "    [\"Imbalance Combined Dataset\", imbalance_train_generator_combined.samples, imbalance_test_generator_combined.samples,\n",
    "     steps_per_epoch3, validation_steps3],\n",
    "    [\"Balanced Dataset 1\", balance_train_generator1.samples, balance_test_generator1.samples,\n",
    "     steps_per_epoch4, validation_steps4],\n",
    "    [\"Balanced Dataset 2\", balance_train_generator2.samples, balance_test_generator2.samples,\n",
    "     steps_per_epoch5, validation_steps5],\n",
    "    [\"Balanced Combined Dataset\", balance_train_generator_combined.samples, balance_test_generator_combined.samples,\n",
    "     steps_per_epoch6, validation_steps6]\n",
    "]\n",
    "\n",
    "headers = [\"Dataset\", \"Training Samples\", \"Validation Samples\", \"Steps per Epoch\", \"Validation Steps\"]\n",
    "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058ba604",
   "metadata": {},
   "source": [
    "Train Dataset 1 with Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bb69e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Sizes and Steps:\n",
      "+----------------------------+--------------------+----------------------+-------------------+--------------------+---------------+\n",
      "| Dataset                    |   Training Samples |   Validation Samples |   Steps per Epoch |   Validation Steps |   Num Classes |\n",
      "+============================+====================+======================+===================+====================+===============+\n",
      "| Imbalance Dataset 1        |               2757 |                  307 |               172 |                 19 |             3 |\n",
      "+----------------------------+--------------------+----------------------+-------------------+--------------------+---------------+\n",
      "| Imbalance Dataset 2        |               2936 |                  328 |               183 |                 20 |             4 |\n",
      "+----------------------------+--------------------+----------------------+-------------------+--------------------+---------------+\n",
      "| Imbalance Combined Dataset |               5693 |                  635 |               355 |                 39 |             4 |\n",
      "+----------------------------+--------------------+----------------------+-------------------+--------------------+---------------+\n",
      "| Balanced Dataset 1         |               1911 |                  213 |               119 |                 13 |             3 |\n",
      "+----------------------------+--------------------+----------------------+-------------------+--------------------+---------------+\n",
      "| Balanced Dataset 2         |               1800 |                  200 |               112 |                 12 |             4 |\n",
      "+----------------------------+--------------------+----------------------+-------------------+--------------------+---------------+\n",
      "| Balanced Combined Dataset  |               3711 |                  413 |               231 |                 25 |             4 |\n",
      "+----------------------------+--------------------+----------------------+-------------------+--------------------+---------------+\n",
      "\n",
      "Training on Imbalance Dataset 1...\n",
      "\n",
      "Model Summary for 3-Class Dataset:\n",
      "Model: \"MobDenseNet_3class\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " mobilenet_1.00_224 (Functional  (None, 7, 7, 1024)  3228864     ['input_1[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " densenet121 (Functional)       (None, 7, 7, 1024)   7037504     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 1024)        0           ['mobilenet_1.00_224[0][0]']     \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " global_average_pooling2d_1 (Gl  (None, 1024)        0           ['densenet121[0][0]']            \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 2048)         0           ['global_average_pooling2d[0][0]'\n",
      "                                                                 , 'global_average_pooling2d_1[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 2048)         0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 3)            6147        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10,272,515\n",
      "Trainable params: 10,166,979\n",
      "Non-trainable params: 105,536\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "Hyper-Parameters (3-Class Model):\n",
      "| Hyper-Parameter Name        | Value            |\n",
      "|----------------------------|------------------|\n",
      "| Activation Function         | relu              |\n",
      "| Kernel Initializer          | glorot_uniform   |\n",
      "| Initial Learning Rate       | 0.002            |\n",
      "| Optimizer                   | nadam            |\n",
      "| Batch Size                  | 16               |\n",
      "| Epochs                      | 100               |\n",
      "| Train-test Split            | <function train_test_split at 0x00000233C97E4820>           |\n",
      "| Output Activation Function  | softmax            |\n",
      "| Output Kernel Initializer   | random_uniform |\n",
      "| Number of Output Classes    | 3                |\n",
      "| Dropout Rate                | 0.2               |\n",
      "Epoch 1/100\n",
      "172/172 [==============================] - 163s 659ms/step - loss: 0.3888 - accuracy: 0.8453 - val_loss: 0.8070 - val_accuracy: 0.6842 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "172/172 [==============================] - 106s 617ms/step - loss: 0.2111 - accuracy: 0.9146 - val_loss: 0.3529 - val_accuracy: 0.8750 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "172/172 [==============================] - 107s 618ms/step - loss: 0.1756 - accuracy: 0.9340 - val_loss: 0.1146 - val_accuracy: 0.9605 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "172/172 [==============================] - 100s 578ms/step - loss: 0.1315 - accuracy: 0.9489 - val_loss: 0.1405 - val_accuracy: 0.9507 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "172/172 [==============================] - 100s 577ms/step - loss: 0.0886 - accuracy: 0.9672 - val_loss: 0.2226 - val_accuracy: 0.9276 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "172/172 [==============================] - 106s 613ms/step - loss: 0.0909 - accuracy: 0.9617 - val_loss: 0.0746 - val_accuracy: 0.9671 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "172/172 [==============================] - 106s 613ms/step - loss: 0.0924 - accuracy: 0.9675 - val_loss: 0.0352 - val_accuracy: 0.9901 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "172/172 [==============================] - 99s 576ms/step - loss: 0.0833 - accuracy: 0.9723 - val_loss: 0.0674 - val_accuracy: 0.9737 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "172/172 [==============================] - 100s 577ms/step - loss: 0.0578 - accuracy: 0.9792 - val_loss: 0.0471 - val_accuracy: 0.9836 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "172/172 [==============================] - 99s 576ms/step - loss: 0.0563 - accuracy: 0.9803 - val_loss: 0.0948 - val_accuracy: 0.9671 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "172/172 [==============================] - 99s 576ms/step - loss: 0.0694 - accuracy: 0.9745 - val_loss: 0.1083 - val_accuracy: 0.9507 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "172/172 [==============================] - 99s 576ms/step - loss: 0.0506 - accuracy: 0.9814 - val_loss: 0.0821 - val_accuracy: 0.9737 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "172/172 [==============================] - 99s 574ms/step - loss: 0.0580 - accuracy: 0.9814 - val_loss: 0.0951 - val_accuracy: 0.9704 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "172/172 [==============================] - 99s 574ms/step - loss: 0.0614 - accuracy: 0.9770 - val_loss: 0.0599 - val_accuracy: 0.9770 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "172/172 [==============================] - 99s 576ms/step - loss: 0.0454 - accuracy: 0.9869 - val_loss: 0.0495 - val_accuracy: 0.9868 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "172/172 [==============================] - 106s 617ms/step - loss: 0.0395 - accuracy: 0.9861 - val_loss: 0.0310 - val_accuracy: 0.9901 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "172/172 [==============================] - 99s 575ms/step - loss: 0.0355 - accuracy: 0.9898 - val_loss: 0.0364 - val_accuracy: 0.9803 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "172/172 [==============================] - 108s 626ms/step - loss: 0.0354 - accuracy: 0.9876 - val_loss: 0.0879 - val_accuracy: 0.9671 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "172/172 [==============================] - 106s 615ms/step - loss: 0.0332 - accuracy: 0.9883 - val_loss: 0.0254 - val_accuracy: 0.9934 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "172/172 [==============================] - 99s 573ms/step - loss: 0.0361 - accuracy: 0.9858 - val_loss: 0.0285 - val_accuracy: 0.9934 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "172/172 [==============================] - 105s 609ms/step - loss: 0.0321 - accuracy: 0.9905 - val_loss: 0.0251 - val_accuracy: 0.9901 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "172/172 [==============================] - 99s 575ms/step - loss: 0.0314 - accuracy: 0.9898 - val_loss: 0.0372 - val_accuracy: 0.9868 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "172/172 [==============================] - 103s 596ms/step - loss: 0.0372 - accuracy: 0.9861 - val_loss: 0.0786 - val_accuracy: 0.9770 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "172/172 [==============================] - 106s 616ms/step - loss: 0.0229 - accuracy: 0.9920 - val_loss: 0.0403 - val_accuracy: 0.9836 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "172/172 [==============================] - 105s 610ms/step - loss: 0.0385 - accuracy: 0.9865 - val_loss: 0.0616 - val_accuracy: 0.9803 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "172/172 [==============================] - 107s 620ms/step - loss: 0.0193 - accuracy: 0.9931 - val_loss: 0.0303 - val_accuracy: 0.9868 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "172/172 [==============================] - 106s 615ms/step - loss: 0.0209 - accuracy: 0.9909 - val_loss: 0.0672 - val_accuracy: 0.9770 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "172/172 [==============================] - 112s 650ms/step - loss: 0.0210 - accuracy: 0.9927 - val_loss: 0.0096 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "172/172 [==============================] - 107s 619ms/step - loss: 0.0228 - accuracy: 0.9927 - val_loss: 0.0606 - val_accuracy: 0.9836 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "172/172 [==============================] - 105s 609ms/step - loss: 0.0273 - accuracy: 0.9912 - val_loss: 0.0256 - val_accuracy: 0.9836 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "172/172 [==============================] - 106s 616ms/step - loss: 0.0169 - accuracy: 0.9938 - val_loss: 0.0365 - val_accuracy: 0.9836 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "172/172 [==============================] - 103s 595ms/step - loss: 0.0306 - accuracy: 0.9891 - val_loss: 0.0218 - val_accuracy: 0.9934 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "172/172 [==============================] - 95s 550ms/step - loss: 0.0315 - accuracy: 0.9909 - val_loss: 0.0457 - val_accuracy: 0.9868 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "172/172 [==============================] - 95s 553ms/step - loss: 0.0455 - accuracy: 0.9847 - val_loss: 0.1481 - val_accuracy: 0.9539 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "172/172 [==============================] - 95s 550ms/step - loss: 0.0192 - accuracy: 0.9920 - val_loss: 0.0449 - val_accuracy: 0.9836 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "172/172 [==============================] - 95s 548ms/step - loss: 0.0292 - accuracy: 0.9920 - val_loss: 0.0292 - val_accuracy: 0.9901 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "172/172 [==============================] - 94s 544ms/step - loss: 0.0369 - accuracy: 0.9898 - val_loss: 0.0505 - val_accuracy: 0.9836 - lr: 1.0000e-04\n",
      "Epoch 38/100\n",
      "172/172 [==============================] - 96s 555ms/step - loss: 0.0191 - accuracy: 0.9916 - val_loss: 0.0275 - val_accuracy: 0.9836 - lr: 1.0000e-04\n",
      "Epoch 39/100\n",
      "172/172 [==============================] - 94s 544ms/step - loss: 0.0303 - accuracy: 0.9901 - val_loss: 0.0648 - val_accuracy: 0.9770 - lr: 1.0000e-04\n",
      "Epoch 40/100\n",
      "172/172 [==============================] - 94s 543ms/step - loss: 0.0231 - accuracy: 0.9905 - val_loss: 0.0166 - val_accuracy: 0.9934 - lr: 1.0000e-04\n",
      "Epoch 41/100\n",
      "172/172 [==============================] - 96s 556ms/step - loss: 0.0183 - accuracy: 0.9934 - val_loss: 0.0227 - val_accuracy: 0.9901 - lr: 1.0000e-04\n",
      "Epoch 42/100\n",
      "172/172 [==============================] - 94s 542ms/step - loss: 0.0077 - accuracy: 0.9982 - val_loss: 0.0132 - val_accuracy: 0.9967 - lr: 1.0000e-04\n",
      "Epoch 43/100\n",
      "172/172 [==============================] - 96s 556ms/step - loss: 0.0264 - accuracy: 0.9923 - val_loss: 0.0475 - val_accuracy: 0.9836 - lr: 1.0000e-04\n",
      "20/20 [==============================] - 12s 381ms/step\n",
      "20/20 [==============================] - 8s 375ms/step - loss: 0.0168 - accuracy: 0.9935\n",
      "\n",
      "Classification Report for Imbalance Dataset 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Glioma       0.99      1.00      0.99       143\n",
      "  Meningioma       0.99      0.97      0.98        71\n",
      "   Pituitary       1.00      0.99      0.99        93\n",
      "\n",
      "    accuracy                           0.99       307\n",
      "   macro avg       0.99      0.99      0.99       307\n",
      "weighted avg       0.99      0.99      0.99       307\n",
      "\n",
      "Model saved successfully to E:\\Data001\\SplitData\\hybrid_MobDenseNet_Output_using_tensorflow\\imbalanceoutput\\mobdensenet_1.h5\n",
      "[Imbalance Dataset 1] Processing completed.\n",
      "\n",
      "Training on Imbalance Dataset 2...\n",
      "\n",
      "Model Summary for 4-Class Dataset:\n",
      "Model: \"MobDenseNet_4class\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " mobilenet_1.00_224 (Functional  (None, 7, 7, 1024)  3228864     ['input_1[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " densenet121 (Functional)       (None, 7, 7, 1024)   7037504     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 1024)        0           ['mobilenet_1.00_224[0][0]']     \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " global_average_pooling2d_1 (Gl  (None, 1024)        0           ['densenet121[0][0]']            \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 2048)         0           ['global_average_pooling2d[0][0]'\n",
      "                                                                 , 'global_average_pooling2d_1[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 2048)         0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 4)            8196        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10,274,564\n",
      "Trainable params: 10,169,028\n",
      "Non-trainable params: 105,536\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "Hyper-Parameters (4-Class Model):\n",
      "| Hyper-Parameter Name        | Value            |\n",
      "|----------------------------|------------------|\n",
      "| Activation Function         | relu              |\n",
      "| Kernel Initializer          | glorot_uniform   |\n",
      "| Initial Learning Rate       | 0.002            |\n",
      "| Optimizer                   | nadam            |\n",
      "| Batch Size                  | 16               |\n",
      "| Epochs                      | 100               |\n",
      "| Train-test Split            | <function train_test_split at 0x00000233C97E4820>           |\n",
      "| Output Activation Function  | softmax            |\n",
      "| Output Kernel Initializer   | random_uniform |\n",
      "| Number of Output Classes    | 4                |\n",
      "| Dropout Rate                | 0.2               |\n",
      "Epoch 1/100\n",
      "183/183 [==============================] - 170s 780ms/step - loss: 0.5574 - accuracy: 0.7949 - val_loss: 0.7992 - val_accuracy: 0.7188 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "183/183 [==============================] - 102s 552ms/step - loss: 0.3022 - accuracy: 0.8904 - val_loss: 0.3775 - val_accuracy: 0.8750 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "183/183 [==============================] - 100s 544ms/step - loss: 0.2364 - accuracy: 0.9168 - val_loss: 0.1866 - val_accuracy: 0.9312 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "183/183 [==============================] - 100s 544ms/step - loss: 0.1876 - accuracy: 0.9301 - val_loss: 0.1404 - val_accuracy: 0.9563 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "183/183 [==============================] - 107s 585ms/step - loss: 0.1542 - accuracy: 0.9449 - val_loss: 0.1712 - val_accuracy: 0.9438 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "183/183 [==============================] - 106s 579ms/step - loss: 0.1203 - accuracy: 0.9555 - val_loss: 0.1117 - val_accuracy: 0.9594 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "183/183 [==============================] - 103s 563ms/step - loss: 0.1285 - accuracy: 0.9521 - val_loss: 0.1072 - val_accuracy: 0.9563 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "183/183 [==============================] - 99s 540ms/step - loss: 0.1197 - accuracy: 0.9606 - val_loss: 0.1753 - val_accuracy: 0.9375 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "183/183 [==============================] - 102s 558ms/step - loss: 0.0940 - accuracy: 0.9682 - val_loss: 0.0849 - val_accuracy: 0.9688 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "183/183 [==============================] - 99s 539ms/step - loss: 0.0846 - accuracy: 0.9719 - val_loss: 0.1010 - val_accuracy: 0.9719 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "183/183 [==============================] - 97s 529ms/step - loss: 0.0741 - accuracy: 0.9747 - val_loss: 0.1231 - val_accuracy: 0.9594 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "183/183 [==============================] - 95s 513ms/step - loss: 0.0706 - accuracy: 0.9750 - val_loss: 0.1277 - val_accuracy: 0.9656 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "183/183 [==============================] - 92s 502ms/step - loss: 0.0580 - accuracy: 0.9808 - val_loss: 0.1104 - val_accuracy: 0.9594 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "183/183 [==============================] - 94s 511ms/step - loss: 0.0699 - accuracy: 0.9747 - val_loss: 0.1635 - val_accuracy: 0.9625 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "183/183 [==============================] - 93s 508ms/step - loss: 0.0751 - accuracy: 0.9750 - val_loss: 0.0528 - val_accuracy: 0.9812 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "183/183 [==============================] - 97s 528ms/step - loss: 0.0650 - accuracy: 0.9791 - val_loss: 0.0772 - val_accuracy: 0.9688 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "183/183 [==============================] - 95s 521ms/step - loss: 0.0588 - accuracy: 0.9788 - val_loss: 0.0983 - val_accuracy: 0.9656 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "183/183 [==============================] - 94s 510ms/step - loss: 0.0563 - accuracy: 0.9808 - val_loss: 0.1013 - val_accuracy: 0.9656 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "183/183 [==============================] - 94s 509ms/step - loss: 0.0484 - accuracy: 0.9832 - val_loss: 0.0708 - val_accuracy: 0.9719 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "183/183 [==============================] - 93s 506ms/step - loss: 0.0446 - accuracy: 0.9842 - val_loss: 0.1077 - val_accuracy: 0.9656 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "183/183 [==============================] - 93s 509ms/step - loss: 0.0367 - accuracy: 0.9870 - val_loss: 0.0493 - val_accuracy: 0.9844 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "183/183 [==============================] - 102s 555ms/step - loss: 0.0441 - accuracy: 0.9863 - val_loss: 0.0905 - val_accuracy: 0.9781 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "183/183 [==============================] - 106s 579ms/step - loss: 0.0461 - accuracy: 0.9825 - val_loss: 0.0965 - val_accuracy: 0.9688 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "183/183 [==============================] - 110s 598ms/step - loss: 0.0418 - accuracy: 0.9853 - val_loss: 0.1264 - val_accuracy: 0.9625 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "183/183 [==============================] - 110s 600ms/step - loss: 0.0404 - accuracy: 0.9860 - val_loss: 0.1824 - val_accuracy: 0.9312 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "183/183 [==============================] - 112s 611ms/step - loss: 0.0490 - accuracy: 0.9849 - val_loss: 0.0632 - val_accuracy: 0.9750 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "183/183 [==============================] - 110s 600ms/step - loss: 0.0459 - accuracy: 0.9842 - val_loss: 0.1510 - val_accuracy: 0.9656 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "183/183 [==============================] - 111s 602ms/step - loss: 0.0614 - accuracy: 0.9788 - val_loss: 0.0905 - val_accuracy: 0.9656 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "183/183 [==============================] - 111s 602ms/step - loss: 0.0471 - accuracy: 0.9832 - val_loss: 0.0964 - val_accuracy: 0.9656 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "183/183 [==============================] - 104s 564ms/step - loss: 0.0308 - accuracy: 0.9914 - val_loss: 0.0661 - val_accuracy: 0.9875 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "183/183 [==============================] - 96s 523ms/step - loss: 0.0313 - accuracy: 0.9877 - val_loss: 0.0699 - val_accuracy: 0.9844 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "183/183 [==============================] - 95s 516ms/step - loss: 0.0543 - accuracy: 0.9798 - val_loss: 0.0534 - val_accuracy: 0.9719 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "183/183 [==============================] - 97s 529ms/step - loss: 0.0556 - accuracy: 0.9825 - val_loss: 0.0648 - val_accuracy: 0.9812 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "183/183 [==============================] - 96s 519ms/step - loss: 0.0293 - accuracy: 0.9877 - val_loss: 0.0811 - val_accuracy: 0.9656 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "183/183 [==============================] - 96s 525ms/step - loss: 0.0389 - accuracy: 0.9866 - val_loss: 0.0648 - val_accuracy: 0.9812 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "183/183 [==============================] - 96s 523ms/step - loss: 0.0370 - accuracy: 0.9887 - val_loss: 0.0915 - val_accuracy: 0.9719 - lr: 1.0000e-04\n",
      "21/21 [==============================] - 14s 412ms/step\n",
      "21/21 [==============================] - 9s 403ms/step - loss: 0.0774 - accuracy: 0.9756\n",
      "\n",
      "Classification Report for Imbalance Dataset 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Glioma       1.00      0.96      0.98        93\n",
      "  Meningioma       0.96      0.97      0.96        94\n",
      "   Pituitary       0.98      1.00      0.99        50\n",
      "    No-tumor       0.97      0.99      0.98        91\n",
      "\n",
      "    accuracy                           0.98       328\n",
      "   macro avg       0.98      0.98      0.98       328\n",
      "weighted avg       0.98      0.98      0.98       328\n",
      "\n",
      "Model saved successfully to E:\\Data002\\SplitData\\hybrid_MobDenseNet_Output_using_tensorflow\\imbalanceoutput\\mobdensenet_2.h5\n",
      "[Imbalance Dataset 2] Processing completed.\n",
      "\n",
      "Training on Imbalance Combined Dataset...\n",
      "\n",
      "Model Summary for 4-Class Dataset:\n",
      "Model: \"MobDenseNet_4class\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " mobilenet_1.00_224 (Functional  (None, 7, 7, 1024)  3228864     ['input_1[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " densenet121 (Functional)       (None, 7, 7, 1024)   7037504     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 1024)        0           ['mobilenet_1.00_224[0][0]']     \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " global_average_pooling2d_1 (Gl  (None, 1024)        0           ['densenet121[0][0]']            \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 2048)         0           ['global_average_pooling2d[0][0]'\n",
      "                                                                 , 'global_average_pooling2d_1[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 2048)         0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 4)            8196        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10,274,564\n",
      "Trainable params: 10,169,028\n",
      "Non-trainable params: 105,536\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "Hyper-Parameters (4-Class Model):\n",
      "| Hyper-Parameter Name        | Value            |\n",
      "|----------------------------|------------------|\n",
      "| Activation Function         | relu              |\n",
      "| Kernel Initializer          | glorot_uniform   |\n",
      "| Initial Learning Rate       | 0.002            |\n",
      "| Optimizer                   | nadam            |\n",
      "| Batch Size                  | 16               |\n",
      "| Epochs                      | 100               |\n",
      "| Train-test Split            | <function train_test_split at 0x00000233C97E4820>           |\n",
      "| Output Activation Function  | softmax            |\n",
      "| Output Kernel Initializer   | random_uniform |\n",
      "| Number of Output Classes    | 4                |\n",
      "| Dropout Rate                | 0.2               |\n",
      "Epoch 1/100\n",
      "355/355 [==============================] - 319s 805ms/step - loss: 0.4210 - accuracy: 0.8413 - val_loss: 0.3854 - val_accuracy: 0.8510 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "355/355 [==============================] - 192s 538ms/step - loss: 0.2203 - accuracy: 0.9253 - val_loss: 0.2717 - val_accuracy: 0.9071 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "355/355 [==============================] - 191s 536ms/step - loss: 0.1657 - accuracy: 0.9408 - val_loss: 0.1732 - val_accuracy: 0.9359 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "355/355 [==============================] - 189s 531ms/step - loss: 0.1417 - accuracy: 0.9505 - val_loss: 0.1515 - val_accuracy: 0.9487 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "355/355 [==============================] - 189s 530ms/step - loss: 0.0987 - accuracy: 0.9681 - val_loss: 0.1007 - val_accuracy: 0.9679 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "355/355 [==============================] - 189s 531ms/step - loss: 0.0936 - accuracy: 0.9651 - val_loss: 0.1126 - val_accuracy: 0.9663 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "355/355 [==============================] - 188s 529ms/step - loss: 0.0830 - accuracy: 0.9690 - val_loss: 0.2166 - val_accuracy: 0.9407 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "355/355 [==============================] - 208s 585ms/step - loss: 0.0724 - accuracy: 0.9759 - val_loss: 0.1404 - val_accuracy: 0.9583 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "355/355 [==============================] - 207s 581ms/step - loss: 0.0732 - accuracy: 0.9746 - val_loss: 0.1134 - val_accuracy: 0.9679 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "355/355 [==============================] - 188s 528ms/step - loss: 0.0606 - accuracy: 0.9796 - val_loss: 0.2275 - val_accuracy: 0.9295 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "355/355 [==============================] - 188s 528ms/step - loss: 0.0565 - accuracy: 0.9815 - val_loss: 0.1079 - val_accuracy: 0.9679 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "355/355 [==============================] - 187s 525ms/step - loss: 0.0568 - accuracy: 0.9810 - val_loss: 0.1134 - val_accuracy: 0.9663 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "355/355 [==============================] - 190s 533ms/step - loss: 0.0689 - accuracy: 0.9812 - val_loss: 0.2016 - val_accuracy: 0.9343 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "355/355 [==============================] - 187s 526ms/step - loss: 0.0481 - accuracy: 0.9841 - val_loss: 0.1432 - val_accuracy: 0.9599 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "355/355 [==============================] - 191s 538ms/step - loss: 0.0470 - accuracy: 0.9845 - val_loss: 0.1321 - val_accuracy: 0.9647 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "355/355 [==============================] - 187s 525ms/step - loss: 0.0503 - accuracy: 0.9847 - val_loss: 0.1541 - val_accuracy: 0.9551 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "355/355 [==============================] - 191s 538ms/step - loss: 0.0463 - accuracy: 0.9861 - val_loss: 0.1228 - val_accuracy: 0.9663 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "355/355 [==============================] - 202s 569ms/step - loss: 0.0384 - accuracy: 0.9866 - val_loss: 0.1594 - val_accuracy: 0.9535 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "355/355 [==============================] - 201s 566ms/step - loss: 0.0330 - accuracy: 0.9891 - val_loss: 0.0920 - val_accuracy: 0.9712 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "355/355 [==============================] - 211s 595ms/step - loss: 0.0342 - accuracy: 0.9887 - val_loss: 0.1729 - val_accuracy: 0.9631 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "355/355 [==============================] - 205s 577ms/step - loss: 0.0410 - accuracy: 0.9849 - val_loss: 0.1176 - val_accuracy: 0.9647 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "355/355 [==============================] - 20453s 58s/step - loss: 0.0303 - accuracy: 0.9908 - val_loss: 0.0838 - val_accuracy: 0.9776 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "355/355 [==============================] - 201s 565ms/step - loss: 0.0423 - accuracy: 0.9864 - val_loss: 0.0674 - val_accuracy: 0.9840 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "355/355 [==============================] - 196s 551ms/step - loss: 0.0323 - accuracy: 0.9894 - val_loss: 0.1071 - val_accuracy: 0.9696 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "355/355 [==============================] - 189s 531ms/step - loss: 0.0356 - accuracy: 0.9877 - val_loss: 0.0989 - val_accuracy: 0.9712 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "355/355 [==============================] - 192s 541ms/step - loss: 0.0294 - accuracy: 0.9887 - val_loss: 0.1247 - val_accuracy: 0.9679 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "355/355 [==============================] - 181s 509ms/step - loss: 0.0269 - accuracy: 0.9921 - val_loss: 0.1084 - val_accuracy: 0.9776 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "355/355 [==============================] - 191s 536ms/step - loss: 0.0340 - accuracy: 0.9880 - val_loss: 0.1138 - val_accuracy: 0.9728 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "355/355 [==============================] - 186s 523ms/step - loss: 0.0403 - accuracy: 0.9877 - val_loss: 0.1397 - val_accuracy: 0.9631 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "355/355 [==============================] - 191s 538ms/step - loss: 0.0373 - accuracy: 0.9870 - val_loss: 0.0975 - val_accuracy: 0.9776 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "355/355 [==============================] - 189s 531ms/step - loss: 0.0151 - accuracy: 0.9947 - val_loss: 0.1087 - val_accuracy: 0.9712 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "355/355 [==============================] - 193s 543ms/step - loss: 0.0298 - accuracy: 0.9886 - val_loss: 0.0975 - val_accuracy: 0.9824 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "355/355 [==============================] - 191s 536ms/step - loss: 0.0283 - accuracy: 0.9907 - val_loss: 0.1189 - val_accuracy: 0.9631 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "355/355 [==============================] - 189s 533ms/step - loss: 0.0208 - accuracy: 0.9935 - val_loss: 0.1167 - val_accuracy: 0.9712 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "355/355 [==============================] - 190s 534ms/step - loss: 0.0416 - accuracy: 0.9856 - val_loss: 0.1120 - val_accuracy: 0.9776 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "355/355 [==============================] - 187s 526ms/step - loss: 0.0314 - accuracy: 0.9917 - val_loss: 0.0806 - val_accuracy: 0.9792 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "355/355 [==============================] - 188s 527ms/step - loss: 0.0270 - accuracy: 0.9903 - val_loss: 0.1010 - val_accuracy: 0.9760 - lr: 1.0000e-04\n",
      "Epoch 38/100\n",
      "355/355 [==============================] - 189s 532ms/step - loss: 0.0271 - accuracy: 0.9912 - val_loss: 0.1384 - val_accuracy: 0.9696 - lr: 1.0000e-04\n",
      "40/40 [==============================] - 24s 467ms/step\n",
      "40/40 [==============================] - 18s 453ms/step - loss: 0.0795 - accuracy: 0.9732\n",
      "\n",
      "Classification Report for Imbalance Combined Dataset:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Glioma       0.99      0.98      0.98       236\n",
      "  Meningioma       0.98      0.96      0.97       165\n",
      "   Pituitary       0.96      0.98      0.97        50\n",
      "    No-tumor       0.98      0.99      0.99       184\n",
      "\n",
      "    accuracy                           0.98       635\n",
      "   macro avg       0.98      0.98      0.98       635\n",
      "weighted avg       0.98      0.98      0.98       635\n",
      "\n",
      "Model saved successfully to E:\\CombineData001_002\\SplitData\\hybrid_MobDenseNet_Output_using_tensorflow\\imbalanceoutput\\mobdensenet_3.h5\n",
      "[Imbalance Combined Dataset] Processing completed.\n",
      "\n",
      "Training on Balanced Dataset 1...\n",
      "\n",
      "Model Summary for 3-Class Dataset:\n",
      "Model: \"MobDenseNet_3class\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " mobilenet_1.00_224 (Functional  (None, 7, 7, 1024)  3228864     ['input_1[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " densenet121 (Functional)       (None, 7, 7, 1024)   7037504     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 1024)        0           ['mobilenet_1.00_224[0][0]']     \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " global_average_pooling2d_1 (Gl  (None, 1024)        0           ['densenet121[0][0]']            \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 2048)         0           ['global_average_pooling2d[0][0]'\n",
      "                                                                 , 'global_average_pooling2d_1[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 2048)         0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 3)            6147        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10,272,515\n",
      "Trainable params: 10,166,979\n",
      "Non-trainable params: 105,536\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "Hyper-Parameters (3-Class Model):\n",
      "| Hyper-Parameter Name        | Value            |\n",
      "|----------------------------|------------------|\n",
      "| Activation Function         | relu              |\n",
      "| Kernel Initializer          | glorot_uniform   |\n",
      "| Initial Learning Rate       | 0.002            |\n",
      "| Optimizer                   | nadam            |\n",
      "| Batch Size                  | 16               |\n",
      "| Epochs                      | 100               |\n",
      "| Train-test Split            | <function train_test_split at 0x00000233C97E4820>           |\n",
      "| Output Activation Function  | softmax            |\n",
      "| Output Kernel Initializer   | random_uniform |\n",
      "| Number of Output Classes    | 3                |\n",
      "| Dropout Rate                | 0.2               |\n",
      "Epoch 1/100\n",
      "119/119 [==============================] - 140s 873ms/step - loss: 0.5113 - accuracy: 0.8005 - val_loss: 0.7884 - val_accuracy: 0.6731 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "119/119 [==============================] - 67s 554ms/step - loss: 0.2169 - accuracy: 0.9077 - val_loss: 0.2459 - val_accuracy: 0.9087 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "119/119 [==============================] - 62s 520ms/step - loss: 0.1753 - accuracy: 0.9398 - val_loss: 0.4540 - val_accuracy: 0.8173 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "119/119 [==============================] - 70s 583ms/step - loss: 0.1466 - accuracy: 0.9478 - val_loss: 0.4305 - val_accuracy: 0.8125 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "119/119 [==============================] - 67s 557ms/step - loss: 0.1163 - accuracy: 0.9573 - val_loss: 0.1295 - val_accuracy: 0.9423 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "119/119 [==============================] - 66s 548ms/step - loss: 0.1129 - accuracy: 0.9588 - val_loss: 0.0567 - val_accuracy: 0.9760 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "119/119 [==============================] - 65s 542ms/step - loss: 0.0886 - accuracy: 0.9652 - val_loss: 0.0790 - val_accuracy: 0.9712 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "119/119 [==============================] - 65s 540ms/step - loss: 0.0718 - accuracy: 0.9689 - val_loss: 0.1207 - val_accuracy: 0.9471 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "119/119 [==============================] - 66s 554ms/step - loss: 0.0854 - accuracy: 0.9710 - val_loss: 0.2192 - val_accuracy: 0.9375 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "119/119 [==============================] - 64s 534ms/step - loss: 0.0559 - accuracy: 0.9778 - val_loss: 0.0725 - val_accuracy: 0.9663 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "119/119 [==============================] - 66s 553ms/step - loss: 0.0574 - accuracy: 0.9789 - val_loss: 0.1029 - val_accuracy: 0.9615 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "119/119 [==============================] - 67s 562ms/step - loss: 0.0679 - accuracy: 0.9757 - val_loss: 0.0757 - val_accuracy: 0.9808 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "119/119 [==============================] - 67s 561ms/step - loss: 0.0326 - accuracy: 0.9900 - val_loss: 0.0369 - val_accuracy: 0.9952 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "119/119 [==============================] - 66s 549ms/step - loss: 0.0630 - accuracy: 0.9741 - val_loss: 0.0971 - val_accuracy: 0.9615 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "119/119 [==============================] - 66s 548ms/step - loss: 0.0613 - accuracy: 0.9768 - val_loss: 0.0662 - val_accuracy: 0.9712 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "119/119 [==============================] - 65s 546ms/step - loss: 0.0291 - accuracy: 0.9910 - val_loss: 0.0360 - val_accuracy: 0.9808 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "119/119 [==============================] - 65s 546ms/step - loss: 0.0650 - accuracy: 0.9768 - val_loss: 0.3300 - val_accuracy: 0.8942 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "119/119 [==============================] - 66s 551ms/step - loss: 0.0491 - accuracy: 0.9815 - val_loss: 0.0739 - val_accuracy: 0.9760 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "119/119 [==============================] - 65s 543ms/step - loss: 0.0433 - accuracy: 0.9863 - val_loss: 0.0586 - val_accuracy: 0.9808 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "119/119 [==============================] - 67s 558ms/step - loss: 0.0281 - accuracy: 0.9905 - val_loss: 0.1003 - val_accuracy: 0.9808 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "119/119 [==============================] - 65s 540ms/step - loss: 0.0303 - accuracy: 0.9931 - val_loss: 0.1294 - val_accuracy: 0.9663 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "119/119 [==============================] - 65s 547ms/step - loss: 0.0310 - accuracy: 0.9910 - val_loss: 0.2671 - val_accuracy: 0.8942 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "119/119 [==============================] - 67s 557ms/step - loss: 0.0264 - accuracy: 0.9905 - val_loss: 0.1191 - val_accuracy: 0.9615 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "119/119 [==============================] - 67s 560ms/step - loss: 0.0287 - accuracy: 0.9905 - val_loss: 0.0787 - val_accuracy: 0.9760 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "119/119 [==============================] - 65s 542ms/step - loss: 0.0335 - accuracy: 0.9889 - val_loss: 0.0593 - val_accuracy: 0.9760 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "119/119 [==============================] - 65s 545ms/step - loss: 0.0211 - accuracy: 0.9937 - val_loss: 0.1227 - val_accuracy: 0.9567 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "119/119 [==============================] - 73s 607ms/step - loss: 0.0271 - accuracy: 0.9900 - val_loss: 0.0414 - val_accuracy: 0.9904 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "119/119 [==============================] - 69s 570ms/step - loss: 0.0336 - accuracy: 0.9879 - val_loss: 0.0641 - val_accuracy: 0.9760 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "119/119 [==============================] - 66s 555ms/step - loss: 0.0227 - accuracy: 0.9937 - val_loss: 0.0628 - val_accuracy: 0.9808 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "119/119 [==============================] - 66s 550ms/step - loss: 0.0451 - accuracy: 0.9831 - val_loss: 0.0480 - val_accuracy: 0.9856 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "119/119 [==============================] - 65s 544ms/step - loss: 0.0625 - accuracy: 0.9778 - val_loss: 0.0774 - val_accuracy: 0.9615 - lr: 1.0000e-04\n",
      "14/14 [==============================] - 10s 347ms/step\n",
      "14/14 [==============================] - 5s 342ms/step - loss: 0.0233 - accuracy: 0.9953\n",
      "\n",
      "Classification Report for Balanced Dataset 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Glioma       1.00      0.96      0.98        71\n",
      "  Meningioma       0.97      1.00      0.99        71\n",
      "   Pituitary       0.99      1.00      0.99        71\n",
      "\n",
      "    accuracy                           0.99       213\n",
      "   macro avg       0.99      0.99      0.99       213\n",
      "weighted avg       0.99      0.99      0.99       213\n",
      "\n",
      "Model saved successfully to E:\\Data001\\SplitData\\hybrid_MobDenseNet_Output_using_tensorflow\\balanceoutput\\mobdensenet_4.h5\n",
      "[Balanced Dataset 1] Processing completed.\n",
      "\n",
      "Training on Balanced Dataset 2...\n",
      "\n",
      "Model Summary for 4-Class Dataset:\n",
      "Model: \"MobDenseNet_4class\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " mobilenet_1.00_224 (Functional  (None, 7, 7, 1024)  3228864     ['input_1[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " densenet121 (Functional)       (None, 7, 7, 1024)   7037504     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 1024)        0           ['mobilenet_1.00_224[0][0]']     \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " global_average_pooling2d_1 (Gl  (None, 1024)        0           ['densenet121[0][0]']            \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 2048)         0           ['global_average_pooling2d[0][0]'\n",
      "                                                                 , 'global_average_pooling2d_1[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 2048)         0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 4)            8196        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10,274,564\n",
      "Trainable params: 10,169,028\n",
      "Non-trainable params: 105,536\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "Hyper-Parameters (4-Class Model):\n",
      "| Hyper-Parameter Name        | Value            |\n",
      "|----------------------------|------------------|\n",
      "| Activation Function         | relu              |\n",
      "| Kernel Initializer          | glorot_uniform   |\n",
      "| Initial Learning Rate       | 0.002            |\n",
      "| Optimizer                   | nadam            |\n",
      "| Batch Size                  | 16               |\n",
      "| Epochs                      | 100               |\n",
      "| Train-test Split            | <function train_test_split at 0x00000233C97E4820>           |\n",
      "| Output Activation Function  | softmax            |\n",
      "| Output Kernel Initializer   | random_uniform |\n",
      "| Number of Output Classes    | 4                |\n",
      "| Dropout Rate                | 0.2               |\n",
      "Epoch 1/100\n",
      "112/112 [==============================] - 108s 738ms/step - loss: 0.5071 - accuracy: 0.8145 - val_loss: 0.5149 - val_accuracy: 0.7656 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "112/112 [==============================] - 64s 570ms/step - loss: 0.2088 - accuracy: 0.9283 - val_loss: 0.6180 - val_accuracy: 0.7396 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "112/112 [==============================] - 65s 574ms/step - loss: 0.1661 - accuracy: 0.9395 - val_loss: 0.2959 - val_accuracy: 0.9062 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "112/112 [==============================] - 63s 563ms/step - loss: 0.1252 - accuracy: 0.9546 - val_loss: 0.0892 - val_accuracy: 0.9740 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "112/112 [==============================] - 62s 551ms/step - loss: 0.0983 - accuracy: 0.9686 - val_loss: 0.1617 - val_accuracy: 0.9479 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "112/112 [==============================] - 62s 557ms/step - loss: 0.0881 - accuracy: 0.9692 - val_loss: 0.0445 - val_accuracy: 0.9792 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "112/112 [==============================] - 64s 565ms/step - loss: 0.0640 - accuracy: 0.9770 - val_loss: 0.0239 - val_accuracy: 0.9948 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "112/112 [==============================] - 63s 556ms/step - loss: 0.0613 - accuracy: 0.9798 - val_loss: 0.0799 - val_accuracy: 0.9688 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "112/112 [==============================] - 62s 551ms/step - loss: 0.0568 - accuracy: 0.9776 - val_loss: 0.0242 - val_accuracy: 0.9844 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "112/112 [==============================] - 63s 561ms/step - loss: 0.0594 - accuracy: 0.9781 - val_loss: 0.0424 - val_accuracy: 0.9740 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "112/112 [==============================] - 63s 560ms/step - loss: 0.0719 - accuracy: 0.9776 - val_loss: 0.0315 - val_accuracy: 0.9896 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "112/112 [==============================] - 62s 555ms/step - loss: 0.0528 - accuracy: 0.9781 - val_loss: 0.2125 - val_accuracy: 0.9115 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "112/112 [==============================] - 62s 552ms/step - loss: 0.0520 - accuracy: 0.9821 - val_loss: 0.0385 - val_accuracy: 0.9896 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "112/112 [==============================] - 63s 556ms/step - loss: 0.0586 - accuracy: 0.9776 - val_loss: 0.0575 - val_accuracy: 0.9688 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "112/112 [==============================] - 62s 551ms/step - loss: 0.0525 - accuracy: 0.9809 - val_loss: 0.0387 - val_accuracy: 0.9896 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "112/112 [==============================] - 63s 556ms/step - loss: 0.0449 - accuracy: 0.9826 - val_loss: 0.1032 - val_accuracy: 0.9740 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "112/112 [==============================] - 62s 554ms/step - loss: 0.0461 - accuracy: 0.9871 - val_loss: 0.1441 - val_accuracy: 0.9583 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "112/112 [==============================] - 64s 565ms/step - loss: 0.0334 - accuracy: 0.9877 - val_loss: 0.0237 - val_accuracy: 0.9948 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "112/112 [==============================] - 67s 595ms/step - loss: 0.0246 - accuracy: 0.9916 - val_loss: 0.0093 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "112/112 [==============================] - 63s 556ms/step - loss: 0.0233 - accuracy: 0.9905 - val_loss: 0.0375 - val_accuracy: 0.9896 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "112/112 [==============================] - 61s 546ms/step - loss: 0.0285 - accuracy: 0.9916 - val_loss: 0.0384 - val_accuracy: 0.9896 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "112/112 [==============================] - 106s 948ms/step - loss: 0.0115 - accuracy: 0.9966 - val_loss: 0.0172 - val_accuracy: 0.9896 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "112/112 [==============================] - 69s 616ms/step - loss: 0.0259 - accuracy: 0.9888 - val_loss: 0.0400 - val_accuracy: 0.9792 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "112/112 [==============================] - 69s 609ms/step - loss: 0.0324 - accuracy: 0.9877 - val_loss: 0.0203 - val_accuracy: 0.9896 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "112/112 [==============================] - 71s 628ms/step - loss: 0.0181 - accuracy: 0.9938 - val_loss: 0.0229 - val_accuracy: 0.9896 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "112/112 [==============================] - 75s 661ms/step - loss: 0.0421 - accuracy: 0.9865 - val_loss: 0.0236 - val_accuracy: 0.9948 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "112/112 [==============================] - 71s 633ms/step - loss: 0.0364 - accuracy: 0.9854 - val_loss: 0.0698 - val_accuracy: 0.9844 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "112/112 [==============================] - 71s 626ms/step - loss: 0.0391 - accuracy: 0.9865 - val_loss: 0.0307 - val_accuracy: 0.9896 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "112/112 [==============================] - 71s 633ms/step - loss: 0.0728 - accuracy: 0.9787 - val_loss: 0.0471 - val_accuracy: 0.9844 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "112/112 [==============================] - 71s 633ms/step - loss: 0.0304 - accuracy: 0.9888 - val_loss: 0.0295 - val_accuracy: 0.9896 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "112/112 [==============================] - 77s 688ms/step - loss: 0.0168 - accuracy: 0.9966 - val_loss: 0.0065 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "112/112 [==============================] - 73s 645ms/step - loss: 0.0167 - accuracy: 0.9972 - val_loss: 0.0167 - val_accuracy: 0.9948 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "112/112 [==============================] - 73s 651ms/step - loss: 0.0199 - accuracy: 0.9905 - val_loss: 0.0344 - val_accuracy: 0.9948 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "112/112 [==============================] - 75s 670ms/step - loss: 0.0338 - accuracy: 0.9871 - val_loss: 0.1437 - val_accuracy: 0.9479 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "112/112 [==============================] - 72s 642ms/step - loss: 0.0232 - accuracy: 0.9933 - val_loss: 0.0706 - val_accuracy: 0.9740 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "112/112 [==============================] - 73s 645ms/step - loss: 0.0252 - accuracy: 0.9922 - val_loss: 0.1353 - val_accuracy: 0.9635 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "112/112 [==============================] - 73s 649ms/step - loss: 0.0497 - accuracy: 0.9815 - val_loss: 0.0531 - val_accuracy: 0.9896 - lr: 1.0000e-04\n",
      "Epoch 38/100\n",
      "112/112 [==============================] - 72s 640ms/step - loss: 0.0399 - accuracy: 0.9871 - val_loss: 0.0303 - val_accuracy: 0.9896 - lr: 1.0000e-04\n",
      "Epoch 39/100\n",
      "112/112 [==============================] - 73s 652ms/step - loss: 0.0162 - accuracy: 0.9944 - val_loss: 0.0732 - val_accuracy: 0.9792 - lr: 1.0000e-04\n",
      "Epoch 40/100\n",
      "112/112 [==============================] - 73s 648ms/step - loss: 0.0150 - accuracy: 0.9938 - val_loss: 0.0327 - val_accuracy: 0.9844 - lr: 1.0000e-04\n",
      "Epoch 41/100\n",
      "112/112 [==============================] - 72s 647ms/step - loss: 0.0124 - accuracy: 0.9955 - val_loss: 0.0145 - val_accuracy: 0.9948 - lr: 1.0000e-04\n",
      "Epoch 42/100\n",
      "112/112 [==============================] - 73s 650ms/step - loss: 0.0097 - accuracy: 0.9972 - val_loss: 0.0143 - val_accuracy: 0.9896 - lr: 1.0000e-04\n",
      "Epoch 43/100\n",
      "112/112 [==============================] - 73s 650ms/step - loss: 0.0242 - accuracy: 0.9899 - val_loss: 0.0733 - val_accuracy: 0.9844 - lr: 1.0000e-04\n",
      "Epoch 44/100\n",
      "112/112 [==============================] - 73s 644ms/step - loss: 0.0144 - accuracy: 0.9955 - val_loss: 0.0182 - val_accuracy: 0.9948 - lr: 1.0000e-04\n",
      "Epoch 45/100\n",
      "112/112 [==============================] - 73s 648ms/step - loss: 0.0322 - accuracy: 0.9877 - val_loss: 0.0203 - val_accuracy: 0.9896 - lr: 1.0000e-04\n",
      "Epoch 46/100\n",
      "112/112 [==============================] - 74s 659ms/step - loss: 0.0247 - accuracy: 0.9905 - val_loss: 0.0927 - val_accuracy: 0.9688 - lr: 1.0000e-04\n",
      "13/13 [==============================] - 12s 470ms/step\n",
      "13/13 [==============================] - 7s 466ms/step - loss: 0.0108 - accuracy: 0.9950\n",
      "\n",
      "Classification Report for Balanced Dataset 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Glioma       1.00      0.98      0.99        50\n",
      "  Meningioma       0.96      1.00      0.98        50\n",
      "   Pituitary       1.00      0.98      0.99        50\n",
      "    No-tumor       1.00      1.00      1.00        50\n",
      "\n",
      "    accuracy                           0.99       200\n",
      "   macro avg       0.99      0.99      0.99       200\n",
      "weighted avg       0.99      0.99      0.99       200\n",
      "\n",
      "Model saved successfully to E:\\Data002\\SplitData\\hybrid_MobDenseNet_Output_using_tensorflow\\balanceoutput\\mobdensenet_5.h5\n",
      "[Balanced Dataset 2] Processing completed.\n",
      "\n",
      "Training on Balanced Combined Dataset...\n",
      "\n",
      "Model Summary for 4-Class Dataset:\n",
      "Model: \"MobDenseNet_4class\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " mobilenet_1.00_224 (Functional  (None, 7, 7, 1024)  3228864     ['input_1[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " densenet121 (Functional)       (None, 7, 7, 1024)   7037504     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 1024)        0           ['mobilenet_1.00_224[0][0]']     \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " global_average_pooling2d_1 (Gl  (None, 1024)        0           ['densenet121[0][0]']            \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 2048)         0           ['global_average_pooling2d[0][0]'\n",
      "                                                                 , 'global_average_pooling2d_1[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 2048)         0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 4)            8196        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10,274,564\n",
      "Trainable params: 10,169,028\n",
      "Non-trainable params: 105,536\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "Hyper-Parameters (4-Class Model):\n",
      "| Hyper-Parameter Name        | Value            |\n",
      "|----------------------------|------------------|\n",
      "| Activation Function         | relu              |\n",
      "| Kernel Initializer          | glorot_uniform   |\n",
      "| Initial Learning Rate       | 0.002            |\n",
      "| Optimizer                   | nadam            |\n",
      "| Batch Size                  | 16               |\n",
      "| Epochs                      | 100               |\n",
      "| Train-test Split            | <function train_test_split at 0x00000233C97E4820>           |\n",
      "| Output Activation Function  | softmax            |\n",
      "| Output Kernel Initializer   | random_uniform |\n",
      "| Number of Output Classes    | 4                |\n",
      "| Dropout Rate                | 0.2               |\n",
      "Epoch 1/100\n",
      "231/231 [==============================] - 242s 893ms/step - loss: 0.4686 - accuracy: 0.8160 - val_loss: 0.4892 - val_accuracy: 0.8125 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "231/231 [==============================] - 152s 656ms/step - loss: 0.2271 - accuracy: 0.9223 - val_loss: 0.4609 - val_accuracy: 0.8450 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "231/231 [==============================] - 153s 660ms/step - loss: 0.1538 - accuracy: 0.9451 - val_loss: 0.1778 - val_accuracy: 0.9500 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "231/231 [==============================] - 155s 669ms/step - loss: 0.1296 - accuracy: 0.9589 - val_loss: 0.1714 - val_accuracy: 0.9400 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "231/231 [==============================] - 157s 677ms/step - loss: 0.1291 - accuracy: 0.9507 - val_loss: 0.1365 - val_accuracy: 0.9575 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "231/231 [==============================] - 154s 664ms/step - loss: 0.0826 - accuracy: 0.9716 - val_loss: 0.1478 - val_accuracy: 0.9550 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "231/231 [==============================] - 154s 664ms/step - loss: 0.0806 - accuracy: 0.9743 - val_loss: 0.1502 - val_accuracy: 0.9525 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "231/231 [==============================] - 154s 665ms/step - loss: 0.0766 - accuracy: 0.9729 - val_loss: 0.1448 - val_accuracy: 0.9550 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "231/231 [==============================] - 155s 668ms/step - loss: 0.0718 - accuracy: 0.9737 - val_loss: 0.0754 - val_accuracy: 0.9600 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "231/231 [==============================] - 156s 673ms/step - loss: 0.0490 - accuracy: 0.9840 - val_loss: 0.1339 - val_accuracy: 0.9650 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "231/231 [==============================] - 154s 666ms/step - loss: 0.0468 - accuracy: 0.9811 - val_loss: 0.0723 - val_accuracy: 0.9775 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "231/231 [==============================] - 157s 677ms/step - loss: 0.0461 - accuracy: 0.9838 - val_loss: 0.1416 - val_accuracy: 0.9675 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "231/231 [==============================] - 154s 664ms/step - loss: 0.0540 - accuracy: 0.9802 - val_loss: 0.1783 - val_accuracy: 0.9450 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "231/231 [==============================] - 155s 671ms/step - loss: 0.0612 - accuracy: 0.9797 - val_loss: 0.1823 - val_accuracy: 0.9400 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "231/231 [==============================] - 155s 669ms/step - loss: 0.0473 - accuracy: 0.9843 - val_loss: 0.0939 - val_accuracy: 0.9775 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "231/231 [==============================] - 157s 675ms/step - loss: 0.0414 - accuracy: 0.9865 - val_loss: 0.0672 - val_accuracy: 0.9825 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "231/231 [==============================] - 156s 672ms/step - loss: 0.0453 - accuracy: 0.9835 - val_loss: 0.0974 - val_accuracy: 0.9600 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "231/231 [==============================] - 159s 688ms/step - loss: 0.0330 - accuracy: 0.9892 - val_loss: 0.0417 - val_accuracy: 0.9825 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "231/231 [==============================] - 155s 669ms/step - loss: 0.0441 - accuracy: 0.9846 - val_loss: 0.0556 - val_accuracy: 0.9825 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "231/231 [==============================] - 157s 679ms/step - loss: 0.0381 - accuracy: 0.9876 - val_loss: 0.1149 - val_accuracy: 0.9775 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "231/231 [==============================] - 155s 668ms/step - loss: 0.0356 - accuracy: 0.9892 - val_loss: 0.0814 - val_accuracy: 0.9800 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "231/231 [==============================] - 154s 666ms/step - loss: 0.0309 - accuracy: 0.9889 - val_loss: 0.0477 - val_accuracy: 0.9925 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "231/231 [==============================] - 154s 664ms/step - loss: 0.0309 - accuracy: 0.9894 - val_loss: 0.1558 - val_accuracy: 0.9550 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "231/231 [==============================] - 152s 656ms/step - loss: 0.0382 - accuracy: 0.9878 - val_loss: 0.2923 - val_accuracy: 0.9325 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "231/231 [==============================] - 152s 656ms/step - loss: 0.0399 - accuracy: 0.9857 - val_loss: 0.0733 - val_accuracy: 0.9800 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "231/231 [==============================] - 154s 665ms/step - loss: 0.0235 - accuracy: 0.9911 - val_loss: 0.0690 - val_accuracy: 0.9800 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "231/231 [==============================] - 153s 659ms/step - loss: 0.0279 - accuracy: 0.9905 - val_loss: 0.1526 - val_accuracy: 0.9700 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "231/231 [==============================] - 153s 660ms/step - loss: 0.0299 - accuracy: 0.9905 - val_loss: 0.0789 - val_accuracy: 0.9750 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "231/231 [==============================] - 151s 653ms/step - loss: 0.0297 - accuracy: 0.9919 - val_loss: 0.1053 - val_accuracy: 0.9675 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "231/231 [==============================] - 153s 662ms/step - loss: 0.0498 - accuracy: 0.9813 - val_loss: 0.0739 - val_accuracy: 0.9700 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "231/231 [==============================] - 151s 653ms/step - loss: 0.0212 - accuracy: 0.9924 - val_loss: 0.0857 - val_accuracy: 0.9800 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "231/231 [==============================] - 154s 664ms/step - loss: 0.0255 - accuracy: 0.9908 - val_loss: 0.0737 - val_accuracy: 0.9800 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "231/231 [==============================] - 151s 651ms/step - loss: 0.0228 - accuracy: 0.9935 - val_loss: 0.0656 - val_accuracy: 0.9775 - lr: 1.0000e-04\n",
      "26/26 [==============================] - 19s 504ms/step\n",
      "26/26 [==============================] - 13s 487ms/step - loss: 0.0847 - accuracy: 0.9637\n",
      "\n",
      "Classification Report for Balanced Combined Dataset:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Glioma       0.97      0.98      0.98       121\n",
      "  Meningioma       0.97      0.95      0.96       121\n",
      "   Pituitary       0.98      0.98      0.98        50\n",
      "    No-tumor       0.98      0.98      0.98       121\n",
      "\n",
      "    accuracy                           0.97       413\n",
      "   macro avg       0.97      0.97      0.97       413\n",
      "weighted avg       0.97      0.97      0.97       413\n",
      "\n",
      "Model saved successfully to E:\\CombineData001_002\\SplitData\\hybrid_MobDenseNet_Output_using_tensorflow\\balanceoutput\\mobdensenet_6.h5\n",
      "[Balanced Combined Dataset] Processing completed.\n",
      "\n",
      "Summary of Results Across Datasets:\n",
      "+----------------------------+------------+----------+--------+-------------+----------+------------+-----------------------+\n",
      "| Dataset                    |   Accuracy |      AUC |   Loss |   Precision |   Recall |   F1 Score |   Inference Time (ms) |\n",
      "+============================+============+==========+========+=============+==========+============+=======================+\n",
      "| Imbalance Dataset 1        |     0.9935 | 0.999807 | 0.0168 |      0.9906 |   0.987  |     0.9888 |               13147.3 |\n",
      "+----------------------------+------------+----------+--------+-------------+----------+------------+-----------------------+\n",
      "| Imbalance Dataset 2        |     0.9756 | 0.999034 | 0.0774 |      0.9765 |   0.9785 |     0.9773 |               16390.2 |\n",
      "+----------------------------+------------+----------+--------+-------------+----------+------------+-----------------------+\n",
      "| Imbalance Combined Dataset |     0.9732 | 0.998932 | 0.0795 |      0.9755 |   0.9793 |     0.9773 |               25120.3 |\n",
      "+----------------------------+------------+----------+--------+-------------+----------+------------+-----------------------+\n",
      "| Balanced Dataset 1         |     0.9953 | 0.999504 | 0.0233 |      0.9862 |   0.9859 |     0.9858 |               10138.7 |\n",
      "+----------------------------+------------+----------+--------+-------------+----------+------------+-----------------------+\n",
      "| Balanced Dataset 2         |     0.995  | 1        | 0.0108 |      0.9904 |   0.99   |     0.99   |               13156.6 |\n",
      "+----------------------------+------------+----------+--------+-------------+----------+------------+-----------------------+\n",
      "| Balanced Combined Dataset  |     0.9637 | 0.999435 | 0.0847 |      0.9744 |   0.9743 |     0.9743 |               19372.8 |\n",
      "+----------------------------+------------+----------+--------+-------------+----------+------------+-----------------------+\n",
      "\n",
      "Training and evaluation completed for all datasets.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "from tabulate import tabulate\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define dataset configurations\n",
    "datasets = [\n",
    "    {\n",
    "        \"name\": \"Imbalance Dataset 1\",\n",
    "        \"train_generator\": imbalance_train_generator1,\n",
    "        \"test_generator\": imbalance_test_generator1,\n",
    "        \"steps_per_epoch\": max(1, imbalance_train_generator1.samples // batch_size),\n",
    "        \"validation_steps\": max(1, imbalance_test_generator1.samples // batch_size),\n",
    "        \"output_dir\": r\"E:\\Data001\\SplitData\\hybrid_MobDenseNet_Output_using_tensorflow\\imbalanceoutput\",\n",
    "        \"class_names\": ['Glioma', 'Meningioma', 'Pituitary'],\n",
    "        \"num_classes\": 3\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Imbalance Dataset 2\",\n",
    "        \"train_generator\": imbalance_train_generator2,\n",
    "        \"test_generator\": imbalance_test_generator2,\n",
    "        \"steps_per_epoch\": max(1, imbalance_train_generator2.samples // batch_size),\n",
    "        \"validation_steps\": max(1, imbalance_test_generator2.samples // batch_size),\n",
    "        \"output_dir\": r\"E:\\Data002\\SplitData\\hybrid_MobDenseNetwithCBAMattention_Output_using_tensorflow\\imbalanceoutput\",\n",
    "        \"class_names\": ['Glioma', 'Meningioma', 'Pituitary','No-tumor'],\n",
    "        \"num_classes\": 4\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Imbalance Combined Dataset\",\n",
    "        \"train_generator\": imbalance_train_generator_combined,\n",
    "        \"test_generator\": imbalance_test_generator_combined,\n",
    "        \"steps_per_epoch\": max(1, imbalance_train_generator_combined.samples // batch_size),\n",
    "        \"validation_steps\": max(1, imbalance_test_generator_combined.samples // batch_size),\n",
    "        \"output_dir\": r\"E:\\CombineData001_002\\SplitData\\hybrid_MobDenseNetwithCBAMattention_Output_using_tensorflow\\imbalanceoutput\",\n",
    "        \"class_names\": ['Glioma', 'Meningioma', 'Pituitary','No-tumor'],\n",
    "        \"num_classes\": 4\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Balanced Dataset 1\",\n",
    "        \"train_generator\": balance_train_generator1,\n",
    "        \"test_generator\": balance_test_generator1,\n",
    "        \"steps_per_epoch\": max(1, balance_train_generator1.samples // batch_size),\n",
    "        \"validation_steps\": max(1, balance_test_generator1.samples // batch_size),\n",
    "        \"output_dir\": r\"E:\\Data001\\SplitData\\hybrid_MobDenseNetwithCBAMattention_Output_using_tensorflow\\balanceoutput\",\n",
    "        \"class_names\": ['Glioma', 'Meningioma', 'Pituitary'],\n",
    "        \"num_classes\": 3\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Balanced Dataset 2\",\n",
    "        \"train_generator\": balance_train_generator2,\n",
    "        \"test_generator\": balance_test_generator2,\n",
    "        \"steps_per_epoch\": max(1, balance_train_generator2.samples // batch_size),\n",
    "        \"validation_steps\": max(1, balance_test_generator2.samples // batch_size),\n",
    "        \"output_dir\": r\"E:\\Data002\\SplitData\\hybrid_MobDenseNetwithCBAMattention_Output_using_tensorflow\\balanceoutput\",\n",
    "        \"class_names\": ['Glioma', 'Meningioma', 'Pituitary','No-tumor'],\n",
    "        \"num_classes\": 4\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Balanced Combined Dataset\",\n",
    "        \"train_generator\": balance_train_generator_combined,\n",
    "        \"test_generator\": balance_test_generator_combined,\n",
    "        \"steps_per_epoch\": max(1, balance_train_generator_combined.samples // batch_size),\n",
    "        \"validation_steps\": max(1, balance_test_generator_combined.samples // batch_size),\n",
    "        \"output_dir\": r\"E:\\CombineData001_002\\SplitData\\hybrid_MobDenseNetwithCBAMattention_Output_using_tensorflow\\balanceoutput\",\n",
    "        \"class_names\": ['Glioma', 'Meningioma', 'Pituitary','No-tumor'],\n",
    "        \"num_classes\": 4\n",
    "    }\n",
    "]\n",
    "\n",
    "for dataset in datasets:\n",
    "    if not hasattr(dataset[\"train_generator\"], \"samples\") or not hasattr(dataset[\"test_generator\"], \"samples\"):\n",
    "        raise ValueError(f\"Generator for {dataset['name']} is missing 'samples' attribute.\")\n",
    "    if dataset[\"train_generator\"].samples == 0 or dataset[\"test_generator\"].samples == 0:\n",
    "        raise ValueError(f\"{dataset['name']} has zero samples in train or test generator.\")\n",
    "\n",
    "# Display dataset sizes and steps\n",
    "table_data = [\n",
    "    [d[\"name\"], d[\"train_generator\"].samples, d[\"test_generator\"].samples, \n",
    "     d[\"steps_per_epoch\"], d[\"validation_steps\"], d[\"num_classes\"]]\n",
    "    for d in datasets\n",
    "]\n",
    "headers = [\"Dataset\", \"Training Samples\", \"Validation Samples\", \"Steps per Epoch\", \"Validation Steps\", \"Num Classes\"]\n",
    "print(\"\\nDataset Sizes and Steps:\")\n",
    "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))\n",
    "\n",
    "# Model parameters\n",
    "MODEL_NAME = \"mobdensenet\"\n",
    "input_shape = (224, 224, 3)\n",
    "input_image_size = f\"{input_shape[0]}x{input_shape[1]}\" \n",
    "EPOCHS_INITIAL = 100  # Defined as 100, but can use 5 if preferred\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ModelCheckpoint('hybrid_model_best.h5', monitor='val_loss', save_best_only=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=15, min_lr=0.002)\n",
    "]\n",
    "\n",
    "# Function to plot training history and confusion matrix\n",
    "def plot_training_history(history, output_dir, model_name, counter, class_names, y_true, y_pred):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy', color='#1f77b4')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='#ff7f0e')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss', color='#2ca02c')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss', color='#d62728')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f'{model_name}_training_validation_{counter}.png'))\n",
    "    plt.close()\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.savefig(os.path.join(output_dir, f'{model_name}_confusion_matrix_{counter}.png'))\n",
    "    plt.close()\n",
    "\n",
    "# Function to save Chart.js config\n",
    "def save_chartjs_configs(history, output_dir, model_name, counter):\n",
    "    chart_config = {\n",
    "        \"type\": \"line\",\n",
    "        \"data\": {\n",
    "            \"labels\": list(range(1, len(history.history['accuracy']) + 1)),\n",
    "            \"datasets\": [\n",
    "                {\"label\": \"Training Accuracy\", \"data\": history.history['accuracy'], \n",
    "                 \"borderColor\": \"rgba(31, 119, 180, 1)\", \"fill\": False},\n",
    "                {\"label\": \"Validation Accuracy\", \"data\": history.history['val_accuracy'], \n",
    "                 \"borderColor\": \"rgba(255, 127, 14, 1)\", \"fill\": False},\n",
    "                {\"label\": \"Training Loss\", \"data\": history.history['loss'], \n",
    "                 \"borderColor\": \"rgba(44, 160, 44, 1)\", \"fill\": False},\n",
    "                {\"label\": \"Validation Loss\", \"data\": history.history['val_loss'], \n",
    "                 \"borderColor\": \"rgba(214, 39, 40, 1)\", \"fill\": False}\n",
    "            ]\n",
    "        },\n",
    "        \"options\": {\n",
    "            \"scales\": {\n",
    "                \"x\": {\"title\": {\"display\": True, \"text\": \"Epoch\"}},\n",
    "                \"y\": {\"title\": {\"display\": True, \"text\": \"Value\"}}\n",
    "            },\n",
    "            \"plugins\": {\"title\": {\"display\": True, \"text\": f\"{model_name} Training History\"}}\n",
    "        }\n",
    "    }\n",
    "    with open(os.path.join(output_dir, f'{model_name}_chartjs_{counter}.json'), 'w') as f:\n",
    "        json.dump(chart_config, f, indent=4)\n",
    "\n",
    "# Train and evaluate for each dataset\n",
    "results_summary = []\n",
    "for counter, dataset in enumerate(datasets, 1):\n",
    "    dataset_name = dataset[\"name\"]\n",
    "    train_generator = dataset[\"train_generator\"]\n",
    "    test_generator = dataset[\"test_generator\"]\n",
    "    steps_per_epoch = dataset[\"steps_per_epoch\"]\n",
    "    validation_steps = dataset[\"validation_steps\"]\n",
    "    output_dir = dataset[\"output_dir\"]\n",
    "    class_names = dataset[\"class_names\"]\n",
    "    num_classes = dataset[\"num_classes\"]\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"\\nTraining on {dataset_name}...\")\n",
    "    # Build and compile a fresh model for each dataset\n",
    "    model = build_mobdensenet(num_classes=num_classes, input_shape=input_shape)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=test_generator,\n",
    "        validation_steps=validation_steps,\n",
    "        epochs=EPOCHS_INITIAL,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    # Save training history\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    history_df.to_csv(os.path.join(output_dir, f'{MODEL_NAME}_history_{counter}.csv'), index=False)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_generator.reset()\n",
    "    start_time = time.time()\n",
    "    predictions = model.predict(test_generator, steps=test_generator.samples // batch_size + 1)\n",
    "    inference_time = (time.time() - start_time) * 1000\n",
    "    y_pred = np.argmax(predictions, axis=1)\n",
    "    y_true = test_generator.classes\n",
    "\n",
    "    # Calculate metrics\n",
    "    precision = precision_score(y_true, y_pred, average=None)\n",
    "    recall = recall_score(y_true, y_pred, average=None)\n",
    "    f1 = f1_score(y_true, y_pred, average=None)\n",
    "    try:\n",
    "        auc = roc_auc_score(test_generator.classes, predictions, multi_class='ovr')\n",
    "    except ValueError as e:\n",
    "        auc = \"N/A\"\n",
    "        print(f\"Warning: AUC calculation failed for {dataset_name}: {str(e)}\")\n",
    "\n",
    "    test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "\n",
    "    # Print classification report\n",
    "    print(f\"\\nClassification Report for {dataset_name}:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "    # Save metrics\n",
    "    metrics = {\n",
    "        'precision_per_class': precision.tolist(),\n",
    "        'recall_per_class': recall.tolist(),\n",
    "        'f1_per_class': f1.tolist(),\n",
    "        'auc': auc,\n",
    "        'accuracy': test_accuracy\n",
    "    }\n",
    "    with open(os.path.join(output_dir, f'{MODEL_NAME}_metrics_{counter}.json'), 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "    # Calculate average metrics\n",
    "    precision_avg = np.mean(metrics['precision_per_class'])\n",
    "    recall_avg = np.mean(metrics['recall_per_class'])\n",
    "    f1_avg = np.mean(metrics['f1_per_class'])\n",
    "\n",
    "    # Save results to CSV\n",
    "    results = [{\n",
    "        'Model with input image size': f\"{MODEL_NAME} {input_image_size}\",\n",
    "        'Dataset': dataset_name,\n",
    "        'Accuracy': test_accuracy,\n",
    "        'AUC': auc if auc != \"N/A\" else 0.0,\n",
    "        'Loss': test_loss,\n",
    "        'Precision': precision_avg,\n",
    "        'Recall': recall_avg,\n",
    "        'F1 Score': f1_avg,\n",
    "        'Inference time (in miliseconds)': inference_time\n",
    "    }]\n",
    "    results_summary.append(results[0])\n",
    "    with open(os.path.join(output_dir, f'{MODEL_NAME}_results_{counter}.csv'), 'w', newline='') as csvfile:\n",
    "        fieldnames = ['Model with input image size', 'Dataset', 'Accuracy', 'AUC', 'Loss', 'Precision', 'Recall', 'F1 Score', 'Inference time (in miliseconds)']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerow(results[0])\n",
    "\n",
    "    # Plot and save training history and confusion matrix\n",
    "    plot_training_history(history, output_dir, MODEL_NAME, counter, class_names, y_true, y_pred)\n",
    "    save_chartjs_configs(history, output_dir, MODEL_NAME, counter)\n",
    "\n",
    "    # Save the model\n",
    "    model.save(os.path.join(output_dir, f'{MODEL_NAME}_{counter}.h5'))\n",
    "    print(f\"Model saved successfully to {os.path.join(output_dir, f'{MODEL_NAME}_{counter}.h5')}\")\n",
    "\n",
    "    # Clear session to free memory\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    print(f\"[{dataset_name}] Processing completed.\")\n",
    "\n",
    "# Display summary table of results\n",
    "print(\"\\nSummary of Results Across Datasets:\")\n",
    "summary_table = [\n",
    "    [r['Dataset'], f\"{r['Accuracy']:.4f}\", r['AUC'] if r['AUC'] != \"N/A\" else \"N/A\", \n",
    "     f\"{r['Loss']:.4f}\", f\"{r['Precision']:.4f}\", f\"{r['Recall']:.4f}\", \n",
    "     f\"{r['F1 Score']:.4f}\", f\"{r['Inference time (in miliseconds)']:.2f}\"]\n",
    "    for r in results_summary\n",
    "]\n",
    "summary_headers = [\"Dataset\", \"Accuracy\", \"AUC\", \"Loss\", \"Precision\", \"Recall\", \"F1 Score\", \"Inference Time (ms)\"]\n",
    "print(tabulate(summary_table, headers=summary_headers, tablefmt=\"grid\"))\n",
    "\n",
    "# Save summary table to CSV\n",
    "summary_output_dir = r\"E:\\CombineData001_002\\SplitData\\hybrid_MobDenseNet_Output_using_tensorflow\"\n",
    "os.makedirs(summary_output_dir, exist_ok=True)\n",
    "with open(os.path.join(summary_output_dir, 'all_datasets_results_summary.csv'), 'w', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=summary_headers)\n",
    "    writer.writeheader()\n",
    "    for row in results_summary:\n",
    "        writer.writerow({\n",
    "            'Dataset': row['Dataset'],\n",
    "            'Accuracy': f\"{row['Accuracy']:.4f}\",\n",
    "            'AUC': row['AUC'] if row['AUC'] != \"N/A\" else \"N/A\",\n",
    "            'Loss': f\"{row['Loss']:.4f}\",\n",
    "            'Precision': f\"{row['Precision']:.4f}\",\n",
    "            'Recall': f\"{row['Recall']:.4f}\",\n",
    "            'F1 Score': f\"{row['F1 Score']:.4f}\",\n",
    "            'Inference Time (ms)': f\"{row['Inference time (in miliseconds)']:.2f}\"\n",
    "        })\n",
    "\n",
    "# Optional: Bar chart comparing accuracy across datasets (uncomment to enable)\n",
    "\"\"\"\n",
    "accuracy_data = [r['Accuracy'] for r in results_summary]\n",
    "dataset_names = [r['Dataset'] for r in results_summary]\n",
    "chart_config = {\n",
    "    \"type\": \"bar\",\n",
    "    \"data\": {\n",
    "        \"labels\": dataset_names,\n",
    "        \"datasets\": [{\n",
    "            \"label\": \"Test Accuracy\",\n",
    "            \"data\": accuracy_data,\n",
    "            \"backgroundColor\": \"rgba(31, 119, 180, 0.7)\",\n",
    "            \"borderColor\": \"rgba(31, 119, 180, 1)\",\n",
    "            \"borderWidth\": 1\n",
    "        }]\n",
    "    },\n",
    "    \"options\": {\n",
    "        \"scales\": {\n",
    "            \"x\": {\"title\": {\"display\": True, \"text\": \"Dataset\"}},\n",
    "            \"y\": {\"title\": {\"display\": True, \"text\": \"Accuracy\"}, \"beginAtZero\": True, \"max\": 1}\n",
    "        },\n",
    "        \"plugins\": {\"title\": {\"display\": True, \"text\": \"Test Accuracy Across Datasets\"}}\n",
    "    }\n",
    "}\n",
    "with open(os.path.join(summary_output_dir, 'accuracy_comparison_chart.json'), 'w') as f:\n",
    "    json.dump(chart_config, f, indent=4)\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nTraining and evaluation completed for all datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d31554",
   "metadata": {},
   "source": [
    "CBAM + MOBDENSENET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e364c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5514 images belonging to 3 classes.\n",
      "Found 614 images belonging to 3 classes.\n",
      "Found 5872 images belonging to 4 classes.\n",
      "Found 656 images belonging to 4 classes.\n",
      "Found 17082 images belonging to 4 classes.\n",
      "Found 1902 images belonging to 4 classes.\n",
      "Total dataset1 images: 6128\n",
      "Total dataset2 images: 6528\n",
      "Total dataset Combined images: 18984\n"
     ]
    }
   ],
   "source": [
    "# Define parameters\n",
    "target_size_cbam = (128, 128)\n",
    "batch_size_cbam = 16\n",
    "EPOCHS_INITIAL = 5\n",
    "\n",
    "# Define dataset directories\n",
    "imbalance_dataset1_train_dir = r'E:\\Data001\\SplitData\\train'\n",
    "imbalance_dataset1_test_dir = r'E:\\Data001\\SplitData\\test'\n",
    "imbalance_dataset2_train_dir = r'E:\\Data002\\SplitData\\train'\n",
    "imbalance_dataset2_test_dir = r'E:\\Data002\\SplitData\\test'\n",
    "imbalance_combined_train_dir = r'E:\\CombineData001_002\\SplitData\\train'\n",
    "imbalance_combined_test_dir = r'E:\\CombineData001_002\\SplitData\\test'\n",
    "\n",
    "# Data augmentation for training\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1/255,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Dataset001 Generators\n",
    "imbalance_train_generator1_cbam = datagen.flow_from_directory(\n",
    "    directory=imbalance_dataset1_train_dir,\n",
    "    target_size=target_size_cbam,\n",
    "    batch_size=batch_size_cbam,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "imbalance_test_generator1_cbam = datagen.flow_from_directory(\n",
    "    directory=imbalance_dataset1_test_dir,\n",
    "    target_size=target_size_cbam,\n",
    "    batch_size=batch_size_cbam,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Dataset002 Generators\n",
    "imbalance_train_generator2_cbam = datagen.flow_from_directory(\n",
    "    directory=imbalance_dataset2_train_dir,\n",
    "    target_size=target_size_cbam,\n",
    "    batch_size=batch_size_cbam,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "imbalance_test_generator2_cbam = datagen.flow_from_directory(\n",
    "    directory=imbalance_dataset2_test_dir,\n",
    "    target_size=target_size_cbam,\n",
    "    batch_size=batch_size_cbam,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# CombinedDataset Generators\n",
    "imbalance_train_generator_combined_cbam = datagen.flow_from_directory(\n",
    "    directory=imbalance_combined_train_dir,\n",
    "    target_size=target_size_cbam,\n",
    "    batch_size=batch_size_cbam,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "imbalance_test_generator_combined_cbam = datagen.flow_from_directory(\n",
    "    directory=imbalance_combined_test_dir,\n",
    "    target_size=target_size_cbam,\n",
    "    batch_size=batch_size_cbam,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Calculate total images using generator .n attribute\n",
    "dataset1 = imbalance_train_generator1_cbam.n + imbalance_test_generator1_cbam.n\n",
    "dataset2 = imbalance_train_generator2_cbam.n + imbalance_test_generator2_cbam.n\n",
    "combineddataset = imbalance_train_generator_combined_cbam.n + imbalance_test_generator_combined_cbam.n\n",
    "\n",
    "print(f\"Total dataset1 images: {dataset1}\")\n",
    "print(f\"Total dataset2 images: {dataset2}\")\n",
    "print(f\"Total dataset Combined images: {combineddataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eef4b8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1911 images belonging to 3 classes.\n",
      "Found 213 images belonging to 3 classes.\n",
      "Found 1800 images belonging to 4 classes.\n",
      "Found 200 images belonging to 4 classes.\n",
      "Found 4446 images belonging to 4 classes.\n",
      "Found 413 images belonging to 4 classes.\n",
      "Total dataset1 images: 2124\n",
      "Total dataset2 images: 413\n",
      "Total dataset Combined images: 4859\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define parameters\n",
    "target_size_cbam = (128, 128)\n",
    "batch_size_cbam = 16\n",
    "EPOCHS_INITIAL = 5\n",
    "\n",
    "# Define dataset directories\n",
    "balance_dataset1_train_dir = r'E:\\Data001\\SplitData1\\train'\n",
    "balance_dataset1_test_dir = r'E:\\Data001\\SplitData1\\test'\n",
    "balance_dataset2_train_dir = r'E:\\Data002\\SplitData1\\train'\n",
    "balance_dataset2_test_dir = r'E:\\Data002\\SplitData1\\test'\n",
    "balance_combined_train_dir = r'E:\\CombineData001_002\\SplitData1\\train'\n",
    "balance_combined_test_dir = r'E:\\CombineData001_002\\SplitData1\\test'\n",
    "\n",
    "# Data augmentation for training\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1/255,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Dataset001 Generators\n",
    "balance_train_generator1_cbam = datagen.flow_from_directory(\n",
    "    directory=balance_dataset1_train_dir,\n",
    "    target_size=target_size_cbam,\n",
    "    batch_size=batch_size_cbam,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "balance_test_generator1_cbam = datagen.flow_from_directory(\n",
    "    directory=balance_dataset1_test_dir,\n",
    "    target_size=target_size_cbam,\n",
    "    batch_size=batch_size_cbam,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Dataset002 Generators\n",
    "balance_train_generator2_cbam = datagen.flow_from_directory(\n",
    "    directory=balance_dataset2_train_dir,\n",
    "    target_size=target_size_cbam,\n",
    "    batch_size=batch_size_cbam,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "balance_test_generator2_cbam = datagen.flow_from_directory(\n",
    "    directory=balance_dataset2_test_dir,\n",
    "    target_size=target_size_cbam,\n",
    "    batch_size=batch_size_cbam,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# CombinedDataset Generators\n",
    "balance_train_generator_combined_cbam = datagen.flow_from_directory(\n",
    "    directory=balance_combined_train_dir,\n",
    "    target_size=target_size_cbam,\n",
    "    batch_size=batch_size_cbam,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "balance_test_generator_combined_cbam = datagen.flow_from_directory(\n",
    "    directory=balance_combined_test_dir,\n",
    "    target_size=target_size_cbam,\n",
    "    batch_size=batch_size_cbam,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Calculate total images using generator .n attribute\n",
    "dataset1 = balance_train_generator1_cbam.n + balance_test_generator1_cbam.n\n",
    "dataset2 = balance_test_generator1_cbam.n + balance_test_generator2_cbam.n\n",
    "combineddataset = balance_train_generator_combined_cbam.n + balance_test_generator_combined_cbam.n\n",
    "\n",
    "print(f\"Total dataset1 images: {dataset1}\")\n",
    "print(f\"Total dataset2 images: {dataset2}\")\n",
    "print(f\"Total dataset Combined images: {combineddataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95d08cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+--------------------+----------------------+-------------------+--------------------+\n",
      "| Dataset                    |   Training Samples |   Validation Samples |   Steps per Epoch |   Validation Steps |\n",
      "+============================+====================+======================+===================+====================+\n",
      "| Imbalance Dataset 1        |               5514 |                  614 |               344 |                 38 |\n",
      "+----------------------------+--------------------+----------------------+-------------------+--------------------+\n",
      "| Imbalance Dataset 2        |               5872 |                  656 |               367 |                 41 |\n",
      "+----------------------------+--------------------+----------------------+-------------------+--------------------+\n",
      "| Imbalance Combined Dataset |              17082 |                 1902 |              1067 |                118 |\n",
      "+----------------------------+--------------------+----------------------+-------------------+--------------------+\n",
      "| Balanced Dataset 1         |               1911 |                  213 |               119 |                 13 |\n",
      "+----------------------------+--------------------+----------------------+-------------------+--------------------+\n",
      "| Balanced Dataset 2         |               1800 |                  200 |               112 |                 12 |\n",
      "+----------------------------+--------------------+----------------------+-------------------+--------------------+\n",
      "| Balanced Combined Dataset  |               4446 |                  413 |               277 |                 25 |\n",
      "+----------------------------+--------------------+----------------------+-------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "# Calculate steps per epoch and validation steps for each dataset\n",
    "steps_per_epoch1 = max(1, imbalance_train_generator1_cbam.samples // batch_size)\n",
    "validation_steps1 = max(1, imbalance_test_generator1_cbam.samples // batch_size)\n",
    "\n",
    "steps_per_epoch2 = max(1, imbalance_train_generator2_cbam.samples // batch_size)\n",
    "validation_steps2 = max(1, imbalance_test_generator2_cbam.samples // batch_size)\n",
    "\n",
    "steps_per_epoch3 = max(1, imbalance_train_generator_combined_cbam.samples // batch_size)\n",
    "validation_steps3 = max(1, imbalance_test_generator_combined_cbam.samples // batch_size)\n",
    "\n",
    "steps_per_epoch4 = max(1, balance_train_generator1_cbam.samples // batch_size)\n",
    "validation_steps4 = max(1, balance_test_generator1_cbam.samples // batch_size)\n",
    "\n",
    "steps_per_epoch5 = max(1, balance_train_generator2_cbam.samples // batch_size)\n",
    "validation_steps5 = max(1, balance_test_generator2_cbam.samples // batch_size)\n",
    "\n",
    "steps_per_epoch6 = max(1, balance_train_generator_combined_cbam.samples // batch_size)\n",
    "validation_steps6 = max(1, balance_test_generator_combined_cbam.samples // batch_size)\n",
    "\n",
    "table_data = [\n",
    "    [\"Imbalance Dataset 1\", imbalance_train_generator1_cbam.samples, imbalance_test_generator1_cbam.samples,\n",
    "     steps_per_epoch1, validation_steps1],\n",
    "    [\"Imbalance Dataset 2\", imbalance_train_generator2_cbam.samples, imbalance_test_generator2_cbam.samples,\n",
    "     steps_per_epoch2, validation_steps2],\n",
    "    [\"Imbalance Combined Dataset\", imbalance_train_generator_combined_cbam.samples, imbalance_test_generator_combined_cbam.samples,\n",
    "     steps_per_epoch3, validation_steps3],\n",
    "    [\"Balanced Dataset 1\", balance_train_generator1_cbam.samples, balance_test_generator1_cbam.samples,\n",
    "     steps_per_epoch4, validation_steps4],\n",
    "    [\"Balanced Dataset 2\", balance_train_generator2_cbam.samples, balance_test_generator2_cbam.samples,\n",
    "     steps_per_epoch5, validation_steps5],\n",
    "    [\"Balanced Combined Dataset\", balance_train_generator_combined_cbam.samples, balance_test_generator_combined_cbam.samples,\n",
    "     steps_per_epoch6, validation_steps6]\n",
    "]\n",
    "\n",
    "headers = [\"Dataset\", \"Training Samples\", \"Validation Samples\", \"Steps per Epoch\", \"Validation Steps\"]\n",
    "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6939300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GPU memory growth enabled for 1 GPU(s)\n",
      " Mixed precision training enabled\n",
      " Using your parameters: batch_size=16, input_shape=(128, 128, 3), EPOCHS=5\n",
      "\n",
      "Dataset Sizes and Steps:\n",
      "+-----------------------------+--------------------+----------------------+-------------------+--------------------+---------------+\n",
      "| Dataset                     |   Training Samples |   Validation Samples |   Steps per Epoch |   Validation Steps |   Num Classes |\n",
      "+=============================+====================+======================+===================+====================+===============+\n",
      "| Imbalanced Dataset 1        |               5514 |                  614 |               344 |                 38 |             3 |\n",
      "+-----------------------------+--------------------+----------------------+-------------------+--------------------+---------------+\n",
      "| Imbalanced Dataset 2        |               5872 |                  656 |               367 |                 41 |             4 |\n",
      "+-----------------------------+--------------------+----------------------+-------------------+--------------------+---------------+\n",
      "| Imbalanced Combined Dataset |              17082 |                 1902 |              1067 |                118 |             4 |\n",
      "+-----------------------------+--------------------+----------------------+-------------------+--------------------+---------------+\n",
      "| Balanced Dataset 1          |               1911 |                  213 |               119 |                 13 |             3 |\n",
      "+-----------------------------+--------------------+----------------------+-------------------+--------------------+---------------+\n",
      "| Balanced Dataset 2          |               1800 |                  200 |               112 |                 12 |             4 |\n",
      "+-----------------------------+--------------------+----------------------+-------------------+--------------------+---------------+\n",
      "| Balanced Combined Dataset   |               4446 |                  413 |               277 |                 25 |             4 |\n",
      "+-----------------------------+--------------------+----------------------+-------------------+--------------------+---------------+\n",
      "\n",
      "============================================================\n",
      "TRAINING ON Imbalanced Dataset 1\n",
      "============================================================\n",
      "\n",
      "Model Summary for 3-Class Dataset:\n",
      "Model: \"MobDenseNet_CBAM_3class\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 128, 128, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " mobilenet_0.75_128 (Functional  (None, 4, 4, 768)   1832976     ['input_4[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " densenet121 (Functional)       (None, 4, 4, 1024)   7037504     ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " global_average_pooling2d_3 (Gl  (None, 768)         0           ['mobilenet_0.75_128[0][0]']     \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " global_max_pooling2d_2 (Global  (None, 768)         0           ['mobilenet_0.75_128[0][0]']     \n",
      " MaxPooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " global_average_pooling2d_4 (Gl  (None, 1024)        0           ['densenet121[0][0]']            \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " global_max_pooling2d_3 (Global  (None, 1024)        0           ['densenet121[0][0]']            \n",
      " MaxPooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 96)           73824       ['global_average_pooling2d_3[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'global_max_pooling2d_2[0][0]'] \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 128)          131200      ['global_average_pooling2d_4[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'global_max_pooling2d_3[0][0]'] \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 768)          74496       ['dense_5[0][0]',                \n",
      "                                                                  'dense_5[1][0]']                \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 1024)         132096      ['dense_7[0][0]',                \n",
      "                                                                  'dense_7[1][0]']                \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 768)          0           ['dense_6[0][0]',                \n",
      "                                                                  'dense_6[1][0]']                \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 1024)         0           ['dense_8[0][0]',                \n",
      "                                                                  'dense_8[1][0]']                \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 768)          0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 1024)         0           ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)            (None, 1, 1, 768)    0           ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " reshape_3 (Reshape)            (None, 1, 1, 1024)   0           ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " multiply_4 (Multiply)          (None, 4, 4, 768)    0           ['mobilenet_0.75_128[0][0]',     \n",
      "                                                                  'reshape_2[0][0]']              \n",
      "                                                                                                  \n",
      " multiply_6 (Multiply)          (None, 4, 4, 1024)   0           ['densenet121[0][0]',            \n",
      "                                                                  'reshape_3[0][0]']              \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_2 (TFOpLam  (None, 4, 4, 1)     0           ['multiply_4[0][0]']             \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_2 (TFOpLamb  (None, 4, 4, 1)     0           ['multiply_4[0][0]']             \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_3 (TFOpLam  (None, 4, 4, 1)     0           ['multiply_6[0][0]']             \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_3 (TFOpLamb  (None, 4, 4, 1)     0           ['multiply_6[0][0]']             \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 4, 4, 2)      0           ['tf.math.reduce_mean_2[0][0]',  \n",
      "                                                                  'tf.math.reduce_max_2[0][0]']   \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 4, 4, 2)      0           ['tf.math.reduce_mean_3[0][0]',  \n",
      "                                                                  'tf.math.reduce_max_3[0][0]']   \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 4, 4, 1)      99          ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 4, 4, 1)      99          ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " multiply_5 (Multiply)          (None, 4, 4, 768)    0           ['multiply_4[0][0]',             \n",
      "                                                                  'conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " multiply_7 (Multiply)          (None, 4, 4, 1024)   0           ['multiply_6[0][0]',             \n",
      "                                                                  'conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " cbam_mobilenet (Lambda)        (None, 4, 4, 768)    0           ['multiply_5[0][0]']             \n",
      "                                                                                                  \n",
      " cbam_densenet (Lambda)         (None, 4, 4, 1024)   0           ['multiply_7[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 4, 4, 1792)   0           ['cbam_mobilenet[0][0]',         \n",
      "                                                                  'cbam_densenet[0][0]']          \n",
      "                                                                                                  \n",
      " gradcam_conv2d (Conv2D)        (None, 4, 4, 256)    4129024     ['concatenate_5[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 4, 4, 256)   1024        ['gradcam_conv2d[0][0]']         \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " global_average_pooling2d_5 (Gl  (None, 256)         0           ['batch_normalization_1[0][0]']  \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 256)          0           ['global_average_pooling2d_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 3)            771         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 13,413,113\n",
      "Trainable params: 13,312,537\n",
      "Non-trainable params: 100,576\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "Hyper-Parameters (3)-Class Model):\n",
      "| Hyper-Parameter Name        | Value            |\n",
      "|----------------------------|------------------|\n",
      "| Activation Function         | relu              |\n",
      "| Kernel Initializer          | glorot_uniform   |\n",
      "| Initial Learning Rate       | 0.002            |\n",
      "| Optimizer                   | nadam            |\n",
      "| Output Activation Function  | softmax            |\n",
      "| Output Kernel Initializer   | random_uniform |\n",
      "| Number of Output Classes    | 3                |\n",
      "| Dropout Rate                | 0.2               |\n",
      "Starting training with batch_size=16, input_size=(128, 128, 3)\n",
      "Epoch 1/5\n",
      "344/344 [==============================] - 273s 625ms/step - loss: 0.3232 - accuracy: 0.8774 - val_loss: 0.1618 - val_accuracy: 0.9424 - lr: 1.0000e-04\n",
      "Epoch 2/5\n",
      "344/344 [==============================] - 214s 621ms/step - loss: 0.1911 - accuracy: 0.9320 - val_loss: 0.2424 - val_accuracy: 0.9030 - lr: 1.0000e-04\n",
      "Epoch 3/5\n",
      "344/344 [==============================] - 219s 635ms/step - loss: 0.1498 - accuracy: 0.9474 - val_loss: 0.1083 - val_accuracy: 0.9589 - lr: 1.0000e-04\n",
      "Epoch 4/5\n",
      "344/344 [==============================] - 225s 651ms/step - loss: 0.1076 - accuracy: 0.9640 - val_loss: 0.0756 - val_accuracy: 0.9704 - lr: 1.0000e-04\n",
      "Epoch 5/5\n",
      "344/344 [==============================] - 210s 611ms/step - loss: 0.1007 - accuracy: 0.9658 - val_loss: 0.1524 - val_accuracy: 0.9572 - lr: 1.0000e-04\n",
      " Training completed successfully!\n",
      "\n",
      "Classification Report for Imbalanced Dataset 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Glioma       0.92      1.00      0.96       286\n",
      "  Meningioma       0.98      0.83      0.90       142\n",
      "   Pituitary       1.00      0.98      0.99       186\n",
      "\n",
      "    accuracy                           0.95       614\n",
      "   macro avg       0.96      0.94      0.95       614\n",
      "weighted avg       0.96      0.95      0.95       614\n",
      "\n",
      " Model saved successfully to E:\\Data001\\SplitData\\hybrid_MobDenseNetwithCBAMattention_Output_using_tensorflow\\mobdensenet_cbam_1.h5\n",
      " Memory cleared for next dataset\n",
      "[Imbalanced Dataset 1] Processing completed.\n",
      "\n",
      "============================================================\n",
      "TRAINING ON Imbalanced Dataset 2\n",
      "============================================================\n",
      "\n",
      "Model Summary for 4-Class Dataset:\n",
      "Model: \"MobDenseNet_CBAM_4class\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 128, 128, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " mobilenet_0.75_128 (Functional  (None, 4, 4, 768)   1832976     ['input_1[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " densenet121 (Functional)       (None, 4, 4, 1024)   7037504     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 768)         0           ['mobilenet_0.75_128[0][0]']     \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " global_max_pooling2d (GlobalMa  (None, 768)         0           ['mobilenet_0.75_128[0][0]']     \n",
      " xPooling2D)                                                                                      \n",
      "                                                                                                  \n",
      " global_average_pooling2d_1 (Gl  (None, 1024)        0           ['densenet121[0][0]']            \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " global_max_pooling2d_1 (Global  (None, 1024)        0           ['densenet121[0][0]']            \n",
      " MaxPooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 96)           73824       ['global_average_pooling2d[0][0]'\n",
      "                                                                 , 'global_max_pooling2d[0][0]']  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 128)          131200      ['global_average_pooling2d_1[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'global_max_pooling2d_1[0][0]'] \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 768)          74496       ['dense[0][0]',                  \n",
      "                                                                  'dense[1][0]']                  \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1024)         132096      ['dense_2[0][0]',                \n",
      "                                                                  'dense_2[1][0]']                \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 768)          0           ['dense_1[0][0]',                \n",
      "                                                                  'dense_1[1][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 1024)         0           ['dense_3[0][0]',                \n",
      "                                                                  'dense_3[1][0]']                \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 768)          0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 1024)         0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 1, 1, 768)    0           ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 1, 1, 1024)   0           ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, 4, 4, 768)    0           ['mobilenet_0.75_128[0][0]',     \n",
      "                                                                  'reshape[0][0]']                \n",
      "                                                                                                  \n",
      " multiply_2 (Multiply)          (None, 4, 4, 1024)   0           ['densenet121[0][0]',            \n",
      "                                                                  'reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean (TFOpLambd  (None, 4, 4, 1)     0           ['multiply[0][0]']               \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.reduce_max (TFOpLambda  (None, 4, 4, 1)     0           ['multiply[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_1 (TFOpLam  (None, 4, 4, 1)     0           ['multiply_2[0][0]']             \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_1 (TFOpLamb  (None, 4, 4, 1)     0           ['multiply_2[0][0]']             \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 4, 4, 2)      0           ['tf.math.reduce_mean[0][0]',    \n",
      "                                                                  'tf.math.reduce_max[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 4, 4, 2)      0           ['tf.math.reduce_mean_1[0][0]',  \n",
      "                                                                  'tf.math.reduce_max_1[0][0]']   \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 4, 4, 1)      99          ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 4, 4, 1)      99          ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " multiply_1 (Multiply)          (None, 4, 4, 768)    0           ['multiply[0][0]',               \n",
      "                                                                  'conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " multiply_3 (Multiply)          (None, 4, 4, 1024)   0           ['multiply_2[0][0]',             \n",
      "                                                                  'conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " cbam_mobilenet (Lambda)        (None, 4, 4, 768)    0           ['multiply_1[0][0]']             \n",
      "                                                                                                  \n",
      " cbam_densenet (Lambda)         (None, 4, 4, 1024)   0           ['multiply_3[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 4, 4, 1792)   0           ['cbam_mobilenet[0][0]',         \n",
      "                                                                  'cbam_densenet[0][0]']          \n",
      "                                                                                                  \n",
      " gradcam_conv2d (Conv2D)        (None, 4, 4, 256)    4129024     ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 4, 4, 256)   1024        ['gradcam_conv2d[0][0]']         \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " global_average_pooling2d_2 (Gl  (None, 256)         0           ['batch_normalization[0][0]']    \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 256)          0           ['global_average_pooling2d_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 4)            1028        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 13,413,370\n",
      "Trainable params: 13,312,794\n",
      "Non-trainable params: 100,576\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "Hyper-Parameters (4)-Class Model):\n",
      "| Hyper-Parameter Name        | Value            |\n",
      "|----------------------------|------------------|\n",
      "| Activation Function         | relu              |\n",
      "| Kernel Initializer          | glorot_uniform   |\n",
      "| Initial Learning Rate       | 0.002            |\n",
      "| Optimizer                   | nadam            |\n",
      "| Output Activation Function  | softmax            |\n",
      "| Output Kernel Initializer   | random_uniform |\n",
      "| Number of Output Classes    | 4                |\n",
      "| Dropout Rate                | 0.2               |\n",
      "Starting training with batch_size=16, input_size=(128, 128, 3)\n",
      "Epoch 1/5\n",
      "367/367 [==============================] - 1043s 3s/step - loss: 0.4892 - accuracy: 0.8239 - val_loss: 0.2894 - val_accuracy: 0.9024 - lr: 1.0000e-04\n",
      "Epoch 2/5\n",
      "367/367 [==============================] - 153s 415ms/step - loss: 0.2592 - accuracy: 0.9116 - val_loss: 0.2074 - val_accuracy: 0.9253 - lr: 1.0000e-04\n",
      "Epoch 3/5\n",
      "367/367 [==============================] - 165s 450ms/step - loss: 0.1937 - accuracy: 0.9339 - val_loss: 0.2042 - val_accuracy: 0.9253 - lr: 1.0000e-04\n",
      "Epoch 4/5\n",
      "367/367 [==============================] - 171s 467ms/step - loss: 0.1658 - accuracy: 0.9429 - val_loss: 0.1356 - val_accuracy: 0.9527 - lr: 1.0000e-04\n",
      "Epoch 5/5\n",
      "367/367 [==============================] - 176s 479ms/step - loss: 0.1400 - accuracy: 0.9499 - val_loss: 0.1408 - val_accuracy: 0.9466 - lr: 1.0000e-04\n",
      " Training completed successfully!\n",
      "\n",
      "Classification Report for Imbalanced Dataset 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Glioma       0.99      0.90      0.94       186\n",
      "  Meningioma       0.93      0.94      0.94       188\n",
      "    No-tumor       0.96      1.00      0.98       100\n",
      "   Pituitary       0.94      0.99      0.96       182\n",
      "\n",
      "    accuracy                           0.95       656\n",
      "   macro avg       0.95      0.96      0.96       656\n",
      "weighted avg       0.95      0.95      0.95       656\n",
      "\n",
      " Model saved successfully to E:\\Data002\\SplitData\\hybrid_MobDenseNetwithCBAMattention_Output_using_tensorflow\\mobdensenet_cbam_2.h5\n",
      " Memory cleared for next dataset\n",
      "[Imbalanced Dataset 2] Processing completed.\n",
      "\n",
      "============================================================\n",
      "TRAINING ON Imbalanced Combined Dataset\n",
      "============================================================\n",
      "\n",
      "Model Summary for 4-Class Dataset:\n",
      "Model: \"MobDenseNet_CBAM_4class\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 128, 128, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " mobilenet_0.75_128 (Functional  (None, 4, 4, 768)   1832976     ['input_1[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " densenet121 (Functional)       (None, 4, 4, 1024)   7037504     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 768)         0           ['mobilenet_0.75_128[0][0]']     \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " global_max_pooling2d (GlobalMa  (None, 768)         0           ['mobilenet_0.75_128[0][0]']     \n",
      " xPooling2D)                                                                                      \n",
      "                                                                                                  \n",
      " global_average_pooling2d_1 (Gl  (None, 1024)        0           ['densenet121[0][0]']            \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " global_max_pooling2d_1 (Global  (None, 1024)        0           ['densenet121[0][0]']            \n",
      " MaxPooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 96)           73824       ['global_average_pooling2d[0][0]'\n",
      "                                                                 , 'global_max_pooling2d[0][0]']  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 128)          131200      ['global_average_pooling2d_1[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'global_max_pooling2d_1[0][0]'] \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 768)          74496       ['dense[0][0]',                  \n",
      "                                                                  'dense[1][0]']                  \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1024)         132096      ['dense_2[0][0]',                \n",
      "                                                                  'dense_2[1][0]']                \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 768)          0           ['dense_1[0][0]',                \n",
      "                                                                  'dense_1[1][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 1024)         0           ['dense_3[0][0]',                \n",
      "                                                                  'dense_3[1][0]']                \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 768)          0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 1024)         0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 1, 1, 768)    0           ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 1, 1, 1024)   0           ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, 4, 4, 768)    0           ['mobilenet_0.75_128[0][0]',     \n",
      "                                                                  'reshape[0][0]']                \n",
      "                                                                                                  \n",
      " multiply_2 (Multiply)          (None, 4, 4, 1024)   0           ['densenet121[0][0]',            \n",
      "                                                                  'reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean (TFOpLambd  (None, 4, 4, 1)     0           ['multiply[0][0]']               \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.reduce_max (TFOpLambda  (None, 4, 4, 1)     0           ['multiply[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_1 (TFOpLam  (None, 4, 4, 1)     0           ['multiply_2[0][0]']             \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_1 (TFOpLamb  (None, 4, 4, 1)     0           ['multiply_2[0][0]']             \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 4, 4, 2)      0           ['tf.math.reduce_mean[0][0]',    \n",
      "                                                                  'tf.math.reduce_max[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 4, 4, 2)      0           ['tf.math.reduce_mean_1[0][0]',  \n",
      "                                                                  'tf.math.reduce_max_1[0][0]']   \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 4, 4, 1)      99          ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 4, 4, 1)      99          ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " multiply_1 (Multiply)          (None, 4, 4, 768)    0           ['multiply[0][0]',               \n",
      "                                                                  'conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " multiply_3 (Multiply)          (None, 4, 4, 1024)   0           ['multiply_2[0][0]',             \n",
      "                                                                  'conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " cbam_mobilenet (Lambda)        (None, 4, 4, 768)    0           ['multiply_1[0][0]']             \n",
      "                                                                                                  \n",
      " cbam_densenet (Lambda)         (None, 4, 4, 1024)   0           ['multiply_3[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 4, 4, 1792)   0           ['cbam_mobilenet[0][0]',         \n",
      "                                                                  'cbam_densenet[0][0]']          \n",
      "                                                                                                  \n",
      " gradcam_conv2d (Conv2D)        (None, 4, 4, 256)    4129024     ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 4, 4, 256)   1024        ['gradcam_conv2d[0][0]']         \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " global_average_pooling2d_2 (Gl  (None, 256)         0           ['batch_normalization[0][0]']    \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 256)          0           ['global_average_pooling2d_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 4)            1028        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 13,413,370\n",
      "Trainable params: 13,312,794\n",
      "Non-trainable params: 100,576\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "Hyper-Parameters (4)-Class Model):\n",
      "| Hyper-Parameter Name        | Value            |\n",
      "|----------------------------|------------------|\n",
      "| Activation Function         | relu              |\n",
      "| Kernel Initializer          | glorot_uniform   |\n",
      "| Initial Learning Rate       | 0.002            |\n",
      "| Optimizer                   | nadam            |\n",
      "| Output Activation Function  | softmax            |\n",
      "| Output Kernel Initializer   | random_uniform |\n",
      "| Number of Output Classes    | 4                |\n",
      "| Dropout Rate                | 0.2               |\n",
      "Starting training with batch_size=16, input_size=(128, 128, 3)\n",
      "Epoch 1/5\n",
      "1067/1067 [==============================] - 1126s 1s/step - loss: 0.3304 - accuracy: 0.8889 - val_loss: 0.1392 - val_accuracy: 0.9603 - lr: 1.0000e-04\n",
      "Epoch 2/5\n",
      "1067/1067 [==============================] - 1086s 1s/step - loss: 0.1708 - accuracy: 0.9445 - val_loss: 0.1002 - val_accuracy: 0.9698 - lr: 1.0000e-04\n",
      "Epoch 3/5\n",
      "1067/1067 [==============================] - 924s 865ms/step - loss: 0.1257 - accuracy: 0.9573 - val_loss: 0.0897 - val_accuracy: 0.9719 - lr: 1.0000e-04\n",
      "Epoch 4/5\n",
      "1067/1067 [==============================] - 1120s 1s/step - loss: 0.0951 - accuracy: 0.9685 - val_loss: 0.0672 - val_accuracy: 0.9767 - lr: 1.0000e-04\n",
      "Epoch 5/5\n",
      "1067/1067 [==============================] - 1610s 2s/step - loss: 0.0764 - accuracy: 0.9751 - val_loss: 0.0496 - val_accuracy: 0.9836 - lr: 1.0000e-04\n",
      " Training completed successfully!\n",
      "\n",
      "Classification Report for Imbalanced Combined Dataset:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Glioma       1.00      0.97      0.99       707\n",
      "  Meningioma       0.96      0.98      0.97       494\n",
      "    No-tumor       0.98      0.99      0.98       150\n",
      "   Pituitary       0.99      0.99      0.99       551\n",
      "\n",
      "    accuracy                           0.98      1902\n",
      "   macro avg       0.98      0.98      0.98      1902\n",
      "weighted avg       0.98      0.98      0.98      1902\n",
      "\n",
      " Model saved successfully to E:\\CombineData001_002\\SplitData\\hybrid_MobDenseNetwithCBAMattention_Output_using_tensorflow\\mobdensenet_cbam_3.h5\n",
      " Memory cleared for next dataset\n",
      "[Imbalanced Combined Dataset] Processing completed.\n",
      "\n",
      "============================================================\n",
      "TRAINING ON Balanced Dataset 1\n",
      "============================================================\n",
      "\n",
      "Model Summary for 3-Class Dataset:\n",
      "Model: \"MobDenseNet_CBAM_3class\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 128, 128, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " mobilenet_0.75_128 (Functional  (None, 4, 4, 768)   1832976     ['input_1[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " densenet121 (Functional)       (None, 4, 4, 1024)   7037504     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 768)         0           ['mobilenet_0.75_128[0][0]']     \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " global_max_pooling2d (GlobalMa  (None, 768)         0           ['mobilenet_0.75_128[0][0]']     \n",
      " xPooling2D)                                                                                      \n",
      "                                                                                                  \n",
      " global_average_pooling2d_1 (Gl  (None, 1024)        0           ['densenet121[0][0]']            \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " global_max_pooling2d_1 (Global  (None, 1024)        0           ['densenet121[0][0]']            \n",
      " MaxPooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 96)           73824       ['global_average_pooling2d[0][0]'\n",
      "                                                                 , 'global_max_pooling2d[0][0]']  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 128)          131200      ['global_average_pooling2d_1[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'global_max_pooling2d_1[0][0]'] \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 768)          74496       ['dense[0][0]',                  \n",
      "                                                                  'dense[1][0]']                  \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1024)         132096      ['dense_2[0][0]',                \n",
      "                                                                  'dense_2[1][0]']                \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 768)          0           ['dense_1[0][0]',                \n",
      "                                                                  'dense_1[1][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 1024)         0           ['dense_3[0][0]',                \n",
      "                                                                  'dense_3[1][0]']                \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 768)          0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 1024)         0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 1, 1, 768)    0           ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 1, 1, 1024)   0           ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, 4, 4, 768)    0           ['mobilenet_0.75_128[0][0]',     \n",
      "                                                                  'reshape[0][0]']                \n",
      "                                                                                                  \n",
      " multiply_2 (Multiply)          (None, 4, 4, 1024)   0           ['densenet121[0][0]',            \n",
      "                                                                  'reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean (TFOpLambd  (None, 4, 4, 1)     0           ['multiply[0][0]']               \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.reduce_max (TFOpLambda  (None, 4, 4, 1)     0           ['multiply[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_1 (TFOpLam  (None, 4, 4, 1)     0           ['multiply_2[0][0]']             \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_1 (TFOpLamb  (None, 4, 4, 1)     0           ['multiply_2[0][0]']             \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 4, 4, 2)      0           ['tf.math.reduce_mean[0][0]',    \n",
      "                                                                  'tf.math.reduce_max[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 4, 4, 2)      0           ['tf.math.reduce_mean_1[0][0]',  \n",
      "                                                                  'tf.math.reduce_max_1[0][0]']   \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 4, 4, 1)      99          ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 4, 4, 1)      99          ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " multiply_1 (Multiply)          (None, 4, 4, 768)    0           ['multiply[0][0]',               \n",
      "                                                                  'conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " multiply_3 (Multiply)          (None, 4, 4, 1024)   0           ['multiply_2[0][0]',             \n",
      "                                                                  'conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " cbam_mobilenet (Lambda)        (None, 4, 4, 768)    0           ['multiply_1[0][0]']             \n",
      "                                                                                                  \n",
      " cbam_densenet (Lambda)         (None, 4, 4, 1024)   0           ['multiply_3[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 4, 4, 1792)   0           ['cbam_mobilenet[0][0]',         \n",
      "                                                                  'cbam_densenet[0][0]']          \n",
      "                                                                                                  \n",
      " gradcam_conv2d (Conv2D)        (None, 4, 4, 256)    4129024     ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 4, 4, 256)   1024        ['gradcam_conv2d[0][0]']         \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " global_average_pooling2d_2 (Gl  (None, 256)         0           ['batch_normalization[0][0]']    \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 256)          0           ['global_average_pooling2d_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 3)            771         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 13,413,113\n",
      "Trainable params: 13,312,537\n",
      "Non-trainable params: 100,576\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "Hyper-Parameters (3)-Class Model):\n",
      "| Hyper-Parameter Name        | Value            |\n",
      "|----------------------------|------------------|\n",
      "| Activation Function         | relu              |\n",
      "| Kernel Initializer          | glorot_uniform   |\n",
      "| Initial Learning Rate       | 0.002            |\n",
      "| Optimizer                   | nadam            |\n",
      "| Output Activation Function  | softmax            |\n",
      "| Output Kernel Initializer   | random_uniform |\n",
      "| Number of Output Classes    | 3                |\n",
      "| Dropout Rate                | 0.2               |\n",
      "Starting training with batch_size=16, input_size=(128, 128, 3)\n",
      "Epoch 1/5\n",
      "119/119 [==============================] - 313s 2s/step - loss: 0.4973 - accuracy: 0.7968 - val_loss: 0.6170 - val_accuracy: 0.7356 - lr: 1.0000e-04\n",
      "Epoch 2/5\n",
      "119/119 [==============================] - 222s 2s/step - loss: 0.3023 - accuracy: 0.8786 - val_loss: 0.5178 - val_accuracy: 0.7837 - lr: 1.0000e-04\n",
      "Epoch 3/5\n",
      "119/119 [==============================] - 206s 2s/step - loss: 0.2412 - accuracy: 0.9135 - val_loss: 0.2941 - val_accuracy: 0.8798 - lr: 1.0000e-04\n",
      "Epoch 4/5\n",
      "119/119 [==============================] - 219s 2s/step - loss: 0.2077 - accuracy: 0.9272 - val_loss: 0.3352 - val_accuracy: 0.8558 - lr: 1.0000e-04\n",
      "Epoch 5/5\n",
      "119/119 [==============================] - 182s 2s/step - loss: 0.1986 - accuracy: 0.9251 - val_loss: 0.2226 - val_accuracy: 0.9231 - lr: 1.0000e-04\n",
      " Training completed successfully!\n",
      "\n",
      "Classification Report for Balanced Dataset 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Glioma       0.93      0.97      0.95        71\n",
      "  Meningioma       0.93      0.93      0.93        71\n",
      "   Pituitary       0.99      0.94      0.96        71\n",
      "\n",
      "    accuracy                           0.95       213\n",
      "   macro avg       0.95      0.95      0.95       213\n",
      "weighted avg       0.95      0.95      0.95       213\n",
      "\n",
      " Model saved successfully to E:\\Data001\\SplitData1\\hybrid_MobDenseNetwithCBAMattention_Output_using_tensorflow\\balanceoutput\\mobdensenet_cbam_4.h5\n",
      " Memory cleared for next dataset\n",
      "[Balanced Dataset 1] Processing completed.\n",
      "\n",
      "============================================================\n",
      "TRAINING ON Balanced Dataset 2\n",
      "============================================================\n",
      "\n",
      "Model Summary for 4-Class Dataset:\n",
      "Model: \"MobDenseNet_CBAM_4class\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 128, 128, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " mobilenet_0.75_128 (Functional  (None, 4, 4, 768)   1832976     ['input_1[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " densenet121 (Functional)       (None, 4, 4, 1024)   7037504     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 768)         0           ['mobilenet_0.75_128[0][0]']     \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " global_max_pooling2d (GlobalMa  (None, 768)         0           ['mobilenet_0.75_128[0][0]']     \n",
      " xPooling2D)                                                                                      \n",
      "                                                                                                  \n",
      " global_average_pooling2d_1 (Gl  (None, 1024)        0           ['densenet121[0][0]']            \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " global_max_pooling2d_1 (Global  (None, 1024)        0           ['densenet121[0][0]']            \n",
      " MaxPooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 96)           73824       ['global_average_pooling2d[0][0]'\n",
      "                                                                 , 'global_max_pooling2d[0][0]']  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 128)          131200      ['global_average_pooling2d_1[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'global_max_pooling2d_1[0][0]'] \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 768)          74496       ['dense[0][0]',                  \n",
      "                                                                  'dense[1][0]']                  \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1024)         132096      ['dense_2[0][0]',                \n",
      "                                                                  'dense_2[1][0]']                \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 768)          0           ['dense_1[0][0]',                \n",
      "                                                                  'dense_1[1][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 1024)         0           ['dense_3[0][0]',                \n",
      "                                                                  'dense_3[1][0]']                \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 768)          0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 1024)         0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 1, 1, 768)    0           ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 1, 1, 1024)   0           ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, 4, 4, 768)    0           ['mobilenet_0.75_128[0][0]',     \n",
      "                                                                  'reshape[0][0]']                \n",
      "                                                                                                  \n",
      " multiply_2 (Multiply)          (None, 4, 4, 1024)   0           ['densenet121[0][0]',            \n",
      "                                                                  'reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean (TFOpLambd  (None, 4, 4, 1)     0           ['multiply[0][0]']               \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.reduce_max (TFOpLambda  (None, 4, 4, 1)     0           ['multiply[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_1 (TFOpLam  (None, 4, 4, 1)     0           ['multiply_2[0][0]']             \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_1 (TFOpLamb  (None, 4, 4, 1)     0           ['multiply_2[0][0]']             \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 4, 4, 2)      0           ['tf.math.reduce_mean[0][0]',    \n",
      "                                                                  'tf.math.reduce_max[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 4, 4, 2)      0           ['tf.math.reduce_mean_1[0][0]',  \n",
      "                                                                  'tf.math.reduce_max_1[0][0]']   \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 4, 4, 1)      99          ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 4, 4, 1)      99          ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " multiply_1 (Multiply)          (None, 4, 4, 768)    0           ['multiply[0][0]',               \n",
      "                                                                  'conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " multiply_3 (Multiply)          (None, 4, 4, 1024)   0           ['multiply_2[0][0]',             \n",
      "                                                                  'conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " cbam_mobilenet (Lambda)        (None, 4, 4, 768)    0           ['multiply_1[0][0]']             \n",
      "                                                                                                  \n",
      " cbam_densenet (Lambda)         (None, 4, 4, 1024)   0           ['multiply_3[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 4, 4, 1792)   0           ['cbam_mobilenet[0][0]',         \n",
      "                                                                  'cbam_densenet[0][0]']          \n",
      "                                                                                                  \n",
      " gradcam_conv2d (Conv2D)        (None, 4, 4, 256)    4129024     ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 4, 4, 256)   1024        ['gradcam_conv2d[0][0]']         \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " global_average_pooling2d_2 (Gl  (None, 256)         0           ['batch_normalization[0][0]']    \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 256)          0           ['global_average_pooling2d_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 4)            1028        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 13,413,370\n",
      "Trainable params: 13,312,794\n",
      "Non-trainable params: 100,576\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "Hyper-Parameters (4)-Class Model):\n",
      "| Hyper-Parameter Name        | Value            |\n",
      "|----------------------------|------------------|\n",
      "| Activation Function         | relu              |\n",
      "| Kernel Initializer          | glorot_uniform   |\n",
      "| Initial Learning Rate       | 0.002            |\n",
      "| Optimizer                   | nadam            |\n",
      "| Output Activation Function  | softmax            |\n",
      "| Output Kernel Initializer   | random_uniform |\n",
      "| Number of Output Classes    | 4                |\n",
      "| Dropout Rate                | 0.2               |\n",
      "Starting training with batch_size=16, input_size=(128, 128, 3)\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 309s 2s/step - loss: 0.6085 - accuracy: 0.7752 - val_loss: 0.5057 - val_accuracy: 0.8177 - lr: 1.0000e-04\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 177s 2s/step - loss: 0.3021 - accuracy: 0.9030 - val_loss: 0.3142 - val_accuracy: 0.8594 - lr: 1.0000e-04\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 163s 1s/step - loss: 0.2309 - accuracy: 0.9187 - val_loss: 0.2106 - val_accuracy: 0.9219 - lr: 1.0000e-04\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 197s 2s/step - loss: 0.2016 - accuracy: 0.9288 - val_loss: 0.2537 - val_accuracy: 0.9010 - lr: 1.0000e-04\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 162s 1s/step - loss: 0.1530 - accuracy: 0.9490 - val_loss: 0.1121 - val_accuracy: 0.9740 - lr: 1.0000e-04\n",
      " Training completed successfully!\n",
      "\n",
      "Classification Report for Balanced Dataset 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Glioma       1.00      0.98      0.99        50\n",
      "  Meningioma       0.96      0.94      0.95        50\n",
      "    No-tumor       0.94      0.98      0.96        50\n",
      "   Pituitary       0.96      0.96      0.96        50\n",
      "\n",
      "    accuracy                           0.96       200\n",
      "   macro avg       0.97      0.96      0.97       200\n",
      "weighted avg       0.97      0.96      0.97       200\n",
      "\n",
      " Model saved successfully to E:\\Data002\\SplitData1\\hybrid_MobDenseNetwithCBAMattention_Output_using_tensorflow\\balanceoutput\\mobdensenet_cbam_5.h5\n",
      " Memory cleared for next dataset\n",
      "[Balanced Dataset 2] Processing completed.\n",
      "\n",
      "============================================================\n",
      "TRAINING ON Balanced Combined Dataset\n",
      "============================================================\n",
      "\n",
      "Model Summary for 4-Class Dataset:\n",
      "Model: \"MobDenseNet_CBAM_4class\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 128, 128, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " mobilenet_0.75_128 (Functional  (None, 4, 4, 768)   1832976     ['input_1[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " densenet121 (Functional)       (None, 4, 4, 1024)   7037504     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 768)         0           ['mobilenet_0.75_128[0][0]']     \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " global_max_pooling2d (GlobalMa  (None, 768)         0           ['mobilenet_0.75_128[0][0]']     \n",
      " xPooling2D)                                                                                      \n",
      "                                                                                                  \n",
      " global_average_pooling2d_1 (Gl  (None, 1024)        0           ['densenet121[0][0]']            \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " global_max_pooling2d_1 (Global  (None, 1024)        0           ['densenet121[0][0]']            \n",
      " MaxPooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 96)           73824       ['global_average_pooling2d[0][0]'\n",
      "                                                                 , 'global_max_pooling2d[0][0]']  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 128)          131200      ['global_average_pooling2d_1[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'global_max_pooling2d_1[0][0]'] \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 768)          74496       ['dense[0][0]',                  \n",
      "                                                                  'dense[1][0]']                  \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1024)         132096      ['dense_2[0][0]',                \n",
      "                                                                  'dense_2[1][0]']                \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 768)          0           ['dense_1[0][0]',                \n",
      "                                                                  'dense_1[1][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 1024)         0           ['dense_3[0][0]',                \n",
      "                                                                  'dense_3[1][0]']                \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 768)          0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 1024)         0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 1, 1, 768)    0           ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 1, 1, 1024)   0           ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, 4, 4, 768)    0           ['mobilenet_0.75_128[0][0]',     \n",
      "                                                                  'reshape[0][0]']                \n",
      "                                                                                                  \n",
      " multiply_2 (Multiply)          (None, 4, 4, 1024)   0           ['densenet121[0][0]',            \n",
      "                                                                  'reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean (TFOpLambd  (None, 4, 4, 1)     0           ['multiply[0][0]']               \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.reduce_max (TFOpLambda  (None, 4, 4, 1)     0           ['multiply[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_1 (TFOpLam  (None, 4, 4, 1)     0           ['multiply_2[0][0]']             \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_1 (TFOpLamb  (None, 4, 4, 1)     0           ['multiply_2[0][0]']             \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 4, 4, 2)      0           ['tf.math.reduce_mean[0][0]',    \n",
      "                                                                  'tf.math.reduce_max[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 4, 4, 2)      0           ['tf.math.reduce_mean_1[0][0]',  \n",
      "                                                                  'tf.math.reduce_max_1[0][0]']   \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 4, 4, 1)      99          ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 4, 4, 1)      99          ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " multiply_1 (Multiply)          (None, 4, 4, 768)    0           ['multiply[0][0]',               \n",
      "                                                                  'conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " multiply_3 (Multiply)          (None, 4, 4, 1024)   0           ['multiply_2[0][0]',             \n",
      "                                                                  'conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " cbam_mobilenet (Lambda)        (None, 4, 4, 768)    0           ['multiply_1[0][0]']             \n",
      "                                                                                                  \n",
      " cbam_densenet (Lambda)         (None, 4, 4, 1024)   0           ['multiply_3[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 4, 4, 1792)   0           ['cbam_mobilenet[0][0]',         \n",
      "                                                                  'cbam_densenet[0][0]']          \n",
      "                                                                                                  \n",
      " gradcam_conv2d (Conv2D)        (None, 4, 4, 256)    4129024     ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 4, 4, 256)   1024        ['gradcam_conv2d[0][0]']         \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " global_average_pooling2d_2 (Gl  (None, 256)         0           ['batch_normalization[0][0]']    \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 256)          0           ['global_average_pooling2d_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 4)            1028        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 13,413,370\n",
      "Trainable params: 13,312,794\n",
      "Non-trainable params: 100,576\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "Hyper-Parameters (4)-Class Model):\n",
      "| Hyper-Parameter Name        | Value            |\n",
      "|----------------------------|------------------|\n",
      "| Activation Function         | relu              |\n",
      "| Kernel Initializer          | glorot_uniform   |\n",
      "| Initial Learning Rate       | 0.002            |\n",
      "| Optimizer                   | nadam            |\n",
      "| Output Activation Function  | softmax            |\n",
      "| Output Kernel Initializer   | random_uniform |\n",
      "| Number of Output Classes    | 4                |\n",
      "| Dropout Rate                | 0.2               |\n",
      "Starting training with batch_size=16, input_size=(128, 128, 3)\n",
      "Epoch 1/5\n",
      "221/277 [======================>.......] - ETA: 1:15 - loss: 0.5323 - accuracy: 0.8087"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\n2 root error(s) found.\n  (0) UNKNOWN:  UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x000001C9DF4C6C00>\nTraceback (most recent call last):\n\n  File \"e:\\.env\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"e:\\.env\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"e:\\.env\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1035, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"e:\\.env\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 903, in wrapped_generator\n    for data in generator_fn():\n\n  File \"e:\\.env\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 1050, in generator_fn\n    yield x[i]\n\n  File \"e:\\.env\\lib\\site-packages\\keras\\preprocessing\\image.py\", line 116, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"e:\\.env\\lib\\site-packages\\keras\\preprocessing\\image.py\", line 370, in _get_batches_of_transformed_samples\n    img = image_utils.load_img(\n\n  File \"e:\\.env\\lib\\site-packages\\keras\\utils\\image_utils.py\", line 423, in load_img\n    img = pil_image.open(io.BytesIO(f.read()))\n\n  File \"e:\\.env\\lib\\site-packages\\PIL\\Image.py\", line 3536, in open\n    raise UnidentifiedImageError(msg)\n\nPIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x000001C9DF4C6C00>\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_2]]\n  (1) UNKNOWN:  UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x000001C9DF4C6C00>\nTraceback (most recent call last):\n\n  File \"e:\\.env\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"e:\\.env\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"e:\\.env\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1035, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"e:\\.env\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 903, in wrapped_generator\n    for data in generator_fn():\n\n  File \"e:\\.env\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 1050, in generator_fn\n    yield x[i]\n\n  File \"e:\\.env\\lib\\site-packages\\keras\\preprocessing\\image.py\", line 116, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"e:\\.env\\lib\\site-packages\\keras\\preprocessing\\image.py\", line 370, in _get_batches_of_transformed_samples\n    img = image_utils.load_img(\n\n  File \"e:\\.env\\lib\\site-packages\\keras\\utils\\image_utils.py\", line 423, in load_img\n    img = pil_image.open(io.BytesIO(f.read()))\n\n  File \"e:\\.env\\lib\\site-packages\\PIL\\Image.py\", line 3536, in open\n    raise UnidentifiedImageError(msg)\n\nPIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x000001C9DF4C6C00>\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_560721]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 437\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training with batch_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, input_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 437\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS_INITIAL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Training completed successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m tf\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mResourceExhaustedError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32me:\\.env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32me:\\.env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mUnknownError\u001b[0m: Graph execution error:\n\n2 root error(s) found.\n  (0) UNKNOWN:  UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x000001C9DF4C6C00>\nTraceback (most recent call last):\n\n  File \"e:\\.env\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"e:\\.env\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"e:\\.env\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1035, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"e:\\.env\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 903, in wrapped_generator\n    for data in generator_fn():\n\n  File \"e:\\.env\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 1050, in generator_fn\n    yield x[i]\n\n  File \"e:\\.env\\lib\\site-packages\\keras\\preprocessing\\image.py\", line 116, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"e:\\.env\\lib\\site-packages\\keras\\preprocessing\\image.py\", line 370, in _get_batches_of_transformed_samples\n    img = image_utils.load_img(\n\n  File \"e:\\.env\\lib\\site-packages\\keras\\utils\\image_utils.py\", line 423, in load_img\n    img = pil_image.open(io.BytesIO(f.read()))\n\n  File \"e:\\.env\\lib\\site-packages\\PIL\\Image.py\", line 3536, in open\n    raise UnidentifiedImageError(msg)\n\nPIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x000001C9DF4C6C00>\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_2]]\n  (1) UNKNOWN:  UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x000001C9DF4C6C00>\nTraceback (most recent call last):\n\n  File \"e:\\.env\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"e:\\.env\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"e:\\.env\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1035, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"e:\\.env\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 903, in wrapped_generator\n    for data in generator_fn():\n\n  File \"e:\\.env\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 1050, in generator_fn\n    yield x[i]\n\n  File \"e:\\.env\\lib\\site-packages\\keras\\preprocessing\\image.py\", line 116, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"e:\\.env\\lib\\site-packages\\keras\\preprocessing\\image.py\", line 370, in _get_batches_of_transformed_samples\n    img = image_utils.load_img(\n\n  File \"e:\\.env\\lib\\site-packages\\keras\\utils\\image_utils.py\", line 423, in load_img\n    img = pil_image.open(io.BytesIO(f.read()))\n\n  File \"e:\\.env\\lib\\site-packages\\PIL\\Image.py\", line 3536, in open\n    raise UnidentifiedImageError(msg)\n\nPIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x000001C9DF4C6C00>\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_560721]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "from tabulate import tabulate\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, GlobalMaxPooling2D, Concatenate, Multiply, Reshape, Conv2D, Input, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import applications\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import applications, layers, models, mixed_precision\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Dropout, Conv2D, BatchNormalization, GlobalAveragePooling2D,\n",
    "    GlobalMaxPooling2D, Multiply, Reshape, Concatenate\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# ==================== GPU MEMORY OPTIMIZATION ====================\n",
    "# Configure GPU memory growth to prevent OOM errors\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Enable memory growth\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\" GPU memory growth enabled for {len(gpus)} GPU(s)\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\" GPU configuration error: {e}\")\n",
    "\n",
    "# Enable mixed precision training (reduces memory usage by ~50%)\n",
    "from tensorflow.keras import mixed_precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "print(\" Mixed precision training enabled\")\n",
    "\n",
    "# ==================== USE YOUR EXISTING PARAMETERS ====================\n",
    "# Use the parameters you defined earlier\n",
    "batch_size = 16\n",
    "input_shape = (128, 128, 3)\n",
    "EPOCHS_INITIAL = 5\n",
    "print(f\" Using your parameters: batch_size={batch_size}, input_shape={input_shape}, EPOCHS={EPOCHS_INITIAL}\")\n",
    "\n",
    "# ==================== CBAM BLOCK ====================\n",
    "def cbam_block(input_feature, ratio=8, kernel_size=7, name=None):\n",
    "    avg_pool = GlobalAveragePooling2D()(input_feature)\n",
    "    max_pool = GlobalMaxPooling2D()(input_feature)\n",
    "    \n",
    "    channel = input_feature.shape[-1]\n",
    "    shared_layer_one = Dense(channel // ratio, activation='relu', kernel_initializer='he_normal')\n",
    "    shared_layer_two = Dense(channel, kernel_initializer='he_normal')\n",
    "    \n",
    "    avg_out = shared_layer_two(shared_layer_one(avg_pool))\n",
    "    max_out = shared_layer_two(shared_layer_one(max_pool))\n",
    "    \n",
    "    channel_attention = tf.keras.layers.Add()([avg_out, max_out])\n",
    "    channel_attention = tf.keras.layers.Activation('sigmoid')(channel_attention)\n",
    "    channel_attention = Reshape((1, 1, channel))(channel_attention)\n",
    "    channel_refined = Multiply()([input_feature, channel_attention])\n",
    "    \n",
    "    avg_pool_spatial = tf.reduce_mean(channel_refined, axis=-1, keepdims=True)\n",
    "    max_pool_spatial = tf.reduce_max(channel_refined, axis=-1, keepdims=True)\n",
    "    spatial_attention = Concatenate(axis=-1)([avg_pool_spatial, max_pool_spatial])\n",
    "    spatial_attention = Conv2D(1, kernel_size, padding='same', activation='sigmoid', kernel_initializer='he_normal')(spatial_attention)\n",
    "    spatial_refined = Multiply()([channel_refined, spatial_attention])\n",
    "    \n",
    "    if name:\n",
    "        spatial_refined = tf.keras.layers.Lambda(lambda x: x, name=name)(spatial_refined)\n",
    "    return spatial_refined\n",
    "\n",
    "# ==================== LIGHTWEIGHT MODEL BUILDING ====================\n",
    "def build_mobdensenet(\n",
    "    num_classes,\n",
    "    input_shape=input_shape,\n",
    "    model_name=\"MobDenseNet_CBAM\",\n",
    "    activation_function=\"relu\",\n",
    "    kernel_initializer=\"glorot_uniform\",\n",
    "    output_activation=\"softmax\",\n",
    "    output_kernel_initializer=\"random_uniform\",\n",
    "    dropout_rate=0.2,\n",
    "    learning_rate=0.002,\n",
    "    optimizer=\"nadam\",\n",
    "):\n",
    "    raw_input = Input(shape=input_shape)\n",
    "\n",
    "    # Use smaller MobileNet version\n",
    "    mob_model = applications.MobileNet(\n",
    "        input_shape=input_shape,\n",
    "        alpha=0.75,  # Reduced from 1.0 to save memory\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        pooling=None\n",
    "    )\n",
    "    mob_out = mob_model(raw_input)\n",
    "    mob_cbam = cbam_block(mob_out, name=\"cbam_mobilenet\")\n",
    "\n",
    "    # Use smaller DenseNet version\n",
    "    dense_model = applications.DenseNet121(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        input_shape=input_shape,\n",
    "        pooling=None\n",
    "    )\n",
    "    dense_out = dense_model(raw_input)\n",
    "    dense_cbam = cbam_block(dense_out, name=\"cbam_densenet\")\n",
    "\n",
    "\n",
    "    combined_features = Concatenate(axis=-1)([mob_cbam, dense_cbam])\n",
    "    \n",
    "    conv_block = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\", name=\"gradcam_conv2d\")(combined_features)\n",
    "    conv_block = BatchNormalization()(conv_block)\n",
    "    gap = GlobalAveragePooling2D()(conv_block)\n",
    "    dropout = Dropout(dropout_rate)(gap)\n",
    "    \n",
    "    # Use mixed precision compatible output layer\n",
    "    output = Dense(\n",
    "        num_classes,\n",
    "        activation=output_activation,\n",
    "        kernel_initializer=output_kernel_initializer,\n",
    "        dtype='float32'  # Ensure output is float32 for stability\n",
    "    )(dropout)\n",
    "\n",
    "    model = Model(inputs=raw_input, outputs=output, name=f\"{model_name}_{num_classes}class\")\n",
    "\n",
    "    print(f\"\\nModel Summary for {num_classes}-Class Dataset:\")\n",
    "    model.summary()\n",
    "\n",
    "    print(f\"\\nHyper-Parameters ({num_classes})-Class Model):\")\n",
    "    print(\"| Hyper-Parameter Name        | Value            |\")\n",
    "    print(\"|----------------------------|------------------|\")\n",
    "    print(f\"| Activation Function         | {activation_function}              |\")\n",
    "    print(f\"| Kernel Initializer          | {kernel_initializer}   |\")\n",
    "    print(f\"| Initial Learning Rate       | {learning_rate}            |\")\n",
    "    print(f\"| Optimizer                   | {optimizer}            |\")\n",
    "    print(f\"| Output Activation Function  | {output_activation}            |\")\n",
    "    print(f\"| Output Kernel Initializer   | {output_kernel_initializer} |\")\n",
    "    print(f\"| Number of Output Classes    | {num_classes}                |\")\n",
    "    print(f\"| Dropout Rate                | {dropout_rate}               |\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# ==================== DATASET CONFIGURATION ====================\n",
    "# Use your existing generators (remove the _cbam suffix since they don't exist)\n",
    "datasets = [\n",
    "    {\n",
    "        \"name\": \"Imbalanced Dataset 1\",\n",
    "        \"train_generator\": imbalance_train_generator1_cbam,\n",
    "        \"test_generator\": imbalance_test_generator1_cbam,\n",
    "        \"steps_per_epoch\": max(1, imbalance_train_generator1_cbam.samples // batch_size),\n",
    "        \"validation_steps\": max(1, imbalance_test_generator1_cbam.samples // batch_size),\n",
    "        \"output_dir\": r\"E:\\Data001\\SplitData\\hybrid_MobDenseNetwithCBAMattention_Output_using_tensorflow\",\n",
    "        \"class_names\": list(imbalance_train_generator1_cbam.class_indices.keys()),\n",
    "        \"num_classes\": len(imbalance_train_generator1_cbam.class_indices)\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Imbalanced Dataset 2\",\n",
    "        \"train_generator\": imbalance_train_generator2_cbam,\n",
    "        \"test_generator\": imbalance_test_generator2_cbam,\n",
    "        \"steps_per_epoch\": max(1, imbalance_train_generator2_cbam.samples // batch_size),\n",
    "        \"validation_steps\": max(1, imbalance_test_generator2_cbam.samples // batch_size),\n",
    "        \"output_dir\": r\"E:\\Data002\\SplitData\\hybrid_MobDenseNetwithCBAMattention_Output_using_tensorflow\",\n",
    "        \"class_names\": list(imbalance_train_generator2_cbam.class_indices.keys()),\n",
    "        \"num_classes\": len(imbalance_train_generator2_cbam.class_indices)\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Imbalanced Combined Dataset\",\n",
    "        \"train_generator\": imbalance_train_generator_combined_cbam,\n",
    "        \"test_generator\": imbalance_test_generator_combined_cbam,\n",
    "        \"steps_per_epoch\": max(1, imbalance_train_generator_combined_cbam.samples // batch_size),\n",
    "        \"validation_steps\": max(1, imbalance_test_generator_combined_cbam.samples // batch_size),\n",
    "        \"output_dir\": r\"E:\\CombineData001_002\\SplitData\\hybrid_MobDenseNetwithCBAMattention_Output_using_tensorflow\",\n",
    "        \"class_names\": list(imbalance_train_generator_combined_cbam.class_indices.keys()),\n",
    "        \"num_classes\": len(imbalance_train_generator_combined_cbam.class_indices)\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Balanced Dataset 1\",\n",
    "        \"train_generator\": balance_train_generator1_cbam,\n",
    "        \"test_generator\": balance_test_generator1_cbam,\n",
    "        \"steps_per_epoch\": max(1, balance_train_generator1_cbam.samples // batch_size),\n",
    "        \"validation_steps\": max(1, balance_test_generator1_cbam.samples // batch_size),\n",
    "        \"output_dir\": r\"E:\\Data001\\SplitData1\\hybrid_MobDenseNetwithCBAMattention_Output_using_tensorflow\\balanceoutput\",\n",
    "        \"class_names\": list(balance_train_generator1_cbam.class_indices.keys()),\n",
    "        \"num_classes\": len(balance_train_generator1_cbam.class_indices)\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Balanced Dataset 2\",\n",
    "        \"train_generator\": balance_train_generator2_cbam,\n",
    "        \"test_generator\": balance_test_generator2_cbam,\n",
    "        \"steps_per_epoch\": max(1, balance_train_generator2_cbam.samples // batch_size),\n",
    "        \"validation_steps\": max(1, balance_test_generator2_cbam.samples // batch_size),\n",
    "        \"output_dir\": r\"E:\\Data002\\SplitData1\\hybrid_MobDenseNetwithCBAMattention_Output_using_tensorflow\\balanceoutput\",\n",
    "        \"class_names\": list(balance_train_generator2_cbam.class_indices.keys()),\n",
    "        \"num_classes\": len(balance_train_generator2_cbam.class_indices)\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Balanced Combined Dataset\",\n",
    "        \"train_generator\": balance_train_generator_combined_cbam,\n",
    "        \"test_generator\": balance_test_generator_combined_cbam,\n",
    "        \"steps_per_epoch\": max(1, balance_train_generator_combined_cbam.samples // batch_size),\n",
    "        \"validation_steps\": max(1, balance_test_generator_combined_cbam.samples // batch_size),\n",
    "        \"output_dir\": r\"E:\\CombineData001_002\\SplitData1\\hybrid_MobDenseNetwithCBAMattention_Output_using_tensorflow\\balanceoutput\",\n",
    "        \"class_names\": list(balance_train_generator_combined_cbam.class_indices.keys()),\n",
    "        \"num_classes\": len(balance_train_generator_combined_cbam.class_indices)\n",
    "    }\n",
    "]\n",
    "\n",
    "# Verify dataset generators\n",
    "for dataset in datasets:\n",
    "    if not hasattr(dataset[\"train_generator\"], \"samples\") or not hasattr(dataset[\"test_generator\"], \"samples\"):\n",
    "        raise ValueError(f\"Generator for {dataset['name']} is missing 'samples' attribute.\")\n",
    "    if dataset[\"train_generator\"].samples == 0 or dataset[\"test_generator\"].samples == 0:\n",
    "        raise ValueError(f\"{dataset['name']} has zero samples in train or test generator.\")\n",
    "\n",
    "# Display dataset sizes and steps\n",
    "table_data = [\n",
    "    [d[\"name\"], d[\"train_generator\"].samples, d[\"test_generator\"].samples, \n",
    "     d[\"steps_per_epoch\"], d[\"validation_steps\"], d[\"num_classes\"]]\n",
    "    for d in datasets\n",
    "]\n",
    "headers = [\"Dataset\", \"Training Samples\", \"Validation Samples\", \"Steps per Epoch\", \"Validation Steps\", \"Num Classes\"]\n",
    "print(\"\\nDataset Sizes and Steps:\")\n",
    "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))\n",
    "\n",
    "MODEL_NAME = \"mobdensenet_cbam\"\n",
    "input_image_size = f\"{input_shape[0]}x{input_shape[1]}\"\n",
    "\n",
    "# Callbacks  memory optimization\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ModelCheckpoint('hybrid_model_best.h5', monitor='val_loss', save_best_only=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=15, min_lr=0.00002)\n",
    "]\n",
    "\n",
    "# ==================== TRAINING FUNCTIONS ====================\n",
    "def plot_training_history(history, output_dir, model_name, counter, class_names, y_true, y_pred):\n",
    "    # Create subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax1.plot(history.history['accuracy'], label='Training Accuracy', color='#1f77b4', linewidth=2)\n",
    "    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy', color='#ff7f0e', linewidth=2)\n",
    "    ax1.set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Accuracy', fontsize=12)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot loss\n",
    "    ax2.plot(history.history['loss'], label='Training Loss', color='#2ca02c', linewidth=2)\n",
    "    ax2.plot(history.history['val_loss'], label='Validation Loss', color='#d62728', linewidth=2)\n",
    "    ax2.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('Loss', fontsize=12)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f'{model_name}_training_validation_{counter}.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                cbar_kws={'shrink': 0.8})\n",
    "    plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Predicted Labels', fontsize=12)\n",
    "    plt.ylabel('True Labels', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f'{model_name}_confusion_matrix_{counter}.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Additional plots: Precision-Recall and F1-Score per class\n",
    "    precision = precision_score(y_true, y_pred, average=None)\n",
    "    recall = recall_score(y_true, y_pred, average=None)\n",
    "    f1 = f1_score(y_true, y_pred, average=None)\n",
    "    \n",
    "    # Precision-Recall bar plot\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    x = np.arange(len(class_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    ax1.bar(x - width, precision, width, label='Precision', alpha=0.8)\n",
    "    ax1.bar(x, recall, width, label='Recall', alpha=0.8)\n",
    "    ax1.bar(x + width, f1, width, label='F1-Score', alpha=0.8)\n",
    "    ax1.set_xlabel('Classes')\n",
    "    ax1.set_title('Precision, Recall, and F1-Score by Class')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(class_names, rotation=45, ha='right')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Training history detailed\n",
    "    ax2.plot(history.history['accuracy'], label='Train Accuracy', color='blue', linestyle='-')\n",
    "    ax2.plot(history.history['val_accuracy'], label='Val Accuracy', color='blue', linestyle='--')\n",
    "    ax2.plot(history.history['loss'], label='Train Loss', color='red', linestyle='-')\n",
    "    ax2.plot(history.history['val_loss'], label='Val Loss', color='red', linestyle='--')\n",
    "    ax2.set_title('Training History Detailed')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Class distribution\n",
    "    class_counts = np.bincount(y_true)\n",
    "    ax3.bar(range(len(class_counts)), class_counts, alpha=0.7)\n",
    "    ax3.set_title('Class Distribution in Test Set')\n",
    "    ax3.set_xlabel('Class Index')\n",
    "    ax3.set_ylabel('Count')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f'{model_name}_detailed_metrics_{counter}.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def save_chartjs_configs(history, output_dir, model_name, counter):\n",
    "    try:\n",
    "        loss_chart = {\n",
    "            \"type\": \"line\",\n",
    "            \"data\": {\n",
    "                \"labels\": [str(i+1) for i in range(len(history.history['loss']))],\n",
    "                \"datasets\": [\n",
    "                    {\n",
    "                        \"label\": \"Train Loss\",\n",
    "                        \"data\": history.history['loss'],\n",
    "                        \"borderColor\": \"rgba(31, 119, 180, 1)\",\n",
    "                        \"backgroundColor\": \"rgba(31, 119, 180, 0.2)\",\n",
    "                        \"fill\": False,\n",
    "                        \"tension\": 0.4\n",
    "                    },\n",
    "                    {\n",
    "                        \"label\": \"Test Loss\",\n",
    "                        \"data\": history.history['val_loss'],\n",
    "                        \"borderColor\": \"rgba(255, 127, 14, 1)\",\n",
    "                        \"backgroundColor\": \"rgba(255, 127, 14, 0.2)\",\n",
    "                        \"fill\": False,\n",
    "                        \"borderDash\": [5, 5]\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \"options\": {\n",
    "                \"responsive\": True,\n",
    "                \"plugins\": {\n",
    "                    \"legend\": {\"position\": \"top\"},\n",
    "                    \"title\": {\"display\": True, \"text\": f\"{model_name} Train vs Test Loss\"}\n",
    "                },\n",
    "                \"scales\": {\n",
    "                    \"x\": {\"title\": {\"display\": True, \"text\": \"Epoch\"}},\n",
    "                    \"y\": {\"title\": {\"display\": True, \"text\": \"Loss\"}, \"beginAtZero\": True}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        accuracy_chart = {\n",
    "            \"type\": \"line\",\n",
    "            \"data\": {\n",
    "                \"labels\": [str(i+1) for i in range(len(history.history['accuracy']))],\n",
    "                \"datasets\": [\n",
    "                    {\n",
    "                        \"label\": \"Train Accuracy\",\n",
    "                        \"data\": history.history['accuracy'],\n",
    "                        \"borderColor\": \"rgba(31, 119, 180, 1)\",\n",
    "                        \"backgroundColor\": \"rgba(31, 119, 180, 0.2)\",\n",
    "                        \"fill\": False,\n",
    "                        \"tension\": 0.4\n",
    "                    },\n",
    "                    {\n",
    "                        \"label\": \"Test Accuracy\",\n",
    "                        \"data\": history.history['val_accuracy'],\n",
    "                        \"borderColor\": \"rgba(255, 127, 14, 1)\",\n",
    "                        \"backgroundColor\": \"rgba(255, 127, 14, 0.2)\",\n",
    "                        \"fill\": False,\n",
    "                        \"borderDash\": [5, 5]\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \"options\": {\n",
    "                \"responsive\": True,\n",
    "                \"plugins\": {\n",
    "                    \"legend\": {\"position\": \"top\"},\n",
    "                    \"title\": {\"display\": True, \"text\": f\"{model_name} Train vs Test Accuracy\"}\n",
    "                },\n",
    "                \"scales\": {\n",
    "                    \"x\": {\"title\": {\"display\": True, \"text\": \"Epoch\"}},\n",
    "                    \"y\": {\"title\": {\"display\": True, \"text\": \"Accuracy\"}, \"beginAtZero\": True, \"max\": 1}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        with open(os.path.join(output_dir, f'{model_name}_loss_chart_{counter}.json'), 'w') as f:\n",
    "            json.dump(loss_chart, f, indent=4)\n",
    "        with open(os.path.join(output_dir, f'{model_name}_accuracy_chart_{counter}.json'), 'w') as f:\n",
    "            json.dump(accuracy_chart, f, indent=4)\n",
    "    except KeyError as e:\n",
    "        print(f\"Error in save_chartjs_configs: Missing key {e}. Available keys: {list(history.history.keys())}\")\n",
    "\n",
    "# ==================== MAIN TRAINING LOOP ====================\n",
    "results_summary = []\n",
    "for counter, dataset in enumerate(datasets, 1):\n",
    "    dataset_name = dataset[\"name\"]\n",
    "    train_generator = dataset[\"train_generator\"]\n",
    "    test_generator = dataset[\"test_generator\"]\n",
    "    steps_per_epoch = dataset[\"steps_per_epoch\"]\n",
    "    validation_steps = dataset[\"validation_steps\"]\n",
    "    output_dir = dataset[\"output_dir\"]\n",
    "    class_names = dataset[\"class_names\"]\n",
    "    num_classes = dataset[\"num_classes\"]\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TRAINING ON {dataset_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Build and compile a fresh model for each dataset\n",
    "    model = build_mobdensenet(num_classes=num_classes, input_shape=input_shape)\n",
    "    \n",
    "    # Use Adam optimizer with mixed precision compatibility\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # Train the model with memory monitoring\n",
    "    try:\n",
    "        print(f\"Starting training with batch_size={batch_size}, input_size={input_shape}\")\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            validation_data=test_generator,\n",
    "            validation_steps=validation_steps,\n",
    "            epochs=EPOCHS_INITIAL,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        print(\" Training completed successfully!\")\n",
    "\n",
    "    except tf.errors.ResourceExhaustedError as e:\n",
    "        print(f\" OOM Error during training: {e}\")\n",
    "        print(\"Trying with even smaller batch size...\")\n",
    "        \n",
    "        # Further reduce batch size for this dataset\n",
    "        train_generator.batch_size = 8\n",
    "        test_generator.batch_size = 8\n",
    "        steps_per_epoch = max(1, train_generator.samples // 8)\n",
    "        validation_steps = max(1, test_generator.samples // 8)\n",
    "        \n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            validation_data=test_generator,\n",
    "            validation_steps=validation_steps,\n",
    "            epochs=EPOCHS_INITIAL,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "    # Save training history\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    history_df.to_csv(os.path.join(output_dir, f'{MODEL_NAME}_history_{counter}.csv'), index=False)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_generator.reset()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Collect all predictions\n",
    "    predictions = []\n",
    "    y_true = test_generator.classes\n",
    "    total_samples = test_generator.samples\n",
    "    steps = int(np.ceil(total_samples / batch_size))\n",
    "    \n",
    "    for i in range(steps):\n",
    "        try:\n",
    "            x_batch, _ = next(test_generator)\n",
    "            batch_pred = model.predict(x_batch, verbose=0)\n",
    "            predictions.append(batch_pred)\n",
    "        except StopIteration:\n",
    "            break\n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    predictions = predictions[:total_samples]\n",
    "    y_pred = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    inference_time = (time.time() - start_time) * 1000\n",
    "\n",
    "    if len(y_pred) != len(y_true):\n",
    "        print(f\"Warning: Mismatch in y_pred ({len(y_pred)}) and y_true ({len(y_true)}) for {dataset_name}\")\n",
    "        continue\n",
    "\n",
    "    # Calculate metrics\n",
    "    precision = precision_score(y_true, y_pred, average=None)\n",
    "    recall = recall_score(y_true, y_pred, average=None)\n",
    "    f1 = f1_score(y_true, y_pred, average=None)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, predictions, multi_class='ovr')\n",
    "    except ValueError as e:\n",
    "        auc = \"N/A\"\n",
    "        print(f\"Warning: AUC calculation failed for {dataset_name}: {str(e)}\")\n",
    "\n",
    "    test_loss, test_accuracy = model.evaluate(test_generator, steps=steps, verbose=0)\n",
    "\n",
    "    # Print classification report\n",
    "    print(f\"\\nClassification Report for {dataset_name}:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "    # Save metrics\n",
    "    metrics = {\n",
    "        'precision_per_class': precision.tolist(),\n",
    "        'recall_per_class': recall.tolist(),\n",
    "        'f1_per_class': f1.tolist(),\n",
    "        'auc': auc,\n",
    "        'accuracy': test_accuracy\n",
    "    }\n",
    "    with open(os.path.join(output_dir, f'{MODEL_NAME}_metrics_{counter}.json'), 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "    # Calculate average metrics\n",
    "    precision_avg = np.mean(metrics['precision_per_class'])\n",
    "    recall_avg = np.mean(metrics['recall_per_class'])\n",
    "    f1_avg = np.mean(metrics['f1_per_class'])\n",
    "\n",
    "    # Save results to CSV\n",
    "    results = [{\n",
    "        'Model with input image size': f\"{MODEL_NAME} {input_image_size}\",\n",
    "        'Dataset': dataset_name,\n",
    "        'Accuracy': test_accuracy,\n",
    "        'AUC': auc if auc != \"N/A\" else 0.0,\n",
    "        'Loss': test_loss,\n",
    "        'Precision': precision_avg,\n",
    "        'Recall': recall_avg,\n",
    "        'F1 Score': f1_avg,\n",
    "        'Inference time (in miliseconds)': inference_time\n",
    "    }]\n",
    "    results_summary.append(results[0])\n",
    "    \n",
    "    with open(os.path.join(output_dir, f'{MODEL_NAME}_results_{counter}.csv'), 'w', newline='') as csvfile:\n",
    "        fieldnames = ['Model with input image size', 'Dataset', 'Accuracy', 'AUC', 'Loss', 'Precision', 'Recall', 'F1 Score', 'Inference time (in miliseconds)']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerow(results[0])\n",
    "\n",
    "    # Plot and save training history and confusion matrix\n",
    "    plot_training_history(history, output_dir, MODEL_NAME, counter, class_names, y_true, y_pred)\n",
    "    save_chartjs_configs(history, output_dir, MODEL_NAME, counter)\n",
    "\n",
    "    # Save the model\n",
    "    model_path = os.path.join(output_dir, f'{MODEL_NAME}_{counter}.h5')\n",
    "    model.save(model_path)\n",
    "    print(f\" Model saved successfully to {model_path}\")\n",
    "\n",
    "    # Clear session to free memory\n",
    "    tf.keras.backend.clear_session()\n",
    "    print(f\" Memory cleared for next dataset\")\n",
    "\n",
    "    print(f\"[{dataset_name}] Processing completed.\")\n",
    "\n",
    "# Display summary table\n",
    "print(\"\\nSummary of Results Across Datasets:\")\n",
    "summary_table = [\n",
    "    [r['Dataset'], f\"{r['Accuracy']:.4f}\", r['AUC'] if r['AUC'] != \"N/A\" else \"N/A\", \n",
    "     f\"{r['Loss']:.4f}\", f\"{r['Precision']:.4f}\", f\"{r['Recall']:.4f}\", \n",
    "     f\"{r['F1 Score']:.4f}\", f\"{r['Inference time (in miliseconds)']:.2f}\"]\n",
    "    for r in results_summary\n",
    "]\n",
    "summary_headers = [\"Dataset\", \"Accuracy\", \"AUC\", \"Loss\", \"Precision\", \"Recall\", \"F1 Score\", \"Inference Time (ms)\"]\n",
    "print(tabulate(summary_table, headers=summary_headers, tablefmt=\"grid\"))\n",
    "\n",
    "# Save summary table to CSV\n",
    "summary_output_dir = r\"E:\\CombineData001_002\\SplitData1\\hybrid_MobDenseNet_Output_using_tensorflow\"\n",
    "os.makedirs(summary_output_dir, exist_ok=True)\n",
    "with open(os.path.join(summary_output_dir, 'all_datasets_results_summary.csv'), 'w', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=summary_headers)\n",
    "    writer.writeheader()\n",
    "    for row in results_summary:\n",
    "        writer.writerow({\n",
    "            'Dataset': row['Dataset'],\n",
    "            'Accuracy': f\"{row['Accuracy']:.4f}\",\n",
    "            'AUC': row['AUC'] if row['AUC'] != \"N/A\" else \"N/A\",\n",
    "            'Loss': f\"{row['Loss']:.4f}\",\n",
    "            'Precision': f\"{row['Precision']:.4f}\",\n",
    "            'Recall': f\"{row['Recall']:.4f}\",\n",
    "            'F1 Score': f\"{row['F1 Score']:.4f}\",\n",
    "            'Inference Time (ms)': f\"{row['Inference time (in miliseconds)']:.2f}\"\n",
    "        })\n",
    "\n",
    "print(\" All datasets processed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4ca593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5515fc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating detection box visualizations...\n",
      "Using layer for Grad-CAM: cbam_mobilenet\n",
      "Error processing image 0: name 'get_gradcam_heatmap' is not defined\n",
      "Error processing image 1: name 'get_gradcam_heatmap' is not defined\n",
      "Detection box visualizations saved to: E:\\CombineData001_002\\SplitData1\\hybrid_MobDenseNetwithCBAMattention_Output_using_tensorflow\\balanceoutput\\detection_boxes\n",
      "Running detection method comparison...\n",
      "Error in comparison analysis for sample 0: name 'get_gradcam_heatmap' is not defined\n",
      "Error in comparison analysis for sample 1: name 'get_gradcam_heatmap' is not defined\n",
      "Comparison analysis saved to: E:\\CombineData001_002\\SplitData1\\hybrid_MobDenseNetwithCBAMattention_Output_using_tensorflow\\balanceoutput\\detection_comparison\n"
     ]
    }
   ],
   "source": [
    "# ==================== DETECTION BOX FUNCTIONS ====================\n",
    "def create_gradcam_bounding_box(img_path, heatmap, output_path, threshold=0.5):\n",
    "    \"\"\"Create bounding boxes based on Grad-CAM heatmap activation\"\"\"\n",
    "    # Load image\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, input_shape[:2])\n",
    "    \n",
    "    # Resize heatmap\n",
    "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    \n",
    "    # Threshold heatmap to create binary mask\n",
    "    _, thresh = cv2.threshold(heatmap, threshold, 1.0, cv2.THRESH_BINARY)\n",
    "    thresh = (thresh * 255).astype(np.uint8)\n",
    "    \n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Draw bounding boxes\n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) > 10:  # Filter small contours\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Green for Grad-CAM\n",
    "    \n",
    "    # Save image with bounding boxes\n",
    "    cv2.imwrite(output_path, img)\n",
    "    return img\n",
    "\n",
    "def create_traditional_bounding_box(img_path, output_path):\n",
    "    \"\"\"Create bounding boxes using traditional image processing methods\"\"\"\n",
    "    # Load image\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, input_shape[:2])\n",
    "    img_copy = img.copy()\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply multiple detection methods\n",
    "    \n",
    "    # Method 1: Edge detection\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "    \n",
    "    # Method 2: Thresholding\n",
    "    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Method 3: Morphological operations to clean up\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    morph = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    # Combine methods\n",
    "    combined = cv2.bitwise_or(edges, morph)\n",
    "    \n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(combined, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Draw bounding boxes\n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) > 50:  # Filter small contours\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            cv2.rectangle(img_copy, (x, y), (x + w, y + h), (255, 0, 0), 2)  # Blue for traditional\n",
    "    \n",
    "    # Save image with bounding boxes\n",
    "    cv2.imwrite(output_path, img_copy)\n",
    "    return img_copy\n",
    "\n",
    "def create_combined_bounding_boxes(img_path, heatmap, output_path, gradcam_threshold=0.5):\n",
    "    \"\"\"Create image with both Grad-CAM and traditional bounding boxes\"\"\"\n",
    "    # Load image\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, input_shape[:2])\n",
    "    img_copy = img.copy()\n",
    "    \n",
    "    # Grad-CAM bounding boxes (Green)\n",
    "    heatmap_resized = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    _, thresh = cv2.threshold(heatmap_resized, gradcam_threshold, 1.0, cv2.THRESH_BINARY)\n",
    "    thresh = (thresh * 255).astype(np.uint8)\n",
    "    contours_gradcam, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    for contour in contours_gradcam:\n",
    "        if cv2.contourArea(contour) > 10:\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            cv2.rectangle(img_copy, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Green\n",
    "    \n",
    "    # Traditional bounding boxes (Blue)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "    _, thresh_trad = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    morph = cv2.morphologyEx(thresh_trad, cv2.MORPH_CLOSE, kernel)\n",
    "    combined = cv2.bitwise_or(edges, morph)\n",
    "    contours_trad, _ = cv2.findContours(combined, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    for contour in contours_trad:\n",
    "        if cv2.contourArea(contour) > 50:\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            cv2.rectangle(img_copy, (x, y), (x + w, y + h), (255, 0, 0), 2)  # Blue\n",
    "    \n",
    "    # Add legend\n",
    "    cv2.putText(img_copy, \"Grad-CAM Boxes (Green)\", (10, 30), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "    cv2.putText(img_copy, \"Traditional Boxes (Blue)\", (10, 60), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
    "    \n",
    "    cv2.imwrite(output_path, img_copy)\n",
    "    return img_copy\n",
    "\n",
    "# ==================== MODIFIED VISUALIZATION FUNCTION ====================\n",
    "def visualize_detection_boxes(model, test_generator, output_dir, dataset_name, counter, num_samples=5):\n",
    "    \"\"\"Visualize both Grad-CAM and traditional detection boxes\"\"\"\n",
    "    detection_dir = os.path.join(output_dir, 'detection_boxes')\n",
    "    os.makedirs(detection_dir, exist_ok=True)\n",
    "    \n",
    "    # Get sample images\n",
    "    test_generator.reset()\n",
    "    sample_images = []\n",
    "    sample_paths = []\n",
    "    \n",
    "    for i in range(min(num_samples, test_generator.samples)):\n",
    "        x_batch, y_batch = next(test_generator)\n",
    "        sample_images.append(x_batch[0])\n",
    "        sample_paths.append(test_generator.filepaths[i])\n",
    "    \n",
    "    # Find suitable convolutional layer for Grad-CAM\n",
    "    last_conv_layers = ['conv_pw_13_relu', 'conv5_block16_concat', 'cbam_mobilenet', 'cbam_densenet']\n",
    "    layer_name = None\n",
    "    \n",
    "    for layer in last_conv_layers:\n",
    "        try:\n",
    "            model.get_layer(layer)\n",
    "            layer_name = layer\n",
    "            print(f\"Using layer for Grad-CAM: {layer_name}\")\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if layer_name is None:\n",
    "        print(\"Could not find suitable convolutional layer for Grad-CAM\")\n",
    "        return\n",
    "    \n",
    "    for i, (img_array, img_path) in enumerate(zip(sample_images, sample_paths)):\n",
    "        try:\n",
    "            # Preprocess image\n",
    "            img_for_gradcam = np.expand_dims(img_array, axis=0)\n",
    "            \n",
    "            # Get prediction\n",
    "            preds = model.predict(img_for_gradcam, verbose=0)\n",
    "            pred_class = np.argmax(preds[0])\n",
    "            pred_prob = np.max(preds[0])\n",
    "            true_class = test_generator.classes[i]\n",
    "            \n",
    "            # Generate heatmap for Grad-CAM\n",
    "            heatmap, _ = get_gradcam_heatmap(img_for_gradcam, model, layer_name, pred_class)\n",
    "            \n",
    "            # Create base filename\n",
    "            base_filename = f\"{dataset_name.replace(' ', '_')}_sample_{i}_true_{true_class}_pred_{pred_class}\"\n",
    "            \n",
    "            # 1. Original image\n",
    "            original_img = cv2.imread(img_path)\n",
    "            original_img = cv2.resize(original_img, input_shape[:2])\n",
    "            cv2.imwrite(os.path.join(detection_dir, f\"{base_filename}_original.png\"), original_img)\n",
    "            \n",
    "            # 2. Grad-CAM detection boxes (Green)\n",
    "            gradcam_bbox_path = os.path.join(detection_dir, f\"{base_filename}_gradcam_boxes.png\")\n",
    "            create_gradcam_bounding_box(img_path, heatmap, gradcam_bbox_path)\n",
    "            \n",
    "            # 3. Traditional detection boxes (Blue)\n",
    "            traditional_bbox_path = os.path.join(detection_dir, f\"{base_filename}_traditional_boxes.png\")\n",
    "            create_traditional_bounding_box(img_path, traditional_bbox_path)\n",
    "            \n",
    "            # 4. Combined boxes (Green + Blue)\n",
    "            combined_bbox_path = os.path.join(detection_dir, f\"{base_filename}_combined_boxes.png\")\n",
    "            create_combined_bounding_boxes(img_path, heatmap, combined_bbox_path)\n",
    "            \n",
    "            # 5. Grad-CAM heatmap only\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.imshow(heatmap, cmap='jet')\n",
    "            plt.title(f'Grad-CAM Heatmap\\nTrue: {true_class}, Pred: {pred_class}, Prob: {pred_prob:.2f}')\n",
    "            plt.colorbar()\n",
    "            plt.savefig(os.path.join(detection_dir, f\"{base_filename}_heatmap.png\"), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"Generated detection boxes for sample {i}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Detection box visualizations saved to: {detection_dir}\")\n",
    "\n",
    "# ==================== COMPARISON ANALYSIS FUNCTION ====================\n",
    "def analyze_detection_comparison(model, test_generator, output_dir, dataset_name, counter, num_samples=10):\n",
    "    \"\"\"Analyze and compare Grad-CAM vs traditional detection methods\"\"\"\n",
    "    comparison_dir = os.path.join(output_dir, 'detection_comparison')\n",
    "    os.makedirs(comparison_dir, exist_ok=True)\n",
    "    \n",
    "    results = []\n",
    "    test_generator.reset()\n",
    "    \n",
    "    # Find suitable convolutional layer for Grad-CAM\n",
    "    last_conv_layers = ['conv_pw_13_relu', 'conv5_block16_concat', 'cbam_mobilenet', 'cbam_densenet']\n",
    "    layer_name = None\n",
    "    \n",
    "    for layer in last_conv_layers:\n",
    "        try:\n",
    "            model.get_layer(layer)\n",
    "            layer_name = layer\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if layer_name is None:\n",
    "        print(\"Could not find suitable convolutional layer for comparison analysis\")\n",
    "        return\n",
    "    \n",
    "    for i in range(min(num_samples, test_generator.samples)):\n",
    "        try:\n",
    "            x_batch, y_batch = next(test_generator)\n",
    "            img_array = x_batch[0]\n",
    "            img_path = test_generator.filepaths[i]\n",
    "            \n",
    "            # Preprocess for Grad-CAM\n",
    "            img_for_gradcam = np.expand_dims(img_array, axis=0)\n",
    "            heatmap, _ = get_gradcam_heatmap(img_for_gradcam, model, layer_name)\n",
    "            \n",
    "            # Count Grad-CAM boxes\n",
    "            gradcam_boxes = count_bounding_boxes_gradcam(heatmap)\n",
    "            \n",
    "            # Count traditional boxes\n",
    "            traditional_boxes = count_bounding_boxes_traditional(img_path)\n",
    "            \n",
    "            results.append({\n",
    "                'sample_id': i,\n",
    "                'gradcam_boxes': gradcam_boxes,\n",
    "                'traditional_boxes': traditional_boxes,\n",
    "                'difference': abs(gradcam_boxes - traditional_boxes)\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in comparison analysis for sample {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Save comparison results\n",
    "    comparison_df = pd.DataFrame(results)\n",
    "    comparison_df.to_csv(os.path.join(comparison_dir, f'detection_comparison_{counter}.csv'), index=False)\n",
    "    \n",
    "    # Create comparison plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    x = range(len(results))\n",
    "    plt.bar(x, [r['gradcam_boxes'] for r in results], alpha=0.7, label='Grad-CAM Boxes', color='green')\n",
    "    plt.bar(x, [r['traditional_boxes'] for r in results], alpha=0.7, label='Traditional Boxes', color='blue')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Number of Detection Boxes')\n",
    "    plt.title('Grad-CAM vs Traditional Detection Box Comparison')\n",
    "    plt.legend()\n",
    "    plt.xticks(x)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(comparison_dir, f'comparison_plot_{counter}.png'), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Comparison analysis saved to: {comparison_dir}\")\n",
    "\n",
    "def count_bounding_boxes_gradcam(heatmap, threshold=0.5):\n",
    "    \"\"\"Count number of bounding boxes from Grad-CAM heatmap\"\"\"\n",
    "    heatmap_resized = cv2.resize(heatmap, input_shape[:2])\n",
    "    _, thresh = cv2.threshold(heatmap_resized, threshold, 1.0, cv2.THRESH_BINARY)\n",
    "    thresh = (thresh * 255).astype(np.uint8)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    return len([c for c in contours if cv2.contourArea(c) > 10])\n",
    "\n",
    "def count_bounding_boxes_traditional(img_path):\n",
    "    \"\"\"Count number of bounding boxes from traditional method\"\"\"\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, input_shape[:2])\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    morph = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
    "    combined = cv2.bitwise_or(edges, morph)\n",
    "    \n",
    "    contours, _ = cv2.findContours(combined, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    return len([c for c in contours if cv2.contourArea(c) > 50])\n",
    "\n",
    "# ==================== MODIFY MAIN TRAINING LOOP ====================\n",
    "# In your main training loop, after model evaluation, add:\n",
    "print(\"Generating detection box visualizations...\")\n",
    "visualize_detection_boxes(model, test_generator, output_dir, dataset_name, counter, num_samples=2)\n",
    "\n",
    "print(\"Running detection method comparison...\")\n",
    "analyze_detection_comparison(model, test_generator, output_dir, dataset_name, counter, num_samples=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fadcb4",
   "metadata": {},
   "source": [
    "Xception Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeb5304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')], Num GPUs: 1\n",
      "TensorFlow version: 2.10.1\n",
      "\n",
      "Processing Imbalanced Dataset 1...\n",
      "Found 2757 images belonging to 3 classes.\n",
      "Found 307 images belonging to 3 classes.\n",
      "[Imbalanced Dataset 1] Detected 3 classes: ['Glioma', 'Meningioma', 'Pituitary']\n",
      "[Imbalanced Dataset 1] Input shape for Xception: (64, 224, 224, 3), Label shape: (64, 3)\n",
      "[Imbalanced Dataset 1] Class weights: {'Glioma': 0.7162899454403742, 'Meningioma': 1.4427001569858713, 'Pituitary': 1.097968936678614}\n",
      "[Imbalanced Dataset 1] Test dataset size: 307\n",
      "[Imbalanced Dataset 1] Test labels unique: [0 1 2]\n",
      "[Imbalanced Dataset 1] Training Xception with lr=0.002, dr=0.2, counter=1\n",
      "Model compiled with optimizer: <keras.optimizers.optimizer_v2.nadam.Nadam object at 0x000002B34B4B4A90>\n",
      "Epoch 1/100\n",
      "44/44 [==============================] - 108s 2s/step - loss: 0.4790 - accuracy: 0.8143 - val_loss: 1.1033 - val_accuracy: 0.7752 - lr: 0.0020\n",
      "Epoch 2/100\n",
      "44/44 [==============================] - 84s 2s/step - loss: 0.2938 - accuracy: 0.9010 - val_loss: 0.2600 - val_accuracy: 0.9479 - lr: 0.0020\n",
      "Epoch 3/100\n",
      "44/44 [==============================] - 68s 2s/step - loss: 0.2420 - accuracy: 0.9126 - val_loss: 1.2453 - val_accuracy: 0.8893 - lr: 0.0020\n",
      "Epoch 4/100\n",
      "44/44 [==============================] - 73s 2s/step - loss: 0.1988 - accuracy: 0.9296 - val_loss: 0.4546 - val_accuracy: 0.9381 - lr: 0.0020\n",
      "Epoch 5/100\n",
      "44/44 [==============================] - 73s 2s/step - loss: 0.2266 - accuracy: 0.9220 - val_loss: 1.0186 - val_accuracy: 0.8371 - lr: 0.0020\n",
      "Epoch 6/100\n",
      "44/44 [==============================] - 72s 2s/step - loss: 0.1539 - accuracy: 0.9478 - val_loss: 0.1294 - val_accuracy: 0.9707 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "44/44 [==============================] - 70s 2s/step - loss: 0.1879 - accuracy: 0.9325 - val_loss: 0.1285 - val_accuracy: 0.9674 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "44/44 [==============================] - 74s 2s/step - loss: 0.1397 - accuracy: 0.9528 - val_loss: 0.0959 - val_accuracy: 0.9772 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "44/44 [==============================] - 73s 2s/step - loss: 0.1139 - accuracy: 0.9630 - val_loss: 0.1045 - val_accuracy: 0.9642 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "44/44 [==============================] - 70s 2s/step - loss: 0.0877 - accuracy: 0.9703 - val_loss: 0.1126 - val_accuracy: 0.9772 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "44/44 [==============================] - 71s 2s/step - loss: 0.0961 - accuracy: 0.9670 - val_loss: 0.1018 - val_accuracy: 0.9870 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "44/44 [==============================] - 74s 2s/step - loss: 0.1087 - accuracy: 0.9652 - val_loss: 0.1004 - val_accuracy: 0.9772 - lr: 5.0000e-04\n",
      "Epoch 13/100\n",
      "44/44 [==============================] - 71s 2s/step - loss: 0.1058 - accuracy: 0.9637 - val_loss: 0.1611 - val_accuracy: 0.9251 - lr: 5.0000e-04\n",
      "Epoch 14/100\n",
      "44/44 [==============================] - 66s 1s/step - loss: 0.0861 - accuracy: 0.9695 - val_loss: 0.1124 - val_accuracy: 0.9609 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "44/44 [==============================] - 68s 2s/step - loss: 0.0627 - accuracy: 0.9779 - val_loss: 0.0434 - val_accuracy: 0.9870 - lr: 2.5000e-04\n",
      "Epoch 16/100\n",
      "44/44 [==============================] - 72s 2s/step - loss: 0.0747 - accuracy: 0.9750 - val_loss: 0.0379 - val_accuracy: 0.9837 - lr: 2.5000e-04\n",
      "Epoch 17/100\n",
      "44/44 [==============================] - 67s 2s/step - loss: 0.0591 - accuracy: 0.9811 - val_loss: 0.0449 - val_accuracy: 0.9902 - lr: 2.5000e-04\n",
      "Epoch 18/100\n",
      "44/44 [==============================] - 63s 1s/step - loss: 0.0751 - accuracy: 0.9775 - val_loss: 0.0348 - val_accuracy: 0.9902 - lr: 2.5000e-04\n",
      "Epoch 19/100\n",
      "44/44 [==============================] - 62s 1s/step - loss: 0.0539 - accuracy: 0.9837 - val_loss: 0.0383 - val_accuracy: 0.9870 - lr: 2.5000e-04\n",
      "Epoch 20/100\n",
      "44/44 [==============================] - 62s 1s/step - loss: 0.0547 - accuracy: 0.9819 - val_loss: 0.0470 - val_accuracy: 0.9870 - lr: 2.5000e-04\n",
      "Epoch 21/100\n",
      "44/44 [==============================] - 62s 1s/step - loss: 0.0441 - accuracy: 0.9855 - val_loss: 0.0384 - val_accuracy: 0.9870 - lr: 2.5000e-04\n",
      "Epoch 22/100\n",
      "44/44 [==============================] - 62s 1s/step - loss: 0.0532 - accuracy: 0.9815 - val_loss: 0.0421 - val_accuracy: 0.9870 - lr: 1.2500e-04\n",
      "Epoch 23/100\n",
      "44/44 [==============================] - 54s 1s/step - loss: 0.0414 - accuracy: 0.9859 - val_loss: 0.0467 - val_accuracy: 0.9870 - lr: 1.2500e-04\n",
      "Epoch 24/100\n",
      "44/44 [==============================] - 64s 1s/step - loss: 0.0515 - accuracy: 0.9815 - val_loss: 0.0553 - val_accuracy: 0.9870 - lr: 1.2500e-04\n",
      "Epoch 25/100\n",
      "44/44 [==============================] - 62s 1s/step - loss: 0.0668 - accuracy: 0.9768 - val_loss: 0.0480 - val_accuracy: 0.9837 - lr: 6.2500e-05\n",
      "Epoch 26/100\n",
      "44/44 [==============================] - 62s 1s/step - loss: 0.0362 - accuracy: 0.9859 - val_loss: 0.0471 - val_accuracy: 0.9837 - lr: 6.2500e-05\n",
      "Epoch 27/100\n",
      "44/44 [==============================] - 62s 1s/step - loss: 0.0535 - accuracy: 0.9844 - val_loss: 0.0456 - val_accuracy: 0.9837 - lr: 6.2500e-05\n",
      "Epoch 28/100\n",
      "44/44 [==============================] - 60s 1s/step - loss: 0.0388 - accuracy: 0.9851 - val_loss: 0.0464 - val_accuracy: 0.9837 - lr: 3.1250e-05\n",
      "Epoch 1: Collecting training predictions\n",
      "Computing train metrics: y_true shape=(2757,), y_pred shape=(2757,)\n",
      "y_true sample: [0, 0, 2, 0, 0], y_pred sample: [0, 0, 2, 0, 0]\n",
      "train metrics - Precision: 0.9895, Recall: 0.9895, F1: 0.9895\n",
      "Epoch 1: Collecting test predictions\n",
      "Computing test metrics: y_true shape=(307,), y_pred shape=(307,)\n",
      "y_true sample: [0, 0, 0, 0, 0], y_pred sample: [0, 0, 0, 0, 0]\n",
      "test metrics - Precision: 0.9903, Recall: 0.9902, F1: 0.9902\n",
      "Epoch 2: Collecting training predictions\n",
      "Computing train metrics: y_true shape=(2757,), y_pred shape=(2757,)\n",
      "y_true sample: [1, 1, 1, 0, 0], y_pred sample: [1, 1, 1, 0, 0]\n",
      "train metrics - Precision: 0.9832, Recall: 0.9830, F1: 0.9830\n",
      "Epoch 2: Collecting test predictions\n",
      "Computing test metrics: y_true shape=(307,), y_pred shape=(307,)\n",
      "y_true sample: [0, 0, 0, 0, 0], y_pred sample: [0, 0, 0, 0, 0]\n",
      "test metrics - Precision: 0.9903, Recall: 0.9902, F1: 0.9902\n",
      "Epoch 3: Collecting training predictions\n",
      "Computing train metrics: y_true shape=(2757,), y_pred shape=(2757,)\n",
      "y_true sample: [2, 0, 2, 0, 0], y_pred sample: [2, 0, 2, 0, 0]\n",
      "train metrics - Precision: 0.9874, Recall: 0.9873, F1: 0.9873\n",
      "Epoch 3: Collecting test predictions\n",
      "Computing test metrics: y_true shape=(307,), y_pred shape=(307,)\n",
      "y_true sample: [0, 0, 0, 0, 0], y_pred sample: [0, 0, 0, 0, 0]\n",
      "test metrics - Precision: 0.9903, Recall: 0.9902, F1: 0.9902\n",
      "Epoch 4: Collecting training predictions\n",
      "Computing train metrics: y_true shape=(2757,), y_pred shape=(2757,)\n",
      "y_true sample: [1, 1, 1, 2, 0], y_pred sample: [1, 1, 1, 2, 0]\n",
      "train metrics - Precision: 0.9819, Recall: 0.9815, F1: 0.9816\n",
      "Epoch 4: Collecting test predictions\n",
      "Computing test metrics: y_true shape=(307,), y_pred shape=(307,)\n",
      "y_true sample: [0, 0, 0, 0, 0], y_pred sample: [0, 0, 0, 0, 0]\n",
      "test metrics - Precision: 0.9903, Recall: 0.9902, F1: 0.9902\n",
      "Epoch 5: Collecting training predictions\n",
      "Computing train metrics: y_true shape=(2757,), y_pred shape=(2757,)\n",
      "y_true sample: [1, 1, 2, 2, 2], y_pred sample: [1, 1, 2, 2, 2]\n",
      "train metrics - Precision: 0.9863, Recall: 0.9862, F1: 0.9862\n",
      "Epoch 5: Collecting test predictions\n",
      "Computing test metrics: y_true shape=(307,), y_pred shape=(307,)\n",
      "y_true sample: [0, 0, 0, 0, 0], y_pred sample: [0, 0, 0, 0, 0]\n",
      "test metrics - Precision: 0.9903, Recall: 0.9902, F1: 0.9902\n",
      "Epoch 6: Collecting training predictions\n",
      "Computing train metrics: y_true shape=(2757,), y_pred shape=(2757,)\n",
      "y_true sample: [2, 2, 2, 0, 2], y_pred sample: [2, 2, 2, 0, 2]\n",
      "train metrics - Precision: 0.9842, Recall: 0.9840, F1: 0.9841\n",
      "Epoch 6: Collecting test predictions\n",
      "Computing test metrics: y_true shape=(307,), y_pred shape=(307,)\n",
      "y_true sample: [0, 0, 0, 0, 0], y_pred sample: [0, 0, 0, 0, 0]\n",
      "test metrics - Precision: 0.9903, Recall: 0.9902, F1: 0.9902\n",
      "Epoch 7: Collecting training predictions\n",
      "Computing train metrics: y_true shape=(2757,), y_pred shape=(2757,)\n",
      "y_true sample: [1, 0, 1, 2, 0], y_pred sample: [1, 0, 1, 2, 0]\n",
      "train metrics - Precision: 0.9842, Recall: 0.9840, F1: 0.9841\n",
      "Epoch 7: Collecting test predictions\n",
      "Computing test metrics: y_true shape=(307,), y_pred shape=(307,)\n",
      "y_true sample: [0, 0, 0, 0, 0], y_pred sample: [0, 0, 0, 0, 0]\n",
      "test metrics - Precision: 0.9903, Recall: 0.9902, F1: 0.9902\n",
      "Epoch 8: Collecting training predictions\n",
      "Computing train metrics: y_true shape=(2757,), y_pred shape=(2757,)\n",
      "y_true sample: [0, 1, 2, 0, 0], y_pred sample: [0, 1, 2, 0, 0]\n",
      "train metrics - Precision: 0.9833, Recall: 0.9830, F1: 0.9830\n",
      "Epoch 8: Collecting test predictions\n",
      "Computing test metrics: y_true shape=(307,), y_pred shape=(307,)\n",
      "y_true sample: [0, 0, 0, 0, 0], y_pred sample: [0, 0, 0, 0, 0]\n",
      "test metrics - Precision: 0.9903, Recall: 0.9902, F1: 0.9902\n",
      "Epoch 9: Collecting training predictions\n",
      "Computing train metrics: y_true shape=(2757,), y_pred shape=(2757,)\n",
      "y_true sample: [0, 2, 1, 2, 0], y_pred sample: [0, 2, 1, 2, 0]\n",
      "train metrics - Precision: 0.9858, Recall: 0.9855, F1: 0.9856\n",
      "Epoch 9: Collecting test predictions\n",
      "Computing test metrics: y_true shape=(307,), y_pred shape=(307,)\n",
      "y_true sample: [0, 0, 0, 0, 0], y_pred sample: [0, 0, 0, 0, 0]\n",
      "test metrics - Precision: 0.9903, Recall: 0.9902, F1: 0.9902\n",
      "Epoch 10: Collecting training predictions\n",
      "Computing train metrics: y_true shape=(2757,), y_pred shape=(2757,)\n",
      "y_true sample: [0, 2, 0, 2, 2], y_pred sample: [0, 2, 0, 2, 2]\n",
      "train metrics - Precision: 0.9838, Recall: 0.9837, F1: 0.9837\n",
      "Epoch 10: Collecting test predictions\n",
      "Computing test metrics: y_true shape=(307,), y_pred shape=(307,)\n",
      "y_true sample: [0, 0, 0, 0, 0], y_pred sample: [0, 0, 0, 0, 0]\n",
      "test metrics - Precision: 0.9903, Recall: 0.9902, F1: 0.9902\n",
      "Epoch 11: Collecting training predictions\n",
      "Computing train metrics: y_true shape=(2757,), y_pred shape=(2757,)\n",
      "y_true sample: [1, 2, 0, 0, 0], y_pred sample: [1, 2, 0, 0, 0]\n",
      "train metrics - Precision: 0.9893, Recall: 0.9891, F1: 0.9892\n",
      "Epoch 11: Collecting test predictions\n",
      "Computing test metrics: y_true shape=(307,), y_pred shape=(307,)\n",
      "y_true sample: [0, 0, 0, 0, 0], y_pred sample: [0, 0, 0, 0, 0]\n",
      "test metrics - Precision: 0.9903, Recall: 0.9902, F1: 0.9902\n",
      "Epoch 12: Collecting training predictions\n",
      "Computing train metrics: y_true shape=(2757,), y_pred shape=(2757,)\n",
      "y_true sample: [1, 1, 2, 0, 1], y_pred sample: [1, 1, 2, 0, 1]\n",
      "train metrics - Precision: 0.9871, Recall: 0.9869, F1: 0.9870\n",
      "Epoch 12: Collecting test predictions\n",
      "Computing test metrics: y_true shape=(307,), y_pred shape=(307,)\n",
      "y_true sample: [0, 0, 0, 0, 0], y_pred sample: [0, 0, 0, 0, 0]\n",
      "test metrics - Precision: 0.9903, Recall: 0.9902, F1: 0.9902\n",
      "Epoch 13: Collecting training predictions\n",
      "Computing train metrics: y_true shape=(2757,), y_pred shape=(2757,)\n",
      "y_true sample: [0, 1, 0, 1, 0], y_pred sample: [0, 1, 0, 1, 0]\n",
      "train metrics - Precision: 0.9850, Recall: 0.9848, F1: 0.9848\n",
      "Epoch 13: Collecting test predictions\n",
      "Computing test metrics: y_true shape=(307,), y_pred shape=(307,)\n",
      "y_true sample: [0, 0, 0, 0, 0], y_pred sample: [0, 0, 0, 0, 0]\n",
      "test metrics - Precision: 0.9903, Recall: 0.9902, F1: 0.9902\n",
      "Epoch 14: Collecting training predictions\n",
      "Computing train metrics: y_true shape=(2757,), y_pred shape=(2757,)\n",
      "y_true sample: [2, 0, 2, 2, 0], y_pred sample: [2, 0, 2, 2, 0]\n",
      "train metrics - Precision: 0.9853, Recall: 0.9851, F1: 0.9852\n",
      "Epoch 14: Collecting test predictions\n",
      "Computing test metrics: y_true shape=(307,), y_pred shape=(307,)\n",
      "y_true sample: [0, 0, 0, 0, 0], y_pred sample: [0, 0, 0, 0, 0]\n",
      "test metrics - Precision: 0.9903, Recall: 0.9902, F1: 0.9902\n",
      "Epoch 15: Collecting training predictions\n",
      "Computing train metrics: y_true shape=(2757,), y_pred shape=(2757,)\n",
      "y_true sample: [0, 0, 0, 2, 0], y_pred sample: [0, 0, 0, 2, 0]\n",
      "train metrics - Precision: 0.9803, Recall: 0.9801, F1: 0.9801\n",
      "Epoch 15: Collecting test predictions\n",
      "Computing test metrics: y_true shape=(307,), y_pred shape=(307,)\n",
      "y_true sample: [0, 0, 0, 0, 0], y_pred sample: [0, 0, 0, 0, 0]\n",
      "test metrics - Precision: 0.9903, Recall: 0.9902, F1: 0.9902\n",
      "Epoch 16: Collecting training predictions\n",
      "Computing train metrics: y_true shape=(2757,), y_pred shape=(2757,)\n",
      "y_true sample: [0, 2, 0, 0, 0], y_pred sample: [0, 2, 0, 0, 0]\n",
      "train metrics - Precision: 0.9857, Recall: 0.9855, F1: 0.9855\n",
      "Epoch 16: Collecting test predictions\n",
      "Computing test metrics: y_true shape=(307,), y_pred shape=(307,)\n",
      "y_true sample: [0, 0, 0, 0, 0], y_pred sample: [0, 0, 0, 0, 0]\n",
      "test metrics - Precision: 0.9903, Recall: 0.9902, F1: 0.9902\n",
      "Epoch 17: Collecting training predictions\n",
      "Computing train metrics: y_true shape=(2757,), y_pred shape=(2757,)\n",
      "y_true sample: [0, 1, 0, 0, 0], y_pred sample: [0, 1, 0, 0, 0]\n",
      "train metrics - Precision: 0.9849, Recall: 0.9848, F1: 0.9848\n",
      "Epoch 17: Collecting test predictions\n",
      "Computing test metrics: y_true shape=(307,), y_pred shape=(307,)\n",
      "y_true sample: [0, 0, 0, 0, 0], y_pred sample: [0, 0, 0, 0, 0]\n",
      "test metrics - Precision: 0.9903, Recall: 0.9902, F1: 0.9902\n",
      "Epoch 18: Collecting training predictions\n",
      "Computing train metrics: y_true shape=(2757,), y_pred shape=(2757,)\n",
      "y_true sample: [1, 2, 0, 1, 0], y_pred sample: [1, 2, 0, 1, 0]\n",
      "train metrics - Precision: 0.9885, Recall: 0.9884, F1: 0.9884\n",
      "Epoch 18: Collecting test predictions\n",
      "Computing test metrics: y_true shape=(307,), y_pred shape=(307,)\n",
      "y_true sample: [0, 0, 0, 0, 0], y_pred sample: [0, 0, 0, 0, 0]\n",
      "test metrics - Precision: 0.9903, Recall: 0.9902, F1: 0.9902\n",
      "Epoch 19: Collecting training predictions\n",
      "Computing train metrics: y_true shape=(2757,), y_pred shape=(2757,)\n",
      "y_true sample: [0, 0, 1, 1, 2], y_pred sample: [0, 0, 1, 1, 2]\n",
      "train metrics - Precision: 0.9807, Recall: 0.9804, F1: 0.9805\n",
      "Epoch 19: Collecting test predictions\n",
      "Computing test metrics: y_true shape=(307,), y_pred shape=(307,)\n",
      "y_true sample: [0, 0, 0, 0, 0], y_pred sample: [0, 0, 0, 0, 0]\n",
      "test metrics - Precision: 0.9903, Recall: 0.9902, F1: 0.9902\n",
      "Epoch 20: Collecting training predictions\n",
      "Computing train metrics: y_true shape=(2757,), y_pred shape=(2757,)\n",
      "y_true sample: [2, 2, 0, 0, 2], y_pred sample: [2, 2, 0, 0, 2]\n",
      "train metrics - Precision: 0.9885, Recall: 0.9884, F1: 0.9884\n",
      "Epoch 20: Collecting test predictions\n",
      "Computing test metrics: y_true shape=(307,), y_pred shape=(307,)\n",
      "y_true sample: [0, 0, 0, 0, 0], y_pred sample: [0, 0, 0, 0, 0]\n",
      "test metrics - Precision: 0.9903, Recall: 0.9902, F1: 0.9902\n",
      "Epoch 21: Collecting training predictions\n",
      "Computing train metrics: y_true shape=(2757,), y_pred shape=(2757,)\n",
      "y_true sample: [0, 0, 2, 1, 2], y_pred sample: [0, 0, 2, 1, 2]\n",
      "train metrics - Precision: 0.9867, Recall: 0.9866, F1: 0.9866\n",
      "Epoch 21: Collecting test predictions\n",
      "Computing test metrics: y_true shape=(307,), y_pred shape=(307,)\n",
      "y_true sample: [0, 0, 0, 0, 0], y_pred sample: [0, 0, 0, 0, 0]\n",
      "test metrics - Precision: 0.9903, Recall: 0.9902, F1: 0.9902\n",
      "Epoch 22: Collecting training predictions\n",
      "Computing train metrics: y_true shape=(2757,), y_pred shape=(2757,)\n",
      "y_true sample: [0, 0, 2, 1, 0], y_pred sample: [0, 0, 2, 1, 0]\n",
      "train metrics - Precision: 0.9856, Recall: 0.9855, F1: 0.9855\n",
      "Epoch 22: Collecting test predictions\n",
      "Computing test metrics: y_true shape=(307,), y_pred shape=(307,)\n",
      "y_true sample: [0, 0, 0, 0, 0], y_pred sample: [0, 0, 0, 0, 0]\n",
      "test metrics - Precision: 0.9903, Recall: 0.9902, F1: 0.9902\n",
      "Epoch 23: Collecting training predictions\n",
      "Error in train_model: Graph execution error:\n",
      "\n",
      "Detected at node 'model/block2_sepconv1/separable_conv2d' defined at (most recent call last):\n",
      "    File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "      return _run_code(code, main_globals, None,\n",
      "    File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n",
      "      exec(code, run_globals)\n",
      "    File \"e:\\.env\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "      app.launch_new_instance()\n",
      "    File \"e:\\.env\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "      app.start()\n",
      "    File \"e:\\.env\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "      self.io_loop.start()\n",
      "    File \"e:\\.env\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "      self.asyncio_loop.run_forever()\n",
      "    File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 595, in run_forever\n",
      "      self._run_once()\n",
      "    File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 1881, in _run_once\n",
      "      handle._run()\n",
      "    File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "      self._context.run(self._callback, *self._args)\n",
      "    File \"e:\\.env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 519, in dispatch_queue\n",
      "      await self.process_one()\n",
      "    File \"e:\\.env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 508, in process_one\n",
      "      await dispatch(*args)\n",
      "    File \"e:\\.env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 400, in dispatch_shell\n",
      "      await result\n",
      "    File \"e:\\.env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 368, in execute_request\n",
      "      await super().execute_request(stream, ident, parent)\n",
      "    File \"e:\\.env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "      reply_content = await reply_content\n",
      "    File \"e:\\.env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 455, in do_execute\n",
      "      res = shell.run_cell(\n",
      "    File \"e:\\.env\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 577, in run_cell\n",
      "      return super().run_cell(*args, **kwargs)\n",
      "    File \"e:\\.env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3077, in run_cell\n",
      "      result = self._run_cell(\n",
      "    File \"e:\\.env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3132, in _run_cell\n",
      "      result = runner(coro)\n",
      "    File \"e:\\.env\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "      coro.send(None)\n",
      "    File \"e:\\.env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3336, in run_cell_async\n",
      "      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "    File \"e:\\.env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "      if await self.run_code(code, result, async_=asy):\n",
      "    File \"e:\\.env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3579, in run_code\n",
      "      exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "    File \"C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_20228\\1202912357.py\", line 1048, in <module>\n",
      "      history, metrics_log = train_model(model, lr, train_generator, test_generator, class_weights, counter, OUTPUT_DIR)\n",
      "    File \"C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_20228\\1202912357.py\", line 225, in train_model\n",
      "      preds = np.argmax(model.predict(batch_x, verbose=0), axis=1)\n",
      "    File \"e:\\.env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 2253, in predict\n",
      "      tmp_batch_outputs = self.predict_function(iterator)\n",
      "    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function\n",
      "      return step_function(self, iterator)\n",
      "    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function\n",
      "      outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step\n",
      "      outputs = model.predict_step(data)\n",
      "    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n",
      "      return self(x, training=False)\n",
      "    File \"e:\\.env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n",
      "      return super().__call__(*args, **kwargs)\n",
      "    File \"e:\\.env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n",
      "      outputs = call_fn(inputs, *args, **kwargs)\n",
      "    File \"e:\\.env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n",
      "      return self._run_internal_graph(inputs, training=training, mask=mask)\n",
      "    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n",
      "      outputs = node.layer(*args, **kwargs)\n",
      "    File \"e:\\.env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n",
      "      outputs = call_fn(inputs, *args, **kwargs)\n",
      "    File \"e:\\.env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"e:\\.env\\lib\\site-packages\\keras\\layers\\convolutional\\separable_conv2d.py\", line 188, in call\n",
      "      outputs = tf.compat.v1.nn.separable_conv2d(\n",
      "Node: 'model/block2_sepconv1/separable_conv2d'\n",
      "OOM when allocating tensor with shape[32,109,109,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node model/block2_sepconv1/separable_conv2d}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n",
      " [Op:__inference_predict_function_480163]\n",
      "An error occurred: Graph execution error:\n",
      "\n",
      "Detected at node 'model/block2_sepconv1/separable_conv2d' defined at (most recent call last):\n",
      "    File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "      return _run_code(code, main_globals, None,\n",
      "    File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n",
      "      exec(code, run_globals)\n",
      "    File \"e:\\.env\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "      app.launch_new_instance()\n",
      "    File \"e:\\.env\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "      app.start()\n",
      "    File \"e:\\.env\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "      self.io_loop.start()\n",
      "    File \"e:\\.env\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "      self.asyncio_loop.run_forever()\n",
      "    File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 595, in run_forever\n",
      "      self._run_once()\n",
      "    File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 1881, in _run_once\n",
      "      handle._run()\n",
      "    File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "      self._context.run(self._callback, *self._args)\n",
      "    File \"e:\\.env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 519, in dispatch_queue\n",
      "      await self.process_one()\n",
      "    File \"e:\\.env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 508, in process_one\n",
      "      await dispatch(*args)\n",
      "    File \"e:\\.env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 400, in dispatch_shell\n",
      "      await result\n",
      "    File \"e:\\.env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 368, in execute_request\n",
      "      await super().execute_request(stream, ident, parent)\n",
      "    File \"e:\\.env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "      reply_content = await reply_content\n",
      "    File \"e:\\.env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 455, in do_execute\n",
      "      res = shell.run_cell(\n",
      "    File \"e:\\.env\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 577, in run_cell\n",
      "      return super().run_cell(*args, **kwargs)\n",
      "    File \"e:\\.env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3077, in run_cell\n",
      "      result = self._run_cell(\n",
      "    File \"e:\\.env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3132, in _run_cell\n",
      "      result = runner(coro)\n",
      "    File \"e:\\.env\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "      coro.send(None)\n",
      "    File \"e:\\.env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3336, in run_cell_async\n",
      "      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "    File \"e:\\.env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "      if await self.run_code(code, result, async_=asy):\n",
      "    File \"e:\\.env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3579, in run_code\n",
      "      exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "    File \"C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_20228\\1202912357.py\", line 1048, in <module>\n",
      "      history, metrics_log = train_model(model, lr, train_generator, test_generator, class_weights, counter, OUTPUT_DIR)\n",
      "    File \"C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_20228\\1202912357.py\", line 225, in train_model\n",
      "      preds = np.argmax(model.predict(batch_x, verbose=0), axis=1)\n",
      "    File \"e:\\.env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 2253, in predict\n",
      "      tmp_batch_outputs = self.predict_function(iterator)\n",
      "    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function\n",
      "      return step_function(self, iterator)\n",
      "    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function\n",
      "      outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step\n",
      "      outputs = model.predict_step(data)\n",
      "    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n",
      "      return self(x, training=False)\n",
      "    File \"e:\\.env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n",
      "      return super().__call__(*args, **kwargs)\n",
      "    File \"e:\\.env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n",
      "      outputs = call_fn(inputs, *args, **kwargs)\n",
      "    File \"e:\\.env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n",
      "      return self._run_internal_graph(inputs, training=training, mask=mask)\n",
      "    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n",
      "      outputs = node.layer(*args, **kwargs)\n",
      "    File \"e:\\.env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n",
      "      outputs = call_fn(inputs, *args, **kwargs)\n",
      "    File \"e:\\.env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"e:\\.env\\lib\\site-packages\\keras\\layers\\convolutional\\separable_conv2d.py\", line 188, in call\n",
      "      outputs = tf.compat.v1.nn.separable_conv2d(\n",
      "Node: 'model/block2_sepconv1/separable_conv2d'\n",
      "OOM when allocating tensor with shape[32,109,109,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node model/block2_sepconv1/separable_conv2d}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n",
      " [Op:__inference_predict_function_480163]\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'model/block2_sepconv1/separable_conv2d' defined at (most recent call last):\n    File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"e:\\.env\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"e:\\.env\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"e:\\.env\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"e:\\.env\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 595, in run_forever\n      self._run_once()\n    File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 1881, in _run_once\n      handle._run()\n    File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"e:\\.env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 519, in dispatch_queue\n      await self.process_one()\n    File \"e:\\.env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 508, in process_one\n      await dispatch(*args)\n    File \"e:\\.env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 400, in dispatch_shell\n      await result\n    File \"e:\\.env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 368, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"e:\\.env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n      reply_content = await reply_content\n    File \"e:\\.env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 455, in do_execute\n      res = shell.run_cell(\n    File \"e:\\.env\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 577, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"e:\\.env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3077, in run_cell\n      result = self._run_cell(\n    File \"e:\\.env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3132, in _run_cell\n      result = runner(coro)\n    File \"e:\\.env\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n      coro.send(None)\n    File \"e:\\.env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3336, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"e:\\.env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3519, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"e:\\.env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3579, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_20228\\1202912357.py\", line 1048, in <module>\n      history, metrics_log = train_model(model, lr, train_generator, test_generator, class_weights, counter, OUTPUT_DIR)\n    File \"C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_20228\\1202912357.py\", line 225, in train_model\n      preds = np.argmax(model.predict(batch_x, verbose=0), axis=1)\n    File \"e:\\.env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 2253, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function\n      return step_function(self, iterator)\n    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step\n      outputs = model.predict_step(data)\n    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n      return self(x, training=False)\n    File \"e:\\.env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"e:\\.env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"e:\\.env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"e:\\.env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"e:\\.env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"e:\\.env\\lib\\site-packages\\keras\\layers\\convolutional\\separable_conv2d.py\", line 188, in call\n      outputs = tf.compat.v1.nn.separable_conv2d(\nNode: 'model/block2_sepconv1/separable_conv2d'\nOOM when allocating tensor with shape[32,109,109,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model/block2_sepconv1/separable_conv2d}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_predict_function_480163]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1048\u001b[0m\n\u001b[0;32m   1046\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Training \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with lr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, dr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, counter=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcounter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1047\u001b[0m model \u001b[38;5;241m=\u001b[39m create_model(dr, num_classes\u001b[38;5;241m=\u001b[39mNUM_CLASSES)\n\u001b[1;32m-> 1048\u001b[0m history, metrics_log \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcounter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1049\u001b[0m histories\u001b[38;5;241m.\u001b[39mappend((history, metrics_log))\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Computing per-class training accuracy...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 225\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, learning_rate, train_generator, test_generator, class_weights, counter, output_dir)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Collecting training predictions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_x, batch_y \u001b[38;5;129;01min\u001b[39;00m train_generator:\n\u001b[1;32m--> 225\u001b[0m     preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    226\u001b[0m     true_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(batch_y, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    227\u001b[0m     train_preds\u001b[38;5;241m.\u001b[39mextend(preds)\n",
      "File \u001b[1;32me:\\.env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32me:\\.env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'model/block2_sepconv1/separable_conv2d' defined at (most recent call last):\n    File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"e:\\.env\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"e:\\.env\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"e:\\.env\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"e:\\.env\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 595, in run_forever\n      self._run_once()\n    File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 1881, in _run_once\n      handle._run()\n    File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"e:\\.env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 519, in dispatch_queue\n      await self.process_one()\n    File \"e:\\.env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 508, in process_one\n      await dispatch(*args)\n    File \"e:\\.env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 400, in dispatch_shell\n      await result\n    File \"e:\\.env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 368, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"e:\\.env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n      reply_content = await reply_content\n    File \"e:\\.env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 455, in do_execute\n      res = shell.run_cell(\n    File \"e:\\.env\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 577, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"e:\\.env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3077, in run_cell\n      result = self._run_cell(\n    File \"e:\\.env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3132, in _run_cell\n      result = runner(coro)\n    File \"e:\\.env\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n      coro.send(None)\n    File \"e:\\.env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3336, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"e:\\.env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3519, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"e:\\.env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3579, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_20228\\1202912357.py\", line 1048, in <module>\n      history, metrics_log = train_model(model, lr, train_generator, test_generator, class_weights, counter, OUTPUT_DIR)\n    File \"C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_20228\\1202912357.py\", line 225, in train_model\n      preds = np.argmax(model.predict(batch_x, verbose=0), axis=1)\n    File \"e:\\.env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 2253, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function\n      return step_function(self, iterator)\n    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step\n      outputs = model.predict_step(data)\n    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n      return self(x, training=False)\n    File \"e:\\.env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"e:\\.env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"e:\\.env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"e:\\.env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"e:\\.env\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"e:\\.env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"e:\\.env\\lib\\site-packages\\keras\\layers\\convolutional\\separable_conv2d.py\", line 188, in call\n      outputs = tf.compat.v1.nn.separable_conv2d(\nNode: 'model/block2_sepconv1/separable_conv2d'\nOOM when allocating tensor with shape[32,109,109,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model/block2_sepconv1/separable_conv2d}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_predict_function_480163]"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, ReLU, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, CSVLogger\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, classification_report, accuracy_score, precision_recall_curve\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import zoom\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "IMG_SIZE_DEFAULT = (224, 224)\n",
    "KERNEL_INITIALIZER = \"glorot_uniform\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS_INITIAL = 100\n",
    "FORCE_TRAIN = True\n",
    "LEARNING_RATES = [0.002]\n",
    "DROPOUT_RATES = [0.2]\n",
    "MODEL_NAME = 'Xception'\n",
    "LAST_CONV_LAYER_NAME = 'block14_sepconv2_act'\n",
    "\n",
    "# Define datasets with directory paths (generators will be set dynamically)\n",
    "DATASETS = [\n",
    "    {\n",
    "        \"name\": \"Imbalanced Dataset 1\",\n",
    "        \"train_dir\": r\"E:\\Data001\\SplitData\\train\",\n",
    "        \"test_dir\": r\"E:\\Data001\\SplitData\\test\",\n",
    "        \"output_dir\": r\"E:\\Data001\\SplitData\\xception_Output_using_tensorflow\",\n",
    "        \"class_names\": None,  \n",
    "        \"num_classes\": None   \n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Imbalanced Dataset 2\",\n",
    "        \"train_dir\": r\"E:\\Data002\\SplitData\\train\",\n",
    "        \"test_dir\": r\"E:\\Data002\\SplitData\\test\",\n",
    "        \"output_dir\": r\"E:\\Data002\\SplitData\\xception_Output_using_tensorflow\",\n",
    "        \"class_names\": None,\n",
    "        \"num_classes\": None\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Imbalanced Combined Dataset\",\n",
    "        \"train_dir\": r\"E:\\CombineData001_002\\SplitData\\train\",\n",
    "        \"test_dir\": r\"E:\\CombineData001_002\\SplitData\\test\",\n",
    "        \"output_dir\": r\"E:\\CombineData001_002\\SplitData\\xception_Output_using_tensorflow\",\n",
    "        \"class_names\": None,\n",
    "        \"num_classes\": None\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Balanced Dataset 1\",\n",
    "        \"train_dir\": r\"E:\\Data001\\SplitData1\\train\",\n",
    "        \"test_dir\": r\"E:\\Data001\\SplitData1\\test\",\n",
    "        \"output_dir\": r\"E:\\Data001\\SplitData1\\xception_Output_using_tensorflow\",\n",
    "        \"class_names\": None,\n",
    "        \"num_classes\": None\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Balanced Dataset 2\",\n",
    "        \"train_dir\": r\"E:\\Data002\\SplitData1\\train\",\n",
    "        \"test_dir\": r\"E:\\Data002\\SplitData1\\test\",\n",
    "        \"output_dir\": r\"E:\\Data002\\SplitData1\\xception_Output_using_tensorflow\",\n",
    "        \"class_names\": None,\n",
    "        \"num_classes\": None\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Balanced Combined Dataset\",\n",
    "        \"train_dir\": r\"E:\\CombineData001_002\\SplitData1\\train\",\n",
    "        \"test_dir\": r\"E:\\CombineData001_002\\SplitData1\\test\",\n",
    "        \"output_dir\": r\"E:\\CombineData001_002\\SplitData1\\xception_Output_using_tensorflow\",\n",
    "        \"class_names\": None,\n",
    "        \"num_classes\": None\n",
    "    }\n",
    "]\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "print(f\"Using device: {physical_devices}, Num GPUs: {len(physical_devices)}\")\n",
    "\n",
    "def get_img_array(img, size):\n",
    "    array = keras.utils.img_to_array(img)\n",
    "    array = np.expand_dims(array, axis=0)\n",
    "    return array\n",
    "\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    try:\n",
    "        grad_model = keras.models.Model(\n",
    "            model.inputs, [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "        )\n",
    "        with tf.GradientTape() as tape:\n",
    "            last_conv_layer_output, preds = grad_model(img_array)\n",
    "            if pred_index is None:\n",
    "                pred_index = tf.argmax(preds[0])\n",
    "            class_channel = preds[:, pred_index]\n",
    "        grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "        last_conv_layer_output = last_conv_layer_output[0]\n",
    "        heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "        heatmap = tf.squeeze(heatmap)\n",
    "        heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap + 1e-8)\n",
    "        return heatmap.numpy()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in make_gradcam_heatmap: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_and_preprocess_image(img_path):\n",
    "    \"\"\"Load and preprocess an image for Xception model.\"\"\"\n",
    "    try:\n",
    "        img = tf.keras.utils.load_img(img_path, target_size=IMG_SIZE_DEFAULT)\n",
    "        img_array = tf.keras.utils.img_to_array(img)\n",
    "        img_array = tf.keras.applications.xception.preprocess_input(img_array)\n",
    "        return img_array\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {img_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_model(dropout_rate, num_classes):\n",
    "    try:\n",
    "        base_model = Xception(weights='imagenet', include_top=False, input_shape=(*IMG_SIZE_DEFAULT, 3))\n",
    "        base_model.trainable = False\n",
    "        for layer in base_model.layers[-20:]:\n",
    "            layer.trainable = True\n",
    "        x = base_model.output\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Dense(512, kernel_initializer=KERNEL_INITIALIZER)(x)\n",
    "        x = ReLU()(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(dropout_rate)(ReLU()(x))\n",
    "        x = Dense(128, kernel_initializer=KERNEL_INITIALIZER)(x)\n",
    "        x = ReLU()(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "        outputs = Dense(num_classes, activation='softmax', kernel_initializer='random_uniform')(x)\n",
    "        model = Model(inputs=base_model.input, outputs=outputs)\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error in create_model: {e}\")\n",
    "        raise\n",
    "\n",
    "def compute_metrics(y_true, y_pred, dataset_name):\n",
    "    try:\n",
    "        print(f\"Computing {dataset_name} metrics: y_true shape={np.array(y_true).shape}, y_pred shape={np.array(y_pred).shape}\")\n",
    "        print(f\"y_true sample: {y_true[:5]}, y_pred sample: {y_pred[:5]}\")\n",
    "        precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        print(f\"{dataset_name} metrics - Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "        return {\n",
    "            f\"{dataset_name}_precision\": precision,\n",
    "            f\"{dataset_name}_recall\": recall,\n",
    "            f\"{dataset_name}_f1\": f1\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error in compute_metrics: {e}\")\n",
    "        return {f\"{dataset_name}_precision\": 0, f\"{dataset_name}_recall\": 0, f\"{dataset_name}_f1\": 0}\n",
    "\n",
    "def train_model(model, learning_rate, train_generator, test_generator, class_weights, counter, output_dir):\n",
    "    try:\n",
    "        optimizer = Nadam(learning_rate=learning_rate)\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        print(f\"Model compiled with optimizer: {model.optimizer}\")\n",
    "\n",
    "        reduce_lr = ReduceLROnPlateau(factor=0.5, patience=3, monitor='val_loss')\n",
    "        early_stopping = EarlyStopping(patience=10, monitor='val_loss', restore_best_weights=True)\n",
    "        csv_logger = CSVLogger(\n",
    "            os.path.join(output_dir, f'{MODEL_NAME}_metrics_per_epoch_{counter}.csv'),\n",
    "            separator=',', append=True\n",
    "        )\n",
    "\n",
    "        class TimeHistory(keras.callbacks.Callback):\n",
    "            def on_train_begin(self, logs={}):\n",
    "                self.times = []\n",
    "            def on_epoch_begin(self, epoch, logs={}):\n",
    "                self.epoch_time_start = time.time()\n",
    "            def on_epoch_end(self, epoch, logs={}):\n",
    "                self.times.append(time.time() - self.epoch_time_start)\n",
    "\n",
    "        time_callback = TimeHistory()\n",
    "\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            epochs=EPOCHS_INITIAL,\n",
    "            validation_data=test_generator,\n",
    "            class_weight=class_weights,\n",
    "            callbacks=[reduce_lr, early_stopping, csv_logger, time_callback],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        history_dict = {\n",
    "            'train_loss': history.history['loss'],\n",
    "            'train_acc': history.history['accuracy'],\n",
    "            'val_loss': history.history.get('val_loss', []),\n",
    "            'val_acc': history.history.get('val_accuracy', []),\n",
    "            'time': time_callback.times\n",
    "        }\n",
    "\n",
    "        metrics_log = {'train_precision': [], 'train_recall': [], 'train_f1': [],\n",
    "                       'test_precision': [], 'test_recall': [], 'test_f1': []}\n",
    "        \n",
    "        for epoch in range(len(history_dict['train_loss'])):\n",
    "            train_preds, train_labels = [], []\n",
    "            train_generator.reset()\n",
    "            print(f\"Epoch {epoch + 1}: Collecting training predictions\")\n",
    "            for batch_x, batch_y in train_generator:\n",
    "                preds = np.argmax(model.predict(batch_x, verbose=0), axis=1)\n",
    "                true_labels = np.argmax(batch_y, axis=1)\n",
    "                train_preds.extend(preds)\n",
    "                train_labels.extend(true_labels)\n",
    "                if len(train_preds) >= train_generator.samples:\n",
    "                    break\n",
    "            train_metrics = compute_metrics(train_labels, train_preds, 'train')\n",
    "            metrics_log['train_precision'].append(train_metrics['train_precision'])\n",
    "            metrics_log['train_recall'].append(train_metrics['train_recall'])\n",
    "            metrics_log['train_f1'].append(train_metrics['train_f1'])\n",
    "\n",
    "            test_preds, test_labels = [], []\n",
    "            test_generator.reset()\n",
    "            print(f\"Epoch {epoch + 1}: Collecting test predictions\")\n",
    "            for batch_x, batch_y in test_generator:\n",
    "                preds = np.argmax(model.predict(batch_x, verbose=0), axis=1)\n",
    "                true_labels = np.argmax(batch_y, axis=1)\n",
    "                test_preds.extend(preds)\n",
    "                test_labels.extend(true_labels)\n",
    "                if len(test_preds) >= test_generator.samples:\n",
    "                    break\n",
    "            test_metrics = compute_metrics(test_labels, test_preds, 'test')\n",
    "            metrics_log['test_precision'].append(test_metrics['test_precision'])\n",
    "            metrics_log['test_recall'].append(test_metrics['test_recall'])\n",
    "            metrics_log['test_f1'].append(test_metrics['test_f1'])\n",
    "\n",
    "            csv_path = os.path.join(output_dir, f'{MODEL_NAME}_metrics_per_epoch_{counter}.csv')\n",
    "            if FORCE_TRAIN and epoch == 0 and os.path.exists(csv_path):\n",
    "                os.remove(csv_path) \n",
    "            with open(csv_path, 'a', newline='') as csvfile:\n",
    "                fieldnames = [\n",
    "                    'epoch', 'time', 'train/loss', 'train/accuracy', 'train/precision', 'train/recall', 'train/f1',\n",
    "                    'test/loss', 'test/accuracy', 'test/precision', 'test/recall', 'test/f1', 'lr'\n",
    "                ]\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                if csvfile.tell() == 0:\n",
    "                    writer.writeheader()\n",
    "                writer.writerow({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'time': history_dict['time'][epoch],\n",
    "                    'train/loss': history_dict['train_loss'][epoch],\n",
    "                    'train/accuracy': history_dict['train_acc'][epoch],\n",
    "                    'train/precision': train_metrics['train_precision'],\n",
    "                    'train/recall': train_metrics['train_recall'],\n",
    "                    'train/f1': train_metrics['train_f1'],\n",
    "                    'test/loss': history_dict['val_loss'][epoch] if epoch < len(history_dict['val_loss']) else 0,\n",
    "                    'test/accuracy': history_dict['val_acc'][epoch] if epoch < len(history_dict['val_acc']) else 0,\n",
    "                    'test/precision': test_metrics['test_precision'],\n",
    "                    'test/recall': test_metrics['test_recall'],\n",
    "                    'test/f1': test_metrics['test_f1'],\n",
    "                    'lr': tf.keras.backend.get_value(model.optimizer.learning_rate)\n",
    "                })\n",
    "                history_dict = {\n",
    "            'train_loss': history.history['loss'],\n",
    "            'train_acc': history.history['accuracy'],\n",
    "            'val_loss': history.history.get('val_loss', []),\n",
    "            'val_acc': history.history.get('val_accuracy', []),\n",
    "            'time': time_callback.times\n",
    "        }\n",
    "\n",
    "        metrics_log = {'train_precision': [], 'train_recall': [], 'train_f1': [],\n",
    "                       'test_precision': [], 'test_recall': [], 'test_f1': []}\n",
    "\n",
    "        model.save_weights(os.path.join(output_dir, f'{MODEL_NAME}_{counter}.weights.h5'))\n",
    "        \n",
    "        with open(os.path.join(output_dir, f'{MODEL_NAME}_{counter}_history.json'), 'w') as f:\n",
    "            json.dump(history_dict, f)\n",
    "        \n",
    "        print(f\"Training completed. Epoch times: {history_dict['time']}\")\n",
    "        return history_dict, metrics_log\n",
    "    except Exception as e:\n",
    "        print(f\"Error in train_model: {e}\")\n",
    "        raise\n",
    "\n",
    "def evaluate_model(model, test_generator, class_names, counter):\n",
    "    try:\n",
    "        test_loss, test_acc = 0, 0\n",
    "        test_preds, test_labels, test_probs, test_images, test_filenames = [], [], [], [], []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        test_generator.reset()\n",
    "        for batch_x, batch_y in test_generator:\n",
    "            loss, acc = model.test_on_batch(batch_x, batch_y)\n",
    "            test_loss += loss * batch_x.shape[0]\n",
    "            test_acc += acc * batch_x.shape[0]\n",
    "            preds = np.argmax(model.predict(batch_x, verbose=0), axis=1)\n",
    "            true_labels = np.argmax(batch_y, axis=1)\n",
    "            probs = model.predict(batch_x, verbose=0)\n",
    "            test_preds.extend(preds)\n",
    "            test_labels.extend(true_labels)\n",
    "            test_probs.extend(probs)\n",
    "            test_images.extend(batch_x)\n",
    "            test_filenames.extend(test_generator.filenames)\n",
    "            if len(test_preds) >= test_generator.samples:\n",
    "                break\n",
    "        \n",
    "        test_loss /= test_generator.samples\n",
    "        test_acc /= test_generator.samples\n",
    "        inference_time = (time.time() - start_time) * 1000\n",
    "        \n",
    "        test_probs = np.array(test_probs)\n",
    "        if np.any(np.isnan(test_probs)) or np.any(test_probs < 0):\n",
    "            print(\"Warning: test_probs contains NaN or negative values\")\n",
    "            test_probs = np.nan_to_num(test_probs, nan=0.0, posinf=1.0, neginf=0.0)\n",
    "        row_sums = test_probs.sum(axis=1, keepdims=True)\n",
    "        row_sums[row_sums == 0] = 1.0\n",
    "        test_probs = test_probs / row_sums\n",
    "        \n",
    "        test_precision = precision_score(test_labels, test_preds, average='weighted', zero_division=0)\n",
    "        test_recall = recall_score(test_labels, test_preds, average='weighted', zero_division=0)\n",
    "        test_f1 = f1_score(test_labels, test_preds, average='weighted', zero_division=0)\n",
    "        auc = None\n",
    "        if len(np.unique(test_labels)) > 1:\n",
    "            try:\n",
    "                auc = roc_auc_score(test_labels, test_probs, multi_class='ovr')\n",
    "            except ValueError as e:\n",
    "                print(f\"ROC AUC calculation failed: {e}\")\n",
    "                auc = None\n",
    "        precision_per_class = precision_score(test_labels, test_preds, average=None, zero_division=0)\n",
    "        recall_per_class = recall_score(test_labels, test_preds, average=None, zero_division=0)\n",
    "        \n",
    "        auc_display = f\"{auc:.4f}\" if auc is not None else 'N/A'\n",
    "        print(f\"Test Results: Loss={test_loss:.4f}, Acc={test_acc:.4f}, \"\n",
    "              f\"Precision={test_precision:.4f}, Recall={test_recall:.4f}, F1={test_f1:.4f}, \"\n",
    "              f\"AUC={auc_display}, Inference Time={inference_time:.2f}ms\")\n",
    "        print(f\"Per-class Precision: {dict(zip(class_names, precision_per_class))}\")\n",
    "        print(f\"Per-class Recall: {dict(zip(class_names, recall_per_class))}\")\n",
    "        \n",
    "        return {\n",
    "            'Model with input image size': MODEL_NAME,\n",
    "            'Accuracy': test_acc,\n",
    "            'AUC': auc,\n",
    "            'Loss': test_loss,\n",
    "            'Precision': test_precision,\n",
    "            'Recall': test_recall,\n",
    "            'F1 Score': test_f1,\n",
    "            'Inference time (in miliseconds)': inference_time\n",
    "        }, test_labels, test_preds, test_images, test_filenames\n",
    "    except Exception as e:\n",
    "        print(f\"Error in evaluate_model: {e}\")\n",
    "        raise\n",
    "\n",
    "def run_gradcam_detection(model, images, true_labels, pred_labels, filenames, class_names, counter, last_conv_layer_name, output_dir, num_images=6):\n",
    "    try:\n",
    "        detected_boxes = []\n",
    "        detected_images = []\n",
    "        \n",
    "        for img, true_label, pred_label, fname in zip(images[:num_images], true_labels[:num_images], pred_labels[:num_images], filenames[:num_images]):\n",
    "            img_array = get_img_array(img, size=IMG_SIZE_DEFAULT)\n",
    "            heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=pred_label)\n",
    "            if heatmap is None:\n",
    "                print(f\"Skipping image {fname} due to heatmap generation failure\")\n",
    "                continue\n",
    "            heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 1e-8)\n",
    "            \n",
    "            heatmap_resized = zoom(heatmap, (IMG_SIZE_DEFAULT[0] / heatmap.shape[0], IMG_SIZE_DEFAULT[1] / heatmap.shape[1]))\n",
    "            heatmap_resized = np.clip(heatmap_resized, 0, 1)\n",
    "            \n",
    "            threshold = 0.7\n",
    "            binary_map = heatmap_resized > threshold\n",
    "            if not np.any(binary_map):\n",
    "                print(f\"No significant regions detected for {fname}\")\n",
    "                continue\n",
    "            \n",
    "            coords = np.where(binary_map)\n",
    "            if len(coords[0]) == 0:\n",
    "                print(f\"No coordinates found for {fname}\")\n",
    "                continue\n",
    "            x_min, x_max = coords[1].min(), coords[1].max()\n",
    "            y_min, y_max = coords[0].min(), coords[0].max()\n",
    "            x_min, x_max = x_min * (IMG_SIZE_DEFAULT[0] / heatmap_resized.shape[1]), x_max * (IMG_SIZE_DEFAULT[0] / heatmap_resized.shape[1])\n",
    "            y_min, y_max = y_min * (IMG_SIZE_DEFAULT[0] / heatmap_resized.shape[0]), y_max * (IMG_SIZE_DEFAULT[0] / heatmap_resized.shape[0])\n",
    "            \n",
    "            detected_boxes.append({\n",
    "                \"filename\": fname,\n",
    "                \"boxes\": [[x_min, y_min, x_max, y_max]],\n",
    "                \"labels\": [class_names[pred_label]],\n",
    "                \"scores\": [heatmap_resized.max()]\n",
    "            })\n",
    "            \n",
    "            img_denorm = img * np.array([0.229, 0.224, 0.225]) * 255 + np.array([0.485, 0.456, 0.406]) * 255\n",
    "            img_denorm = np.clip(img_denorm, 0, 255).astype(np.uint8)\n",
    "            detected_images.append(img_denorm)\n",
    "            detected_images[-1] = np.concatenate([detected_images[-1], heatmap_resized[..., None] * 255], axis=-1).astype(np.uint8)\n",
    "        \n",
    "        plt.figure(figsize=(15, 10), dpi=300)\n",
    "        for i, (img, detection) in enumerate(zip(detected_images, detected_boxes)):\n",
    "            plt.subplot(2, 3, i + 1)\n",
    "            heatmap = img[..., 3] / 255.0\n",
    "            img_rgb = img[..., :3]\n",
    "            heatmap = cm.jet(heatmap)[..., :3] * 255\n",
    "            overlay = (0.5 * img_rgb + 0.5 * heatmap).astype(np.uint8)\n",
    "            plt.imshow(overlay)\n",
    "            for box, label, score in zip(detection[\"boxes\"], detection[\"labels\"], detection[\"scores\"]):\n",
    "                x_min, y_min, x_max, y_max = box\n",
    "                plt.gca().add_patch(plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min,\n",
    "                                                 edgecolor='red', facecolor='none', linewidth=2))\n",
    "                plt.text(x_min, y_min - 5, f\"{label} ({score:.2f})\", color='red', fontsize=8, bbox=dict(facecolor='white', alpha=0.7))\n",
    "            true_label = class_names[true_labels[i]]\n",
    "            pred_label = class_names[pred_labels[i]]\n",
    "            plt.title(f\"File: {os.path.basename(detection['filename'])}\\nActual: {true_label}\\nPredicted: {pred_label}\", fontsize=10)\n",
    "            plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        output_path = os.path.join(output_dir, f'{MODEL_NAME}_gradcam_detections_{counter}.png')\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight', format='png')\n",
    "        print(f\"Saved Grad-CAM detection plot to: {output_path}\")\n",
    "        plt.close()\n",
    "        \n",
    "        return detected_boxes\n",
    "    except Exception as e:\n",
    "        print(f\"Error in run_gradcam_detection: {e}\")\n",
    "        return []\n",
    "\n",
    "def create_gradcam_bounding_box(img_path, heatmap, output_path, threshold=0.5):\n",
    "    \"\"\"Create bounding boxes based on Grad-CAM heatmap activation.\"\"\"\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Failed to load image: {img_path}\")\n",
    "    img = cv2.resize(img, IMG_SIZE_DEFAULT)\n",
    "    \n",
    "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    _, thresh = cv2.threshold(heatmap, threshold, 1.0, cv2.THRESH_BINARY)\n",
    "    thresh = (thresh * 255).astype(np.uint8)\n",
    "    \n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) > 10:\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.imwrite(output_path, img)\n",
    "    return img\n",
    "\n",
    "def create_traditional_bounding_box(img_path, output_path):\n",
    "    \"\"\"Create bounding boxes using traditional image processing methods.\"\"\"\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Failed to load image: {img_path}\")\n",
    "    img = cv2.resize(img, IMG_SIZE_DEFAULT)\n",
    "    img_copy = img.copy()\n",
    "    \n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    morph = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
    "    combined = cv2.bitwise_or(edges, morph)\n",
    "    \n",
    "    contours, _ = cv2.findContours(combined, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) > 50:\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            cv2.rectangle(img_copy, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "    \n",
    "    cv2.imwrite(output_path, img_copy)\n",
    "    return img_copy\n",
    "\n",
    "def create_combined_bounding_boxes(img_path, heatmap, output_path, gradcam_threshold=0.5):\n",
    "    \"\"\"Create image with both Grad-CAM and traditional bounding boxes.\"\"\"\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Failed to load image: {img_path}\")\n",
    "    img = cv2.resize(img, IMG_SIZE_DEFAULT)\n",
    "    img_copy = img.copy()\n",
    "    \n",
    "    heatmap_resized = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    _, thresh = cv2.threshold(heatmap_resized, gradcam_threshold, 1.0, cv2.THRESH_BINARY)\n",
    "    thresh = (thresh * 255).astype(np.uint8)\n",
    "    contours_gradcam, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    for contour in contours_gradcam:\n",
    "        if cv2.contourArea(contour) > 10:\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            cv2.rectangle(img_copy, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    \n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "    _, thresh_trad = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    morph = cv2.morphologyEx(thresh_trad, cv2.MORPH_CLOSE, kernel)\n",
    "    combined = cv2.bitwise_or(edges, morph)\n",
    "    contours_trad, _ = cv2.findContours(combined, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    for contour in contours_trad:\n",
    "        if cv2.contourArea(contour) > 50:\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            cv2.rectangle(img_copy, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "    \n",
    "    cv2.putText(img_copy, \"Grad-CAM Boxes (Green)\", (10, 30), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "    cv2.putText(img_copy, \"Traditional Boxes (Blue)\", (10, 60), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
    "    \n",
    "    cv2.imwrite(output_path, img_copy)\n",
    "    return img_copy\n",
    "\n",
    "def visualize_detection_boxes(model, test_generator, output_dir, dataset_name, counter, num_samples=5):\n",
    "    \"\"\"Visualize both Grad-CAM and traditional detection boxes.\"\"\"\n",
    "    detection_dir = os.path.join(output_dir, 'detection_boxes')\n",
    "    os.makedirs(detection_dir, exist_ok=True)\n",
    "    \n",
    "    test_generator.reset()\n",
    "    sample_images = []\n",
    "    sample_paths = []\n",
    "    \n",
    "    for i in range(min(num_samples, test_generator.samples)):\n",
    "        x_batch, y_batch = next(test_generator)\n",
    "        sample_images.append(x_batch[0])\n",
    "        sample_paths.append(os.path.join(test_generator.directory, test_generator.filenames[i]))\n",
    "    \n",
    "    for i, (img_array, img_path) in enumerate(zip(sample_images, sample_paths)):\n",
    "        try:\n",
    "            img_for_gradcam = np.expand_dims(img_array, axis=0)\n",
    "            preds = model.predict(img_for_gradcam, verbose=0)\n",
    "            pred_class = np.argmax(preds[0])\n",
    "            pred_prob = np.max(preds[0])\n",
    "            true_class = test_generator.classes[i]\n",
    "            \n",
    "            heatmap = make_gradcam_heatmap(img_for_gradcam, model, LAST_CONV_LAYER_NAME, pred_class)\n",
    "            if heatmap is None:\n",
    "                print(f\"Skipping image {img_path} due to heatmap generation failure\")\n",
    "                continue\n",
    "            \n",
    "            base_filename = f\"{dataset_name.replace(' ', '_')}_sample_{i}_true_{true_class}_pred_{pred_class}\"\n",
    "            \n",
    "            original_img = cv2.imread(img_path)\n",
    "            if original_img is None:\n",
    "                print(f\"Failed to load image: {img_path}\")\n",
    "                continue\n",
    "            original_img = cv2.resize(original_img, IMG_SIZE_DEFAULT)\n",
    "            cv2.imwrite(os.path.join(detection_dir, f\"{base_filename}_original.png\"), original_img)\n",
    "            \n",
    "            gradcam_bbox_path = os.path.join(detection_dir, f\"{base_filename}_gradcam_boxes.png\")\n",
    "            create_gradcam_bounding_box(img_path, heatmap, gradcam_bbox_path)\n",
    "            \n",
    "            traditional_bbox_path = os.path.join(detection_dir, f\"{base_filename}_traditional_boxes.png\")\n",
    "            create_traditional_bounding_box(img_path, traditional_bbox_path)\n",
    "            \n",
    "            combined_bbox_path = os.path.join(detection_dir, f\"{base_filename}_combined_boxes.png\")\n",
    "            create_combined_bounding_boxes(img_path, heatmap, combined_bbox_path)\n",
    "            \n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.imshow(heatmap, cmap='jet')\n",
    "            plt.title(f'Grad-CAM Heatmap\\nTrue: {true_class}, Pred: {pred_class}, Prob: {pred_prob:.2f}')\n",
    "            plt.colorbar()\n",
    "            plt.savefig(os.path.join(detection_dir, f\"{base_filename}_heatmap.png\"), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"Generated detection boxes for sample {i}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Detection box visualizations saved to: {detection_dir}\")\n",
    "\n",
    "def count_bounding_boxes_gradcam(heatmap, threshold=0.5):\n",
    "    \"\"\"Count number of bounding boxes from Grad-CAM heatmap.\"\"\"\n",
    "    heatmap_resized = cv2.resize(heatmap, IMG_SIZE_DEFAULT)\n",
    "    _, thresh = cv2.threshold(heatmap_resized, threshold, 1.0, cv2.THRESH_BINARY)\n",
    "    thresh = (thresh * 255).astype(np.uint8)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    return len([c for c in contours if cv2.contourArea(c) > 10])\n",
    "\n",
    "def count_bounding_boxes_traditional(img_path):\n",
    "    \"\"\"Count number of bounding boxes from traditional method.\"\"\"\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Failed to load image: {img_path}\")\n",
    "    img = cv2.resize(img, IMG_SIZE_DEFAULT)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    morph = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
    "    combined = cv2.bitwise_or(edges, morph)\n",
    "    \n",
    "    contours, _ = cv2.findContours(combined, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    return len([c for c in contours if cv2.contourArea(c) > 50])\n",
    "\n",
    "def analyze_detection_comparison(model, test_generator, output_dir, dataset_name, counter, num_samples=10):\n",
    "    \"\"\"Analyze and compare Grad-CAM vs traditional detection methods.\"\"\"\n",
    "    comparison_dir = os.path.join(output_dir, 'detection_comparison')\n",
    "    os.makedirs(comparison_dir, exist_ok=True)\n",
    "    \n",
    "    results = []\n",
    "    test_generator.reset()\n",
    "    \n",
    "    for i in range(min(num_samples, test_generator.samples)):\n",
    "        try:\n",
    "            x_batch, y_batch = next(test_generator)\n",
    "            img_array = x_batch[0]\n",
    "            img_path = os.path.join(test_generator.directory, test_generator.filenames[i])\n",
    "            \n",
    "            img_for_gradcam = np.expand_dims(img_array, axis=0)\n",
    "            preds = model.predict(img_for_gradcam, verbose=0)\n",
    "            pred_class = np.argmax(preds[0])\n",
    "            \n",
    "            heatmap = make_gradcam_heatmap(img_for_gradcam, model, LAST_CONV_LAYER_NAME, pred_class)\n",
    "            if heatmap is None:\n",
    "                print(f\"Skipping sample {i} due to heatmap generation failure\")\n",
    "                continue\n",
    "            \n",
    "            gradcam_boxes = count_bounding_boxes_gradcam(heatmap)\n",
    "            traditional_boxes = count_bounding_boxes_traditional(img_path)\n",
    "            \n",
    "            results.append({\n",
    "                'sample_id': i,\n",
    "                'gradcam_boxes': gradcam_boxes,\n",
    "                'traditional_boxes': traditional_boxes,\n",
    "                'difference': abs(gradcam_boxes - traditional_boxes)\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in comparison analysis for sample {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    comparison_df = pd.DataFrame(results)\n",
    "    comparison_df.to_csv(os.path.join(comparison_dir, f'detection_comparison_{counter}.csv'), index=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    x = range(len(results))\n",
    "    plt.bar(x, [r['gradcam_boxes'] for r in results], alpha=0.7, label='Grad-CAM Boxes', color='green')\n",
    "    plt.bar(x, [r['traditional_boxes'] for r in results], alpha=0.7, label='Traditional Boxes', color='blue')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Number of Detection Boxes')\n",
    "    plt.title('Grad-CAM vs Traditional Detection Box Comparison')\n",
    "    plt.legend()\n",
    "    plt.xticks(x)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(comparison_dir, f'comparison_plot_{counter}.png'), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Comparison analysis saved to: {comparison_dir}\")\n",
    "\n",
    "def plot_training_history(history, metrics_log, output_dir, model_name, counter, class_names, best_y_true, best_y_pred):\n",
    "    try:\n",
    "        plt.figure(figsize=(18, 6), dpi=300)\n",
    "        \n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(range(1, len(history['train_acc']) + 1), history['train_acc'], label='Train Accuracy', \n",
    "                 color='#1f77b4', linewidth=2)\n",
    "        plt.plot(range(1, len(history['val_acc']) + 1), history['val_acc'], label='Test Accuracy', \n",
    "                 color='#ff7f0e', linestyle='--', linewidth=2)\n",
    "        plt.title('Train vs Test Accuracy', fontsize=14, pad=10)\n",
    "        plt.xlabel('Epoch', fontsize=12)\n",
    "        plt.ylabel('Accuracy', fontsize=12)\n",
    "        plt.legend(fontsize=10)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.ylim(0, 1.05)\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(range(1, len(history['train_loss']) + 1), history['train_loss'], label='Train Loss', \n",
    "                 color='#1f77b4', linewidth=2)\n",
    "        plt.plot(range(1, len(history['val_loss']) + 1), history['val_loss'], label='Test Loss', \n",
    "                 color='#ff7f0e', linestyle='--', linewidth=2)\n",
    "        plt.title('Train vs Test Loss', fontsize=14, pad=10)\n",
    "        plt.xlabel('Epoch', fontsize=12)\n",
    "        plt.ylabel('Loss', fontsize=12)\n",
    "        plt.legend(fontsize=10)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.ylim(0, max(max(history['train_loss'], default=1), max(history['val_loss'], default=1)) * 1.1)\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        cm = confusion_matrix(best_y_true, best_y_pred, labels=range(len(class_names)))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=class_names, yticklabels=class_names, \n",
    "                    cbar=False, square=True)\n",
    "        plt.title('Test Confusion Matrix', fontsize=14, pad=10)\n",
    "        plt.xlabel('Predicted', fontsize=12)\n",
    "        plt.ylabel('True', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "        plt.yticks(fontsize=10)\n",
    "        \n",
    "        plt.tight_layout(pad=2.0)\n",
    "        output_path = os.path.join(output_dir, f'{MODEL_NAME}_training_metrics_{counter}.png')\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight', format='png')\n",
    "        print(f\"Saved plot to: {output_path}\")\n",
    "        plt.close()\n",
    "        \n",
    "        for metric in ['precision', 'recall', 'f1']:\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(range(1, len(metrics_log[f'train_{metric}']) + 1), metrics_log[f'train_{metric}'], \n",
    "                     label=f'Train {metric.capitalize()}', color='#1f77b4')\n",
    "            plt.plot(range(1, len(metrics_log[f'test_{metric}']) + 1), metrics_log[f'test_{metric}'], \n",
    "                     label=f'Test {metric.capitalize()}', color='#ff7f0e', linestyle='--')\n",
    "            plt.title(f'{MODEL_NAME} {metric.capitalize()}', fontsize=14)\n",
    "            plt.ylabel(metric.capitalize(), fontsize=12)\n",
    "            plt.xlabel('Epoch', fontsize=12)\n",
    "            plt.legend(fontsize=10)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.savefig(os.path.join(output_dir, f'{MODEL_NAME}_{metric}_{counter}.png'), dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "        \n",
    "        error_tipo_I, error_tipo_II = [], []\n",
    "        for i in range(len(class_names)):\n",
    "            FP = sum(cm[:, i]) - cm[i, i]\n",
    "            FN = sum(cm[i, :]) - cm[i, i]\n",
    "            TP = cm[i, i]\n",
    "            TN = cm.sum() - (FP + FN + TP)\n",
    "            FPR = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
    "            FNR = FN / (FN + TP) if (FN + TP) > 0 else 0\n",
    "            error_tipo_I.append(FPR)\n",
    "            error_tipo_II.append(FNR)\n",
    "        \n",
    "        df_errores = pd.DataFrame({\n",
    "            'Clase': class_names,\n",
    "            'Error Tipo I (FPR)': error_tipo_I,\n",
    "            'Error Tipo II (FNR)': error_tipo_II\n",
    "        })\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(best_y_true, best_y_pred, target_names=class_names, digits=4, zero_division=0))\n",
    "        pd.set_option('display.float_format', '{:.4g}'.format)\n",
    "        print(\"\\nErrores Tipo I y Tipo II por clase:\")\n",
    "        print(df_errores.to_string(index=False))\n",
    "    except Exception as e:\n",
    "        print(f\"Error in plot_training_history: {e}\")\n",
    "\n",
    "def display_classification_images(images, true_labels, pred_labels, class_names, output_dir, counter, num_images=5):\n",
    "    try:\n",
    "        plt.figure(figsize=(15, 5), dpi=300)\n",
    "        images = np.array(images)\n",
    "        mean = np.array([0.485, 0.456, 0.406]) * 255\n",
    "        std = np.array([0.229, 0.224, 0.225]) * 255\n",
    "        images = images * std + mean\n",
    "        images = np.clip(images, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        for i in range(min(num_images, len(images))):\n",
    "            plt.subplot(1, num_images, i + 1)\n",
    "            plt.imshow(images[i])\n",
    "            true_label = class_names[true_labels[i]]\n",
    "            pred_label = class_names[pred_labels[i]]\n",
    "            plt.title(f\"Actual: {true_label}\\nPredicted: {pred_label}\", fontsize=10)\n",
    "            plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        output_path = os.path.join(output_dir, f'{MODEL_NAME}_sample_predictions_{counter}.png')\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight', format='png')\n",
    "        print(f\"Saved classification sample predictions plot to: {output_path}\")\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in display_classification_images: {e}\")\n",
    "\n",
    "def save_chartjs_configs(history, output_dir, model_name, counter):\n",
    "    try:\n",
    "        loss_chart = {\n",
    "            \"type\": \"line\",\n",
    "            \"data\": {\n",
    "                \"labels\": [str(i+1) for i in range(len(history['train_loss']))],\n",
    "                \"datasets\": [\n",
    "                    {\n",
    "                        \"label\": \"Train Loss\",\n",
    "                        \"data\": history['train_loss'],\n",
    "                        \"borderColor\": \"rgba(31, 119, 180, 1)\",\n",
    "                        \"backgroundColor\": \"rgba(31, 119, 180, 0.2)\",\n",
    "                        \"fill\": False,\n",
    "                        \"tension\": 0.4\n",
    "                    },\n",
    "                    {\n",
    "                        \"label\": \"Test Loss\",\n",
    "                        \"data\": history['val_loss'],\n",
    "                        \"borderColor\": \"rgba(255, 127, 14, 1)\",\n",
    "                        \"backgroundColor\": \"rgba(255, 127, 14, 0.2)\",\n",
    "                        \"fill\": False,\n",
    "                        \"borderDash\": [5, 5]\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \"options\": {\n",
    "                \"responsive\": True,\n",
    "                \"plugins\": {\n",
    "                    \"legend\": {\"position\": \"top\"},\n",
    "                    \"title\": {\"display\": True, \"text\": \"Train vs Test Loss\"}\n",
    "                },\n",
    "                \"scales\": {\n",
    "                    \"x\": {\"title\": {\"display\": True, \"text\": \"Epoch\"}},\n",
    "                    \"y\": {\"title\": {\"display\": True, \"text\": \"Loss\"}, \"beginAtZero\": True}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        accuracy_chart = {\n",
    "            \"type\": \"line\",\n",
    "            \"data\": {\n",
    "                \"labels\": [str(i+1) for i in range(len(history['train_acc']))],\n",
    "                \"datasets\": [\n",
    "                    {\n",
    "                        \"label\": \"Train Accuracy\",\n",
    "                        \"data\": history['train_acc'],\n",
    "                        \"borderColor\": \"rgba(31, 119, 180, 1)\",\n",
    "                        \"backgroundColor\": \"rgba(31, 119, 180, 0.2)\",\n",
    "                        \"fill\": False,\n",
    "                        \"tension\": 0.4\n",
    "                    },\n",
    "                    {\n",
    "                        \"label\": \"Test Accuracy\",\n",
    "                        \"data\": history['val_acc'],\n",
    "                        \"borderColor\": \"rgba(255, 127, 14, 1)\",\n",
    "                        \"backgroundColor\": \"rgba(255, 127, 14, 0.2)\",\n",
    "                        \"fill\": False,\n",
    "                        \"borderDash\": [5, 5]\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \"options\": {\n",
    "                \"responsive\": True,\n",
    "                \"plugins\": {\n",
    "                    \"legend\": {\"position\": \"top\"},\n",
    "                    \"title\": {\"display\": True, \"text\": \"Train vs Test Accuracy\"}\n",
    "                },\n",
    "                \"scales\": {\n",
    "                    \"x\": {\"title\": {\"display\": True, \"text\": \"Epoch\"}},\n",
    "                    \"y\": {\"title\": {\"display\": True, \"text\": \"Accuracy\"}, \"beginAtZero\": True, \"max\": 1}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        with open(os.path.join(output_dir, f'{MODEL_NAME}_loss_chart_{counter}.json'), 'w') as f:\n",
    "            json.dump(loss_chart, f, indent=2)\n",
    "        with open(os.path.join(output_dir, f'{MODEL_NAME}_accuracy_chart_{counter}.json'), 'w') as f:\n",
    "            json.dump(accuracy_chart, f, indent=2)\n",
    "        print(f\"Saved Chart.js configurations to: {output_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in save_chartjs_configs: {e}\")\n",
    "\n",
    "def plot_per_class_accuracy(model, train_generator, class_names, output_dir, counter, epochs):\n",
    "    \"\"\"\n",
    "    Compute and visualize per-class accuracy on the training set for each epoch.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Keras model.\n",
    "        train_generator: Training data generator.\n",
    "        class_names: List of class names.\n",
    "        output_dir: Directory to save the plot and Chart.js config.\n",
    "        counter: Counter for unique file naming.\n",
    "        epochs: Number of epochs to evaluate.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        per_class_accuracies = {class_name: [] for class_name in class_names}\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            train_generator.reset()\n",
    "            y_true, y_pred = [], []\n",
    "            print(f\"Epoch {epoch + 1}: Collecting per-class training predictions\")\n",
    "            for batch_x, batch_y in train_generator:\n",
    "                preds = np.argmax(model.predict(batch_x, verbose=0), axis=1)\n",
    "                true_labels = np.argmax(batch_y, axis=1)\n",
    "                y_true.extend(true_labels)\n",
    "                y_pred.extend(preds)\n",
    "                if len(y_true) >= train_generator.samples:\n",
    "                    break\n",
    "            \n",
    "            y_true = np.array(y_true)\n",
    "            y_pred = np.array(y_pred)\n",
    "            \n",
    "            for i, class_name in enumerate(class_names):\n",
    "                class_mask = y_true == i\n",
    "                if np.sum(class_mask) > 0:\n",
    "                    class_acc = accuracy_score(y_true[class_mask], y_pred[class_mask])\n",
    "                else:\n",
    "                    class_acc = 0.0\n",
    "                per_class_accuracies[class_name].append(class_acc)\n",
    "        \n",
    "        colors = ['rgba(31, 119, 180, 1)', 'rgba(255, 127, 14, 1)', 'rgba(44, 160, 44, 1)', \n",
    "                  'rgba(214, 39, 40, 1)', 'rgba(148, 103, 189, 1)']\n",
    "        accuracy_chart_config = {\n",
    "            \"type\": \"line\",\n",
    "            \"data\": {\n",
    "                \"labels\": [str(i+1) for i in range(epochs)],\n",
    "                \"datasets\": [\n",
    "                    {\n",
    "                        \"label\": class_name,\n",
    "                        \"data\": per_class_accuracies[class_name],\n",
    "                        \"borderColor\": colors[i % len(colors)],\n",
    "                        \"backgroundColor\": colors[i % len(colors)].replace('1)', '0.2)'),\n",
    "                        \"fill\": False,\n",
    "                        \"tension\": 0.4\n",
    "                    } for i, class_name in enumerate(class_names)\n",
    "                ]\n",
    "            },\n",
    "            \"options\": {\n",
    "                \"responsive\": True,\n",
    "                \"plugins\": {\n",
    "                    \"legend\": {\"position\": \"top\"},\n",
    "                    \"title\": {\"display\": True, \"text\": \"Per-Class Training Accuracy\"}\n",
    "                },\n",
    "                \"scales\": {\n",
    "                    \"x\": {\"title\": {\"display\": True, \"text\": \"Epoch\"}},\n",
    "                    \"y\": {\"title\": {\"display\": True, \"text\": \"Accuracy\"}, \"min\": 0, \"max\": 1}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        plt.figure(figsize=(10, 6), dpi=300)\n",
    "        for i, class_name in enumerate(class_names):\n",
    "            plt.plot(range(1, epochs + 1), per_class_accuracies[class_name], \n",
    "                     label=f'{class_name}', color=colors[i % len(colors)])\n",
    "        \n",
    "        plt.title('Per-Class Training Accuracy', fontsize=14)\n",
    "        plt.xlabel('Epoch', fontsize=12)\n",
    "        plt.ylabel('Accuracy', fontsize=12)\n",
    "        plt.legend(fontsize=10)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.xlim(1, epochs)\n",
    "        plt.ylim(0, 1)\n",
    "        output_path = os.path.join(output_dir, f'{MODEL_NAME}_per_class_accuracy_{counter}.png')\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Saved per-class accuracy plot to: {output_path}\")\n",
    "        \n",
    "        with open(os.path.join(output_dir, f'{MODEL_NAME}_per_class_accuracy_chart_{counter}.json'), 'w') as f:\n",
    "            json.dump(accuracy_chart_config, f, indent=2)\n",
    "        print(f\"Saved per-class accuracy Chart.js config to: {output_dir}\")\n",
    "        \n",
    "        return per_class_accuracies\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in plot_per_class_accuracy: {e}\")\n",
    "        return {}\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        print(f\"TensorFlow version: {tf.__version__}\")\n",
    "        \n",
    "        for dataset in DATASETS:\n",
    "            dataset_name = dataset['name']\n",
    "            TRAIN_DIR = dataset['train_dir']\n",
    "            TEST_DIR = dataset['test_dir']\n",
    "            OUTPUT_DIR = dataset['output_dir']\n",
    "            \n",
    "            print(f\"\\nProcessing {dataset_name}...\")\n",
    "            os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "            train_datagen = ImageDataGenerator(\n",
    "                rotation_range=15,\n",
    "                horizontal_flip=True,\n",
    "                vertical_flip=True,\n",
    "                zoom_range=[0.8, 1.2],\n",
    "                width_shift_range=0.2,\n",
    "                height_shift_range=0.2,\n",
    "                preprocessing_function=tf.keras.applications.xception.preprocess_input\n",
    "            )\n",
    "            test_datagen = ImageDataGenerator(\n",
    "                preprocessing_function=tf.keras.applications.xception.preprocess_input\n",
    "            )\n",
    "            train_generator = train_datagen.flow_from_directory(\n",
    "                TRAIN_DIR,\n",
    "                target_size=IMG_SIZE_DEFAULT,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                class_mode='categorical',\n",
    "                shuffle=True\n",
    "            )\n",
    "            test_generator = test_datagen.flow_from_directory(\n",
    "                TEST_DIR,\n",
    "                target_size=IMG_SIZE_DEFAULT,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                class_mode='categorical',\n",
    "                shuffle=False\n",
    "            )\n",
    "            \n",
    "            class_names = list(train_generator.class_indices.keys())\n",
    "            NUM_CLASSES = len(class_names)\n",
    "            print(f\"[{dataset_name}] Detected {NUM_CLASSES} classes: {class_names}\")\n",
    "            \n",
    "            if NUM_CLASSES < 2:\n",
    "                raise ValueError(f\"[{dataset_name}] Found {NUM_CLASSES} classes. At least 2 classes are required.\")\n",
    "            \n",
    "            sample_inputs, sample_labels = next(train_generator)\n",
    "            print(f\"[{dataset_name}] Input shape for Xception: {sample_inputs.shape}, Label shape: {sample_labels.shape}\")\n",
    "            \n",
    "            train_classes = train_generator.classes\n",
    "            class_weights = compute_class_weight('balanced', classes=np.unique(train_classes), y=train_classes)\n",
    "            class_weights = dict(enumerate(class_weights))\n",
    "            print(f\"[{dataset_name}] Class weights: {dict(zip(class_names, class_weights.values()))}\")\n",
    "            \n",
    "            test_labels = test_generator.classes\n",
    "            print(f\"[{dataset_name}] Test dataset size: {test_generator.samples}\")\n",
    "            print(f\"[{dataset_name}] Test labels unique: {np.unique(test_labels)}\")\n",
    "            \n",
    "            results = []\n",
    "            counter = 0\n",
    "            histories = []\n",
    "            best_test_accuracy = 0\n",
    "            best_y_true, best_y_pred, best_images, best_filenames = None, None, None, None\n",
    "            \n",
    "            specific_model_path = os.path.join(OUTPUT_DIR, 'Xception_1.weights.h5')\n",
    "            \n",
    "            for lr in LEARNING_RATES:\n",
    "                for dr in DROPOUT_RATES:\n",
    "                    counter += 1\n",
    "                    model_path = os.path.join(OUTPUT_DIR, f'{MODEL_NAME}_{counter}.weights.h5')\n",
    "                    history_json_path = os.path.join(OUTPUT_DIR, f'{MODEL_NAME}_{counter}_history.json')\n",
    "                    \n",
    "                    if counter == 1 and os.path.exists(specific_model_path):\n",
    "                        print(f\"[{dataset_name}] Loading pre-trained model for {MODEL_NAME} with lr={lr}, dr={dr}, counter={counter}\")\n",
    "                        model = create_model(dr, num_classes=NUM_CLASSES)\n",
    "                        model.compile(\n",
    "                            optimizer=Nadam(learning_rate=lr),\n",
    "                            loss='categorical_crossentropy',\n",
    "                            metrics=['accuracy']\n",
    "                        )\n",
    "                        model.load_weights(specific_model_path)\n",
    "                        if os.path.exists(history_json_path):\n",
    "                            with open(history_json_path, 'r') as f:\n",
    "                                history_data = json.load(f)\n",
    "                                histories.append((history_data, None))\n",
    "                        else:\n",
    "                            print(f\"[{dataset_name}] Warning: History file {history_json_path} not found. Skipping plotting.\")\n",
    "                            histories.append((None, None))\n",
    "                    elif os.path.exists(model_path) and not FORCE_TRAIN:\n",
    "                        print(f\"[{dataset_name}] Loading pre-trained model for {MODEL_NAME} with lr={lr}, dr={dr}, counter={counter}\")\n",
    "                        model = create_model(dr, num_classes=NUM_CLASSES)\n",
    "                        model.compile(\n",
    "                            optimizer=Nadam(learning_rate=lr),\n",
    "                            loss='categorical_crossentropy',\n",
    "                            metrics=['accuracy']\n",
    "                        )\n",
    "                        model.load_weights(model_path)\n",
    "                        if os.path.exists(history_json_path):\n",
    "                            with open(history_json_path, 'r') as f:\n",
    "                                history_data = json.load(f)\n",
    "                                histories.append((history_data, None))\n",
    "                        else:\n",
    "                            print(f\"[{dataset_name}] Warning: History file {history_json_path} not found. Skipping plotting.\")\n",
    "                            histories.append((None, None))\n",
    "                    else:\n",
    "                        print(f\"[{dataset_name}] Training {MODEL_NAME} with lr={lr}, dr={dr}, counter={counter}\")\n",
    "                        model = create_model(dr, num_classes=NUM_CLASSES)\n",
    "                        history, metrics_log = train_model(model, lr, train_generator, test_generator, class_weights, counter, OUTPUT_DIR)\n",
    "                        histories.append((history, metrics_log))\n",
    "                        \n",
    "                        print(f\"[{dataset_name}] Computing per-class training accuracy...\")\n",
    "                        plot_per_class_accuracy(model, train_generator, class_names, OUTPUT_DIR, counter, len(history['train_loss']))\n",
    "                    \n",
    "                    result, y_true, y_pred, test_images, test_filenames = evaluate_model(model, test_generator, class_names, counter)\n",
    "                    results.append(result)\n",
    "                    \n",
    "                    print(f\"[{dataset_name}] Generating detection box visualizations...\")\n",
    "                    visualize_detection_boxes(model, test_generator, OUTPUT_DIR, dataset_name, counter, num_samples=2)\n",
    "                    \n",
    "                    print(f\"[{dataset_name}] Running detection method comparison...\")\n",
    "                    analyze_detection_comparison(model, test_generator, OUTPUT_DIR, dataset_name, counter, num_samples=2)\n",
    "                    \n",
    "                    if result['Accuracy'] > best_test_accuracy:\n",
    "                        best_test_accuracy = result['Accuracy']\n",
    "                        best_y_true, best_y_pred, best_images, best_filenames = y_true, y_pred, test_images, test_filenames\n",
    "                    \n",
    "                    if best_y_true is not None and best_images is not None:\n",
    "                        display_classification_images(best_images, best_y_true, best_y_pred, class_names, OUTPUT_DIR, counter)\n",
    "                    \n",
    "                    subfolders = class_names\n",
    "                    selected_images = []\n",
    "                    selected_true_labels = []\n",
    "                    selected_pred_labels = []\n",
    "                    selected_filenames = []\n",
    "                    \n",
    "                    for subfolder in subfolders:\n",
    "                        subfolder_path = os.path.join(TEST_DIR, subfolder)\n",
    "                        if not os.path.exists(subfolder_path):\n",
    "                            print(f\"[{dataset_name}] Warning: Subfolder {subfolder_path} does not exist\")\n",
    "                            continue\n",
    "                        image_files = [f for f in os.listdir(subfolder_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "                        if len(image_files) < 2:\n",
    "                            print(f\"[{dataset_name}] Warning: Subfolder {subfolder} has fewer than 2 images ({len(image_files)})\")\n",
    "                            continue\n",
    "                        selected_files = random.sample(image_files, 2)\n",
    "                        class_idx = class_names.index(subfolder)\n",
    "                        \n",
    "                        for fname in selected_files:\n",
    "                            img_path = os.path.join(subfolder_path, fname)\n",
    "                            img_array = load_and_preprocess_image(img_path)\n",
    "                            if img_array is None:\n",
    "                                continue\n",
    "                            pred = model.predict(np.expand_dims(img_array, axis=0), verbose=0)\n",
    "                            pred_label = np.argmax(pred, axis=1)[0]\n",
    "                            selected_images.append(img_array)\n",
    "                            selected_true_labels.append(class_idx)\n",
    "                            selected_pred_labels.append(pred_label)\n",
    "                            selected_filenames.append(os.path.join(subfolder, fname))\n",
    "                    \n",
    "                    if selected_images:\n",
    "                        detected_boxes = run_gradcam_detection(\n",
    "                            model, selected_images, selected_true_labels, selected_pred_labels,\n",
    "                            selected_filenames, class_names, counter, LAST_CONV_LAYER_NAME, OUTPUT_DIR, num_images=6\n",
    "                        )\n",
    "                    \n",
    "                    if histories and histories[-1][0] is not None and best_y_true is not None:\n",
    "                        history, metrics_log = histories[-1]\n",
    "                        plot_training_history(history, metrics_log if metrics_log is not None else {\n",
    "                            'train_precision': [], 'train_recall': [], 'train_f1': [],\n",
    "                            'test_precision': [], 'test_recall': [], 'test_f1': []\n",
    "                        }, OUTPUT_DIR, MODEL_NAME, counter, class_names, best_y_true, best_y_pred)\n",
    "                        save_chartjs_configs(history, OUTPUT_DIR, MODEL_NAME, counter)\n",
    "                    \n",
    "                    tf.keras.backend.clear_session()\n",
    "            \n",
    "            with open(os.path.join(OUTPUT_DIR, f'{MODEL_NAME}_results_{counter}.csv'), 'w', newline='') as csvfile:\n",
    "                fieldnames = ['Model with input image size', 'Accuracy', 'AUC', 'Loss', 'Precision', 'Recall', 'F1 Score', 'Inference time (in miliseconds)']\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                writer.writeheader()\n",
    "                for result in results:\n",
    "                    writer.writerow(result)\n",
    "            \n",
    "            print(f\"[{dataset_name}] Processing completed.\")\n",
    "            tf.keras.backend.clear_session()\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interrupted by user. Cleaning up...\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c32bdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import Xception, MobileNet, DenseNet121,MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, ReLU, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, CSVLogger\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import zoom\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configuration\n",
    "IMG_SIZE_DEFAULT = (224, 224)\n",
    "KERNEL_INITIALIZER = \"glorot_uniform\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS_INITIAL = 80\n",
    "FORCE_TRAIN = True\n",
    "LEARNING_RATES = [0.002]\n",
    "DROPOUT_RATES = [0.2]\n",
    "DATASETS = [\n",
    "    {\n",
    "        'name': 'Dataset-1',\n",
    "        'TRAIN_DIR': r'E:\\dataset001\\splitdata1\\train',\n",
    "        'TEST_DIR': r'E:\\dataset001\\splitdata1\\test',\n",
    "        'OUTPUT_DIR': r'E:\\dataset001\\splitdata1\\output'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Dataset-2',\n",
    "        'TRAIN_DIR': r'E:\\dataset002\\splitdata1\\train',\n",
    "        'TEST_DIR': r'E:\\dataset002\\splitdata1\\test',\n",
    "        'OUTPUT_DIR': r'E:\\dataset002\\splitdata1\\output'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Define model configurations\n",
    "MODEL_CONFIGS = [\n",
    "    {\n",
    "        'name': 'Xception',\n",
    "        'base_model': Xception,\n",
    "        'preprocess_fn': tf.keras.applications.xception.preprocess_input,\n",
    "        'last_conv_layer': 'block14_sepconv2_act'\n",
    "    },\n",
    "        {\n",
    "        'name': 'MobileNet',\n",
    "        'base_model': MobileNet,\n",
    "        'preprocess_fn': tf.keras.applications.mobilenet.process_input\n",
    "        'last_conv_layer': 'block_16_project'\n",
    "    },\n",
    "    {\n",
    "        'name': 'MobileNetV2',\n",
    "        'base_model': MobileNetV2,\n",
    "        'preprocess_fn': tf.keras.applications.mobilenet_v2.preprocess_input,\n",
    "        'last_conv_layer': 'block_16_project'\n",
    "    },\n",
    "    {\n",
    "        'name': 'DenseNet121',\n",
    "        'base_model': DenseNet121,\n",
    "        'preprocess_fn': tf.keras.applications.densenet.preprocess_input,\n",
    "        'last_conv_layer': 'relu'\n",
    "    }\n",
    "]\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "print(f\"Using device: {physical_devices}, Num GPUs: {len(physical_devices)}\")\n",
    "\n",
    "def get_img_array(img, size):\n",
    "    array = keras.utils.img_to_array(img)\n",
    "    array = np.expand_dims(array, axis=0)\n",
    "    return array\n",
    "\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    try:\n",
    "        grad_model = keras.models.Model(\n",
    "            model.inputs, [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "        )\n",
    "        with tf.GradientTape() as tape:\n",
    "            last_conv_layer_output, preds = grad_model(img_array)\n",
    "            if pred_index is None:\n",
    "                pred_index = tf.argmax(preds[0])\n",
    "            class_channel = preds[:, pred_index]\n",
    "        grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "        last_conv_layer_output = last_conv_layer_output[0]\n",
    "        heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "        heatmap = tf.squeeze(heatmap)\n",
    "        heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap + 1e-8)\n",
    "        return heatmap.numpy()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in make_gradcam_heatmap: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_model(base_model_class, dropout_rate, num_classes, preprocess_fn):\n",
    "    try:\n",
    "        base_model = base_model_class(\n",
    "            weights='imagenet',\n",
    "            include_top=False,\n",
    "            input_shape=(*IMG_SIZE_DEFAULT, 3)\n",
    "        )\n",
    "        base_model.trainable = False\n",
    "        for layer in base_model.layers[-20:]:\n",
    "            layer.trainable = True\n",
    "        x = base_model.output\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Dense(512, kernel_initializer=KERNEL_INITIALIZER)(x)\n",
    "        x = ReLU()(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "        x = Dense(128, kernel_initializer=KERNEL_INITIALIZER)(x)\n",
    "        x = ReLU()(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "        outputs = Dense(num_classes, activation='softmax', kernel_initializer='random_uniform')(x)\n",
    "        model = Model(inputs=base_model.input, outputs=outputs)\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error in create_model: {e}\")\n",
    "        raise\n",
    "\n",
    "def load_and_preprocess_image(img_path, preprocess_fn):\n",
    "    try:\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img = img.resize(IMG_SIZE_DEFAULT)\n",
    "        img_array = keras.utils.img_to_array(img)\n",
    "        img_array = preprocess_fn(img_array)\n",
    "        return img_array\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {img_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Rest of the functions (train_model, evaluate_model, etc.) remain unchanged\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        print(f\"TensorFlow version: {tf.__version__}\")\n",
    "        \n",
    "        for dataset in DATASETS:\n",
    "            dataset_name = dataset['name']\n",
    "            TRAIN_DIR = dataset['TRAIN_DIR']\n",
    "            TEST_DIR = dataset['TEST_DIR']\n",
    "            OUTPUT_DIR = dataset['OUTPUT_DIR']\n",
    "            \n",
    "            print(f\"\\nProcessing {dataset_name}...\")\n",
    "            os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "            # Data generators\n",
    "            for model_config in MODEL_CONFIGS:\n",
    "                MODEL_NAME = model_config['name']\n",
    "                preprocess_fn = model_config['preprocess_fn']\n",
    "                train_datagen = ImageDataGenerator(\n",
    "                    rotation_range=15,\n",
    "                    horizontal_flip=True,\n",
    "                    vertical_flip=True,\n",
    "                    zoom_range=[0.8, 1.2],\n",
    "                    width_shift_range=0.2,\n",
    "                    height_shift_range=0.2,\n",
    "                    preprocessing_function=preprocess_fn\n",
    "                )\n",
    "                test_datagen = ImageDataGenerator(\n",
    "                    preprocessing_function=preprocess_fn\n",
    "                )\n",
    "                train_generator = train_datagen.flow_from_directory(\n",
    "                    TRAIN_DIR,\n",
    "                    target_size=IMG_SIZE_DEFAULT,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    class_mode='categorical',\n",
    "                    shuffle=True\n",
    "                )\n",
    "                test_generator = test_datagen.flow_from_directory(\n",
    "                    TEST_DIR,\n",
    "                    target_size=IMG_SIZE_DEFAULT,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    class_mode='categorical',\n",
    "                    shuffle=False\n",
    "                )\n",
    "                \n",
    "                # Dynamically set NUM_CLASSES\n",
    "                class_names = list(train_generator.class_indices.keys())\n",
    "                NUM_CLASSES = len(class_names)\n",
    "                print(f\"[{dataset_name}] [{MODEL_NAME}] Detected {NUM_CLASSES} classes: {class_names}\")\n",
    "                \n",
    "                if NUM_CLASSES < 2:\n",
    "                    raise ValueError(f\"[{dataset_name}] Found {NUM_CLASSES} classes. At least 2 classes are required.\")\n",
    "                \n",
    "                train_classes = train_generator.classes\n",
    "                class_weights = compute_class_weight('balanced', classes=np.unique(train_classes), y=train_classes)\n",
    "                class_weights = dict(enumerate(class_weights))\n",
    "                print(f\"[{dataset_name}] [{MODEL_NAME}] Class weights: {dict(zip(class_names, class_weights.values()))}\")\n",
    "                \n",
    "                results = []\n",
    "                counter = 0\n",
    "                histories = []\n",
    "                best_test_accuracy = 0\n",
    "                best_y_true, best_y_pred, best_images, best_filenames = None, None, None, None\n",
    "                \n",
    "                specific_model_path = os.path.join(OUTPUT_DIR, f'{MODEL_NAME}_1.weights.h5')\n",
    "                \n",
    "                for lr in LEARNING_RATES:\n",
    "                    for dr in DROPOUT_RATES:\n",
    "                        counter += 1\n",
    "                        model_path = os.path.join(OUTPUT_DIR, f'{MODEL_NAME}_{counter}.weights.h5')\n",
    "                        history_json_path = os.path.join(OUTPUT_DIR, f'{MODEL_NAME}_{counter}_history.json')\n",
    "                        \n",
    "                        if counter == 1 and os.path.exists(specific_model_path):\n",
    "                            print(f\"[{dataset_name}] [{MODEL_NAME}] Loading pre-trained model with lr={lr}, dr={dr}, counter={counter}\")\n",
    "                            model = create_model(model_config['base_model'], dr, NUM_CLASSES, preprocess_fn)\n",
    "                            model.compile(\n",
    "                                optimizer=Nadam(learning_rate=lr),\n",
    "                                loss='categorical_crossentropy',\n",
    "                                metrics=['accuracy']\n",
    "                            )\n",
    "                            model.load_weights(specific_model_path)\n",
    "                            if os.path.exists(history_json_path):\n",
    "                                with open(history_json_path, 'r') as f:\n",
    "                                    history_data = json.load(f)\n",
    "                                    histories.append((history_data, None))\n",
    "                            else:\n",
    "                                print(f\"[{dataset_name}] [{MODEL_NAME}] Warning: History file {history_json_path} not found.\")\n",
    "                                histories.append((None, None))\n",
    "                        elif os.path.exists(model_path) and not FORCE_TRAIN:\n",
    "                            print(f\"[{dataset_name}] [{MODEL_NAME}] Loading pre-trained model with lr={lr}, dr={dr}, counter={counter}\")\n",
    "                            model = create_model(model_config['base_model'], dr, NUM_CLASSES, preprocess_fn)\n",
    "                            model.compile(\n",
    "                                optimizer=Nadam(learning_rate=lr),\n",
    "                                loss='categorical_crossentropy',\n",
    "                                metrics=['accuracy']\n",
    "                            )\n",
    "                            model.load_weights(model_path)\n",
    "                            if os.path.exists(history_json_path):\n",
    "                                with open(history_json_path, 'r') as f:\n",
    "                                    history_data = json.load(f)\n",
    "                                    histories.append((history_data, None))\n",
    "                            else:\n",
    "                                print(f\"[{dataset_name}] [{MODEL_NAME}] Warning: History file {history_json_path} not found.\")\n",
    "                                histories.append((None, None))\n",
    "                        else:\n",
    "                            print(f\"[{dataset_name}] [{MODEL_NAME}] Training with lr={lr}, dr={dr}, counter={counter}\")\n",
    "                            model = create_model(model_config['base_model'], dr, NUM_CLASSES, preprocess_fn)\n",
    "                            history, metrics_log = train_model(model, lr, train_generator, test_generator, class_weights, counter)\n",
    "                            histories.append((history, metrics_log))\n",
    "                        \n",
    "                        # Evaluate and get images\n",
    "                        result, y_true, y_pred, test_images, test_filenames = evaluate_model(\n",
    "                            model, test_generator, class_names, counter\n",
    "                        )\n",
    "                        results.append(result)\n",
    "                        \n",
    "                        if result['Accuracy'] > best_test_accuracy:\n",
    "                            best_test_accuracy = result['Accuracy']\n",
    "                            best_y_true, best_y_pred, best_images, best_filenames = y_true, y_pred, test_images, test_filenames\n",
    "                        \n",
    "                        # Display classification results\n",
    "                        if best_y_true is not None and best_images is not None:\n",
    "                            display_classification_images(\n",
    "                                best_images, best_y_true, best_y_pred, class_names, OUTPUT_DIR, counter\n",
    "                            )\n",
    "                        \n",
    "                        # Select 2 random images from each subfolder\n",
    "                        subfolders = class_names\n",
    "                        selected_images = []\n",
    "                        selected_true_labels = []\n",
    "                        selected_pred_labels = []\n",
    "                        selected_filenames = []\n",
    "                        \n",
    "                        for subfolder in subfolders:\n",
    "                            subfolder_path = os.path.join(TEST_DIR, subfolder)\n",
    "                            if not os.path.exists(subfolder_path):\n",
    "                                print(f\"[{dataset_name}] [{MODEL_NAME}] Warning: Subfolder {subfolder_path} does not exist\")\n",
    "                                continue\n",
    "                            image_files = [f for f in os.listdir(subfolder_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "                            if len(image_files) < 2:\n",
    "                                print(f\"[{dataset_name}] [{MODEL_NAME}] Warning: Subfolder {subfolder} has fewer than 2 images\")\n",
    "                                continue\n",
    "                            selected_files = random.sample(image_files, 2)\n",
    "                            class_idx = class_names.index(subfolder)\n",
    "                            \n",
    "                            for fname in selected_files:\n",
    "                                img_path = os.path.join(subfolder_path, fname)\n",
    "                                img_array = load_and_preprocess_image(img_path, preprocess_fn)\n",
    "                                if img_array is not None:\n",
    "                                    pred = model.predict(np.expand_dims(img_array, axis=0), verbose=0)\n",
    "                                    pred_label = np.argmax(pred, axis=1)[0]\n",
    "                                    selected_images.append(img_array)\n",
    "                                    selected_true_labels.append(class_idx)\n",
    "                                    selected_pred_labels.append(pred_label)\n",
    "                                    selected_filenames.append(os.path.join(subfolder, fname))\n",
    "                        \n",
    "                        if selected_images:\n",
    "                            detected_boxes = run_gradcam_detection(\n",
    "                                model, selected_images, selected_true_labels, selected_pred_labels,\n",
    "                                selected_filenames, class_names, counter, model_config['last_conv_layer'], num_images=6\n",
    "                            )\n",
    "                        \n",
    "                        if histories and histories[-1][0] is not None and best_y_true is not None:\n",
    "                            history, metrics_log = histories[-1]\n",
    "                            plot_training_history(\n",
    "                                history, metrics_log if metrics_log is not None else {\n",
    "                                    'train_precision': [], 'train_recall': [], 'train_f1': [],\n",
    "                                    'test_precision': [], 'test_recall': [], 'test_f1': []\n",
    "                                }, OUTPUT_DIR, MODEL_NAME, counter, class_names, best_y_true, best_y_pred\n",
    "                            )\n",
    "                            save_chartjs_configs(history, OUTPUT_DIR, MODEL_NAME, counter)\n",
    "                        \n",
    "                        tf.keras.backend.clear_session()\n",
    "                \n",
    "                with open(os.path.join(OUTPUT_DIR, f'{MODEL_NAME}_results_{counter}.csv'), 'w', newline='') as csvfile:\n",
    "                    fieldnames = ['Model with input image size', 'Accuracy', 'AUC', 'Loss', 'Precision', 'Recall', 'F1 Score', 'Inference time (in miliseconds)']\n",
    "                    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                    writer.writeheader()\n",
    "                    for result in results:\n",
    "                        writer.writerow(result)\n",
    "                \n",
    "                print(f\"[{dataset_name}] [{MODEL_NAME}] Processing completed.\")\n",
    "                tf.keras.backend.clear_session()\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interrupted by user. Cleaning up...\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a514a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
